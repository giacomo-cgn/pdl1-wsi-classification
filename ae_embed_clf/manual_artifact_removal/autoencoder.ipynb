{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f81b05cc-2aed-4a89-8c2d-5ee90f0a4cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils import data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "\n",
    "from autoencoder import autoencoder, train, evaluate\n",
    "from dataset import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4018f730-c1ae-4934-92c4-a8d3f8a51962",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c31ce237-4d99-4b50-aee3-73f876b18227",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = './ae_models/ae/'\n",
    "\n",
    "# EncoderCNN architecture\n",
    "fc_hidden1 = 256\n",
    "embed_dim = 32     # latent dim extracted by 2D CNN\n",
    "dropout_p = 0.2       # dropout probability\n",
    "\n",
    "# training parameters\n",
    "start_epoch = 0\n",
    "epochs = 20     # training epochs\n",
    "batch_size = 8\n",
    "learning_rate = 1e-3\n",
    "log_interval = 1000   # interval for displaying training info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80c058d5-092a-4f45-a5e1-b2e04683a922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 GPU\n",
      "Train Epoch: 1 [8000/1309486 (1%)]\tLoss: 46248.082031\n",
      "Train Epoch: 1 [16000/1309486 (1%)]\tLoss: 37759.929688\n",
      "Train Epoch: 1 [24000/1309486 (2%)]\tLoss: 48941.843750\n",
      "Train Epoch: 1 [32000/1309486 (2%)]\tLoss: 38080.757812\n",
      "Train Epoch: 1 [40000/1309486 (3%)]\tLoss: 40175.367188\n",
      "Train Epoch: 1 [48000/1309486 (4%)]\tLoss: 40559.285156\n",
      "Train Epoch: 1 [56000/1309486 (4%)]\tLoss: 41244.195312\n",
      "Train Epoch: 1 [64000/1309486 (5%)]\tLoss: 37064.984375\n",
      "Train Epoch: 1 [72000/1309486 (5%)]\tLoss: 42079.996094\n",
      "Train Epoch: 1 [80000/1309486 (6%)]\tLoss: 42243.523438\n",
      "Train Epoch: 1 [88000/1309486 (7%)]\tLoss: 40269.347656\n",
      "Train Epoch: 1 [96000/1309486 (7%)]\tLoss: 41192.726562\n",
      "Train Epoch: 1 [104000/1309486 (8%)]\tLoss: 39926.460938\n",
      "Train Epoch: 1 [112000/1309486 (9%)]\tLoss: 38056.378906\n",
      "Train Epoch: 1 [120000/1309486 (9%)]\tLoss: 38915.656250\n",
      "Train Epoch: 1 [128000/1309486 (10%)]\tLoss: 36561.000000\n",
      "Train Epoch: 1 [136000/1309486 (10%)]\tLoss: 41966.855469\n",
      "Train Epoch: 1 [144000/1309486 (11%)]\tLoss: 37236.902344\n",
      "Train Epoch: 1 [152000/1309486 (12%)]\tLoss: 36199.023438\n",
      "Train Epoch: 1 [160000/1309486 (12%)]\tLoss: 39824.757812\n",
      "Train Epoch: 1 [168000/1309486 (13%)]\tLoss: 38985.312500\n",
      "Train Epoch: 1 [176000/1309486 (13%)]\tLoss: 40850.839844\n",
      "Train Epoch: 1 [184000/1309486 (14%)]\tLoss: 40379.507812\n",
      "Train Epoch: 1 [192000/1309486 (15%)]\tLoss: 40655.746094\n",
      "Train Epoch: 1 [200000/1309486 (15%)]\tLoss: 38452.851562\n",
      "Train Epoch: 1 [208000/1309486 (16%)]\tLoss: 38257.101562\n",
      "Train Epoch: 1 [216000/1309486 (16%)]\tLoss: 40163.210938\n",
      "Train Epoch: 1 [224000/1309486 (17%)]\tLoss: 39059.250000\n",
      "Train Epoch: 1 [232000/1309486 (18%)]\tLoss: 37471.851562\n",
      "Train Epoch: 1 [240000/1309486 (18%)]\tLoss: 40109.582031\n",
      "Train Epoch: 1 [248000/1309486 (19%)]\tLoss: 39921.593750\n",
      "Train Epoch: 1 [256000/1309486 (20%)]\tLoss: 37642.085938\n",
      "Train Epoch: 1 [264000/1309486 (20%)]\tLoss: 37630.773438\n",
      "Train Epoch: 1 [272000/1309486 (21%)]\tLoss: 44819.046875\n",
      "Train Epoch: 1 [280000/1309486 (21%)]\tLoss: 42876.105469\n",
      "Train Epoch: 1 [288000/1309486 (22%)]\tLoss: 42967.062500\n",
      "Train Epoch: 1 [296000/1309486 (23%)]\tLoss: 35949.320312\n",
      "Train Epoch: 1 [304000/1309486 (23%)]\tLoss: 35449.585938\n",
      "Train Epoch: 1 [312000/1309486 (24%)]\tLoss: 38147.753906\n",
      "Train Epoch: 1 [320000/1309486 (24%)]\tLoss: 36415.609375\n",
      "Train Epoch: 1 [328000/1309486 (25%)]\tLoss: 40747.500000\n",
      "Train Epoch: 1 [336000/1309486 (26%)]\tLoss: 36764.421875\n",
      "Train Epoch: 1 [344000/1309486 (26%)]\tLoss: 37144.757812\n",
      "Train Epoch: 1 [352000/1309486 (27%)]\tLoss: 40169.406250\n",
      "Train Epoch: 1 [360000/1309486 (27%)]\tLoss: 36071.320312\n",
      "Train Epoch: 1 [368000/1309486 (28%)]\tLoss: 35797.300781\n",
      "Train Epoch: 1 [376000/1309486 (29%)]\tLoss: 43060.675781\n",
      "Train Epoch: 1 [384000/1309486 (29%)]\tLoss: 35389.851562\n",
      "Train Epoch: 1 [392000/1309486 (30%)]\tLoss: 39310.785156\n",
      "Train Epoch: 1 [400000/1309486 (31%)]\tLoss: 36523.507812\n",
      "Train Epoch: 1 [408000/1309486 (31%)]\tLoss: 37043.667969\n",
      "Train Epoch: 1 [416000/1309486 (32%)]\tLoss: 35757.773438\n",
      "Train Epoch: 1 [424000/1309486 (32%)]\tLoss: 41589.167969\n",
      "Train Epoch: 1 [432000/1309486 (33%)]\tLoss: 38451.949219\n",
      "Train Epoch: 1 [440000/1309486 (34%)]\tLoss: 37674.972656\n",
      "Train Epoch: 1 [448000/1309486 (34%)]\tLoss: 42364.679688\n",
      "Train Epoch: 1 [456000/1309486 (35%)]\tLoss: 41024.156250\n",
      "Train Epoch: 1 [464000/1309486 (35%)]\tLoss: 39059.375000\n",
      "Train Epoch: 1 [472000/1309486 (36%)]\tLoss: 35014.285156\n",
      "Train Epoch: 1 [480000/1309486 (37%)]\tLoss: 40477.914062\n",
      "Train Epoch: 1 [488000/1309486 (37%)]\tLoss: 42445.875000\n",
      "Train Epoch: 1 [496000/1309486 (38%)]\tLoss: 39496.078125\n",
      "Train Epoch: 1 [504000/1309486 (38%)]\tLoss: 39174.757812\n",
      "Train Epoch: 1 [512000/1309486 (39%)]\tLoss: 34241.273438\n",
      "Train Epoch: 1 [520000/1309486 (40%)]\tLoss: 35971.640625\n",
      "Train Epoch: 1 [528000/1309486 (40%)]\tLoss: 38907.738281\n",
      "Train Epoch: 1 [536000/1309486 (41%)]\tLoss: 36353.347656\n",
      "Train Epoch: 1 [544000/1309486 (42%)]\tLoss: 35826.933594\n",
      "Train Epoch: 1 [552000/1309486 (42%)]\tLoss: 37416.601562\n",
      "Train Epoch: 1 [560000/1309486 (43%)]\tLoss: 36396.343750\n",
      "Train Epoch: 1 [568000/1309486 (43%)]\tLoss: 41610.179688\n",
      "Train Epoch: 1 [576000/1309486 (44%)]\tLoss: 37558.906250\n",
      "Train Epoch: 1 [584000/1309486 (45%)]\tLoss: 35676.531250\n",
      "Train Epoch: 1 [592000/1309486 (45%)]\tLoss: 35541.492188\n",
      "Train Epoch: 1 [600000/1309486 (46%)]\tLoss: 38931.160156\n",
      "Train Epoch: 1 [608000/1309486 (46%)]\tLoss: 38488.988281\n",
      "Train Epoch: 1 [616000/1309486 (47%)]\tLoss: 39059.132812\n",
      "Train Epoch: 1 [624000/1309486 (48%)]\tLoss: 40085.238281\n",
      "Train Epoch: 1 [632000/1309486 (48%)]\tLoss: 38782.210938\n",
      "Train Epoch: 1 [640000/1309486 (49%)]\tLoss: 34842.226562\n",
      "Train Epoch: 1 [648000/1309486 (49%)]\tLoss: 36897.781250\n",
      "Train Epoch: 1 [656000/1309486 (50%)]\tLoss: 38007.351562\n",
      "Train Epoch: 1 [664000/1309486 (51%)]\tLoss: 42766.562500\n",
      "Train Epoch: 1 [672000/1309486 (51%)]\tLoss: 38021.390625\n",
      "Train Epoch: 1 [680000/1309486 (52%)]\tLoss: 36000.660156\n",
      "Train Epoch: 1 [688000/1309486 (53%)]\tLoss: 39038.976562\n",
      "Train Epoch: 1 [696000/1309486 (53%)]\tLoss: 33177.570312\n",
      "Train Epoch: 1 [704000/1309486 (54%)]\tLoss: 39415.148438\n",
      "Train Epoch: 1 [712000/1309486 (54%)]\tLoss: 38074.593750\n",
      "Train Epoch: 1 [720000/1309486 (55%)]\tLoss: 38334.632812\n",
      "Train Epoch: 1 [728000/1309486 (56%)]\tLoss: 37704.144531\n",
      "Train Epoch: 1 [736000/1309486 (56%)]\tLoss: 36111.218750\n",
      "Train Epoch: 1 [744000/1309486 (57%)]\tLoss: 39935.289062\n",
      "Train Epoch: 1 [752000/1309486 (57%)]\tLoss: 37932.765625\n",
      "Train Epoch: 1 [760000/1309486 (58%)]\tLoss: 37645.398438\n",
      "Train Epoch: 1 [768000/1309486 (59%)]\tLoss: 44928.250000\n",
      "Train Epoch: 1 [776000/1309486 (59%)]\tLoss: 37127.050781\n",
      "Train Epoch: 1 [784000/1309486 (60%)]\tLoss: 39542.687500\n",
      "Train Epoch: 1 [792000/1309486 (60%)]\tLoss: 38067.601562\n",
      "Train Epoch: 1 [800000/1309486 (61%)]\tLoss: 41885.398438\n",
      "Train Epoch: 1 [808000/1309486 (62%)]\tLoss: 40860.105469\n",
      "Train Epoch: 1 [816000/1309486 (62%)]\tLoss: 38434.453125\n",
      "Train Epoch: 1 [824000/1309486 (63%)]\tLoss: 34238.441406\n",
      "Train Epoch: 1 [832000/1309486 (64%)]\tLoss: 40290.781250\n",
      "Train Epoch: 1 [840000/1309486 (64%)]\tLoss: 36916.890625\n",
      "Train Epoch: 1 [848000/1309486 (65%)]\tLoss: 37762.351562\n",
      "Train Epoch: 1 [856000/1309486 (65%)]\tLoss: 37307.074219\n",
      "Train Epoch: 1 [864000/1309486 (66%)]\tLoss: 41367.406250\n",
      "Train Epoch: 1 [872000/1309486 (67%)]\tLoss: 43069.312500\n",
      "Train Epoch: 1 [880000/1309486 (67%)]\tLoss: 41102.953125\n",
      "Train Epoch: 1 [888000/1309486 (68%)]\tLoss: 37068.523438\n",
      "Train Epoch: 1 [896000/1309486 (68%)]\tLoss: 34623.289062\n",
      "Train Epoch: 1 [904000/1309486 (69%)]\tLoss: 42277.796875\n",
      "Train Epoch: 1 [912000/1309486 (70%)]\tLoss: 39026.335938\n",
      "Train Epoch: 1 [920000/1309486 (70%)]\tLoss: 40277.406250\n",
      "Train Epoch: 1 [928000/1309486 (71%)]\tLoss: 40964.027344\n",
      "Train Epoch: 1 [936000/1309486 (71%)]\tLoss: 35786.714844\n",
      "Train Epoch: 1 [944000/1309486 (72%)]\tLoss: 38452.546875\n",
      "Train Epoch: 1 [952000/1309486 (73%)]\tLoss: 40944.011719\n",
      "Train Epoch: 1 [960000/1309486 (73%)]\tLoss: 39883.734375\n",
      "Train Epoch: 1 [968000/1309486 (74%)]\tLoss: 37419.156250\n",
      "Train Epoch: 1 [976000/1309486 (75%)]\tLoss: 38523.714844\n",
      "Train Epoch: 1 [984000/1309486 (75%)]\tLoss: 39704.328125\n",
      "Train Epoch: 1 [992000/1309486 (76%)]\tLoss: 39112.789062\n",
      "Train Epoch: 1 [1000000/1309486 (76%)]\tLoss: 39933.410156\n",
      "Train Epoch: 1 [1008000/1309486 (77%)]\tLoss: 40565.031250\n",
      "Train Epoch: 1 [1016000/1309486 (78%)]\tLoss: 37308.589844\n",
      "Train Epoch: 1 [1024000/1309486 (78%)]\tLoss: 38769.867188\n",
      "Train Epoch: 1 [1032000/1309486 (79%)]\tLoss: 41179.324219\n",
      "Train Epoch: 1 [1040000/1309486 (79%)]\tLoss: 41163.335938\n",
      "Train Epoch: 1 [1048000/1309486 (80%)]\tLoss: 39072.570312\n",
      "Train Epoch: 1 [1056000/1309486 (81%)]\tLoss: 41834.546875\n",
      "Train Epoch: 1 [1064000/1309486 (81%)]\tLoss: 37967.050781\n",
      "Train Epoch: 1 [1072000/1309486 (82%)]\tLoss: 38612.113281\n",
      "Train Epoch: 1 [1080000/1309486 (82%)]\tLoss: 38478.546875\n",
      "Train Epoch: 1 [1088000/1309486 (83%)]\tLoss: 34969.343750\n",
      "Train Epoch: 1 [1096000/1309486 (84%)]\tLoss: 39942.164062\n",
      "Train Epoch: 1 [1104000/1309486 (84%)]\tLoss: 37589.156250\n",
      "Train Epoch: 1 [1112000/1309486 (85%)]\tLoss: 41177.855469\n",
      "Train Epoch: 1 [1120000/1309486 (86%)]\tLoss: 45959.863281\n",
      "Train Epoch: 1 [1128000/1309486 (86%)]\tLoss: 35281.644531\n",
      "Train Epoch: 1 [1136000/1309486 (87%)]\tLoss: 38856.535156\n",
      "Train Epoch: 1 [1144000/1309486 (87%)]\tLoss: 42033.535156\n",
      "Train Epoch: 1 [1152000/1309486 (88%)]\tLoss: 37378.687500\n",
      "Train Epoch: 1 [1160000/1309486 (89%)]\tLoss: 41024.257812\n",
      "Train Epoch: 1 [1168000/1309486 (89%)]\tLoss: 44705.179688\n",
      "Train Epoch: 1 [1176000/1309486 (90%)]\tLoss: 34387.257812\n",
      "Train Epoch: 1 [1184000/1309486 (90%)]\tLoss: 38684.531250\n",
      "Train Epoch: 1 [1192000/1309486 (91%)]\tLoss: 37775.687500\n",
      "Train Epoch: 1 [1200000/1309486 (92%)]\tLoss: 41093.382812\n",
      "Train Epoch: 1 [1208000/1309486 (92%)]\tLoss: 32796.125000\n",
      "Train Epoch: 1 [1216000/1309486 (93%)]\tLoss: 37840.601562\n",
      "Train Epoch: 1 [1224000/1309486 (93%)]\tLoss: 38074.156250\n",
      "Train Epoch: 1 [1232000/1309486 (94%)]\tLoss: 39001.394531\n",
      "Train Epoch: 1 [1240000/1309486 (95%)]\tLoss: 34200.535156\n",
      "Train Epoch: 1 [1248000/1309486 (95%)]\tLoss: 38919.617188\n",
      "Train Epoch: 1 [1256000/1309486 (96%)]\tLoss: 39751.250000\n",
      "Train Epoch: 1 [1264000/1309486 (97%)]\tLoss: 40129.109375\n",
      "Train Epoch: 1 [1272000/1309486 (97%)]\tLoss: 43311.375000\n",
      "Train Epoch: 1 [1280000/1309486 (98%)]\tLoss: 40157.773438\n",
      "Train Epoch: 1 [1288000/1309486 (98%)]\tLoss: 42248.046875\n",
      "Train Epoch: 1 [1296000/1309486 (99%)]\tLoss: 34899.914062\n",
      "Train Epoch: 1 [1304000/1309486 (100%)]\tLoss: 36678.585938\n",
      "Epoch 1 model saved!\n",
      "epoch train time:  843.49986339\n",
      "Train Epoch: 2 [8000/1309486 (1%)]\tLoss: 34703.085938\n",
      "Train Epoch: 2 [16000/1309486 (1%)]\tLoss: 39913.648438\n",
      "Train Epoch: 2 [24000/1309486 (2%)]\tLoss: 40870.902344\n",
      "Train Epoch: 2 [32000/1309486 (2%)]\tLoss: 38892.679688\n",
      "Train Epoch: 2 [40000/1309486 (3%)]\tLoss: 41853.945312\n",
      "Train Epoch: 2 [48000/1309486 (4%)]\tLoss: 40321.519531\n",
      "Train Epoch: 2 [56000/1309486 (4%)]\tLoss: 39765.359375\n",
      "Train Epoch: 2 [64000/1309486 (5%)]\tLoss: 37478.753906\n",
      "Train Epoch: 2 [72000/1309486 (5%)]\tLoss: 40779.855469\n",
      "Train Epoch: 2 [80000/1309486 (6%)]\tLoss: 40751.089844\n",
      "Train Epoch: 2 [88000/1309486 (7%)]\tLoss: 37775.281250\n",
      "Train Epoch: 2 [96000/1309486 (7%)]\tLoss: 38826.609375\n",
      "Train Epoch: 2 [104000/1309486 (8%)]\tLoss: 36443.289062\n",
      "Train Epoch: 2 [112000/1309486 (9%)]\tLoss: 36884.148438\n",
      "Train Epoch: 2 [120000/1309486 (9%)]\tLoss: 39217.710938\n",
      "Train Epoch: 2 [128000/1309486 (10%)]\tLoss: 41253.054688\n",
      "Train Epoch: 2 [136000/1309486 (10%)]\tLoss: 36757.128906\n",
      "Train Epoch: 2 [144000/1309486 (11%)]\tLoss: 35552.140625\n",
      "Train Epoch: 2 [152000/1309486 (12%)]\tLoss: 37810.355469\n",
      "Train Epoch: 2 [160000/1309486 (12%)]\tLoss: 37914.949219\n",
      "Train Epoch: 2 [168000/1309486 (13%)]\tLoss: 35816.253906\n",
      "Train Epoch: 2 [176000/1309486 (13%)]\tLoss: 38105.429688\n",
      "Train Epoch: 2 [184000/1309486 (14%)]\tLoss: 36286.710938\n",
      "Train Epoch: 2 [192000/1309486 (15%)]\tLoss: 34929.285156\n",
      "Train Epoch: 2 [200000/1309486 (15%)]\tLoss: 37735.226562\n",
      "Train Epoch: 2 [208000/1309486 (16%)]\tLoss: 35318.382812\n",
      "Train Epoch: 2 [216000/1309486 (16%)]\tLoss: 38734.539062\n",
      "Train Epoch: 2 [224000/1309486 (17%)]\tLoss: 40919.078125\n",
      "Train Epoch: 2 [232000/1309486 (18%)]\tLoss: 40192.433594\n",
      "Train Epoch: 2 [240000/1309486 (18%)]\tLoss: 39164.976562\n",
      "Train Epoch: 2 [248000/1309486 (19%)]\tLoss: 36870.195312\n",
      "Train Epoch: 2 [256000/1309486 (20%)]\tLoss: 37788.996094\n",
      "Train Epoch: 2 [264000/1309486 (20%)]\tLoss: 35105.156250\n",
      "Train Epoch: 2 [272000/1309486 (21%)]\tLoss: 39832.558594\n",
      "Train Epoch: 2 [280000/1309486 (21%)]\tLoss: 35773.515625\n",
      "Train Epoch: 2 [288000/1309486 (22%)]\tLoss: 40396.847656\n",
      "Train Epoch: 2 [296000/1309486 (23%)]\tLoss: 38642.562500\n",
      "Train Epoch: 2 [304000/1309486 (23%)]\tLoss: 37714.367188\n",
      "Train Epoch: 2 [312000/1309486 (24%)]\tLoss: 40122.027344\n",
      "Train Epoch: 2 [320000/1309486 (24%)]\tLoss: 39285.914062\n",
      "Train Epoch: 2 [328000/1309486 (25%)]\tLoss: 39976.468750\n",
      "Train Epoch: 2 [336000/1309486 (26%)]\tLoss: 40156.585938\n",
      "Train Epoch: 2 [344000/1309486 (26%)]\tLoss: 39365.179688\n",
      "Train Epoch: 2 [352000/1309486 (27%)]\tLoss: 39641.007812\n",
      "Train Epoch: 2 [360000/1309486 (27%)]\tLoss: 36067.914062\n",
      "Train Epoch: 2 [368000/1309486 (28%)]\tLoss: 35129.179688\n",
      "Train Epoch: 2 [376000/1309486 (29%)]\tLoss: 40739.156250\n",
      "Train Epoch: 2 [384000/1309486 (29%)]\tLoss: 38808.421875\n",
      "Train Epoch: 2 [392000/1309486 (30%)]\tLoss: 37120.726562\n",
      "Train Epoch: 2 [400000/1309486 (31%)]\tLoss: 36058.679688\n",
      "Train Epoch: 2 [408000/1309486 (31%)]\tLoss: 35560.363281\n",
      "Train Epoch: 2 [416000/1309486 (32%)]\tLoss: 38509.859375\n",
      "Train Epoch: 2 [424000/1309486 (32%)]\tLoss: 36628.398438\n",
      "Train Epoch: 2 [432000/1309486 (33%)]\tLoss: 39214.027344\n",
      "Train Epoch: 2 [440000/1309486 (34%)]\tLoss: 38404.578125\n",
      "Train Epoch: 2 [448000/1309486 (34%)]\tLoss: 39914.289062\n",
      "Train Epoch: 2 [456000/1309486 (35%)]\tLoss: 36492.890625\n",
      "Train Epoch: 2 [464000/1309486 (35%)]\tLoss: 35777.105469\n",
      "Train Epoch: 2 [472000/1309486 (36%)]\tLoss: 34978.171875\n",
      "Train Epoch: 2 [480000/1309486 (37%)]\tLoss: 40155.765625\n",
      "Train Epoch: 2 [488000/1309486 (37%)]\tLoss: 35203.437500\n",
      "Train Epoch: 2 [496000/1309486 (38%)]\tLoss: 38236.957031\n",
      "Train Epoch: 2 [504000/1309486 (38%)]\tLoss: 40585.171875\n",
      "Train Epoch: 2 [512000/1309486 (39%)]\tLoss: 38550.437500\n",
      "Train Epoch: 2 [520000/1309486 (40%)]\tLoss: 36288.906250\n",
      "Train Epoch: 2 [528000/1309486 (40%)]\tLoss: 36989.765625\n",
      "Train Epoch: 2 [536000/1309486 (41%)]\tLoss: 38554.484375\n",
      "Train Epoch: 2 [544000/1309486 (42%)]\tLoss: 39776.542969\n",
      "Train Epoch: 2 [552000/1309486 (42%)]\tLoss: 37974.687500\n",
      "Train Epoch: 2 [560000/1309486 (43%)]\tLoss: 37777.085938\n",
      "Train Epoch: 2 [568000/1309486 (43%)]\tLoss: 38760.800781\n",
      "Train Epoch: 2 [576000/1309486 (44%)]\tLoss: 35017.738281\n",
      "Train Epoch: 2 [584000/1309486 (45%)]\tLoss: 35812.000000\n",
      "Train Epoch: 2 [592000/1309486 (45%)]\tLoss: 39319.554688\n",
      "Train Epoch: 2 [600000/1309486 (46%)]\tLoss: 37127.437500\n",
      "Train Epoch: 2 [608000/1309486 (46%)]\tLoss: 40592.343750\n",
      "Train Epoch: 2 [616000/1309486 (47%)]\tLoss: 39974.484375\n",
      "Train Epoch: 2 [624000/1309486 (48%)]\tLoss: 41184.816406\n",
      "Train Epoch: 2 [632000/1309486 (48%)]\tLoss: 38509.507812\n",
      "Train Epoch: 2 [640000/1309486 (49%)]\tLoss: 39161.375000\n",
      "Train Epoch: 2 [648000/1309486 (49%)]\tLoss: 39077.367188\n",
      "Train Epoch: 2 [656000/1309486 (50%)]\tLoss: 41014.839844\n",
      "Train Epoch: 2 [664000/1309486 (51%)]\tLoss: 37216.289062\n",
      "Train Epoch: 2 [672000/1309486 (51%)]\tLoss: 36689.054688\n",
      "Train Epoch: 2 [680000/1309486 (52%)]\tLoss: 39010.406250\n",
      "Train Epoch: 2 [688000/1309486 (53%)]\tLoss: 34968.511719\n",
      "Train Epoch: 2 [696000/1309486 (53%)]\tLoss: 39183.335938\n",
      "Train Epoch: 2 [704000/1309486 (54%)]\tLoss: 42004.906250\n",
      "Train Epoch: 2 [712000/1309486 (54%)]\tLoss: 35902.265625\n",
      "Train Epoch: 2 [720000/1309486 (55%)]\tLoss: 35636.078125\n",
      "Train Epoch: 2 [728000/1309486 (56%)]\tLoss: 37996.398438\n",
      "Train Epoch: 2 [736000/1309486 (56%)]\tLoss: 41120.605469\n",
      "Train Epoch: 2 [744000/1309486 (57%)]\tLoss: 38023.125000\n",
      "Train Epoch: 2 [752000/1309486 (57%)]\tLoss: 42213.136719\n",
      "Train Epoch: 2 [760000/1309486 (58%)]\tLoss: 42316.078125\n",
      "Train Epoch: 2 [768000/1309486 (59%)]\tLoss: 40367.160156\n",
      "Train Epoch: 2 [776000/1309486 (59%)]\tLoss: 44298.531250\n",
      "Train Epoch: 2 [784000/1309486 (60%)]\tLoss: 34580.250000\n",
      "Train Epoch: 2 [792000/1309486 (60%)]\tLoss: 37843.605469\n",
      "Train Epoch: 2 [800000/1309486 (61%)]\tLoss: 44865.703125\n",
      "Train Epoch: 2 [808000/1309486 (62%)]\tLoss: 36867.742188\n",
      "Train Epoch: 2 [816000/1309486 (62%)]\tLoss: 42649.257812\n",
      "Train Epoch: 2 [824000/1309486 (63%)]\tLoss: 37938.824219\n",
      "Train Epoch: 2 [832000/1309486 (64%)]\tLoss: 34844.765625\n",
      "Train Epoch: 2 [840000/1309486 (64%)]\tLoss: 43934.921875\n",
      "Train Epoch: 2 [848000/1309486 (65%)]\tLoss: 40392.484375\n",
      "Train Epoch: 2 [856000/1309486 (65%)]\tLoss: 41617.035156\n",
      "Train Epoch: 2 [864000/1309486 (66%)]\tLoss: 42201.546875\n",
      "Train Epoch: 2 [872000/1309486 (67%)]\tLoss: 39236.308594\n",
      "Train Epoch: 2 [880000/1309486 (67%)]\tLoss: 37832.585938\n",
      "Train Epoch: 2 [888000/1309486 (68%)]\tLoss: 36948.828125\n",
      "Train Epoch: 2 [896000/1309486 (68%)]\tLoss: 39117.414062\n",
      "Train Epoch: 2 [904000/1309486 (69%)]\tLoss: 41426.648438\n",
      "Train Epoch: 2 [912000/1309486 (70%)]\tLoss: 35557.812500\n",
      "Train Epoch: 2 [920000/1309486 (70%)]\tLoss: 34583.843750\n",
      "Train Epoch: 2 [928000/1309486 (71%)]\tLoss: 37054.218750\n",
      "Train Epoch: 2 [936000/1309486 (71%)]\tLoss: 39346.816406\n",
      "Train Epoch: 2 [944000/1309486 (72%)]\tLoss: 39004.832031\n",
      "Train Epoch: 2 [952000/1309486 (73%)]\tLoss: 35988.359375\n",
      "Train Epoch: 2 [960000/1309486 (73%)]\tLoss: 32593.669922\n",
      "Train Epoch: 2 [968000/1309486 (74%)]\tLoss: 39629.460938\n",
      "Train Epoch: 2 [976000/1309486 (75%)]\tLoss: 37473.304688\n",
      "Train Epoch: 2 [984000/1309486 (75%)]\tLoss: 39018.914062\n",
      "Train Epoch: 2 [992000/1309486 (76%)]\tLoss: 39795.398438\n",
      "Train Epoch: 2 [1000000/1309486 (76%)]\tLoss: 35660.179688\n",
      "Train Epoch: 2 [1008000/1309486 (77%)]\tLoss: 38467.636719\n",
      "Train Epoch: 2 [1016000/1309486 (78%)]\tLoss: 36720.554688\n",
      "Train Epoch: 2 [1024000/1309486 (78%)]\tLoss: 43178.195312\n",
      "Train Epoch: 2 [1032000/1309486 (79%)]\tLoss: 36429.023438\n",
      "Train Epoch: 2 [1040000/1309486 (79%)]\tLoss: 40595.269531\n",
      "Train Epoch: 2 [1048000/1309486 (80%)]\tLoss: 36005.234375\n",
      "Train Epoch: 2 [1056000/1309486 (81%)]\tLoss: 37660.433594\n",
      "Train Epoch: 2 [1064000/1309486 (81%)]\tLoss: 38216.710938\n",
      "Train Epoch: 2 [1072000/1309486 (82%)]\tLoss: 36988.988281\n",
      "Train Epoch: 2 [1080000/1309486 (82%)]\tLoss: 38992.980469\n",
      "Train Epoch: 2 [1088000/1309486 (83%)]\tLoss: 37654.371094\n",
      "Train Epoch: 2 [1096000/1309486 (84%)]\tLoss: 35276.882812\n",
      "Train Epoch: 2 [1104000/1309486 (84%)]\tLoss: 38921.632812\n",
      "Train Epoch: 2 [1112000/1309486 (85%)]\tLoss: 40713.382812\n",
      "Train Epoch: 2 [1120000/1309486 (86%)]\tLoss: 36283.753906\n",
      "Train Epoch: 2 [1128000/1309486 (86%)]\tLoss: 37857.101562\n",
      "Train Epoch: 2 [1136000/1309486 (87%)]\tLoss: 40277.007812\n",
      "Train Epoch: 2 [1144000/1309486 (87%)]\tLoss: 38797.652344\n",
      "Train Epoch: 2 [1152000/1309486 (88%)]\tLoss: 39774.957031\n",
      "Train Epoch: 2 [1160000/1309486 (89%)]\tLoss: 41956.050781\n",
      "Train Epoch: 2 [1168000/1309486 (89%)]\tLoss: 34798.445312\n",
      "Train Epoch: 2 [1176000/1309486 (90%)]\tLoss: 37100.304688\n",
      "Train Epoch: 2 [1184000/1309486 (90%)]\tLoss: 42196.898438\n",
      "Train Epoch: 2 [1192000/1309486 (91%)]\tLoss: 37692.671875\n",
      "Train Epoch: 2 [1200000/1309486 (92%)]\tLoss: 39461.445312\n",
      "Train Epoch: 2 [1208000/1309486 (92%)]\tLoss: 38229.277344\n",
      "Train Epoch: 2 [1216000/1309486 (93%)]\tLoss: 39648.843750\n",
      "Train Epoch: 2 [1224000/1309486 (93%)]\tLoss: 37718.671875\n",
      "Train Epoch: 2 [1232000/1309486 (94%)]\tLoss: 38294.800781\n",
      "Train Epoch: 2 [1240000/1309486 (95%)]\tLoss: 34777.511719\n",
      "Train Epoch: 2 [1248000/1309486 (95%)]\tLoss: 38400.218750\n",
      "Train Epoch: 2 [1256000/1309486 (96%)]\tLoss: 36148.093750\n",
      "Train Epoch: 2 [1264000/1309486 (97%)]\tLoss: 36905.617188\n",
      "Train Epoch: 2 [1272000/1309486 (97%)]\tLoss: 37510.187500\n",
      "Train Epoch: 2 [1280000/1309486 (98%)]\tLoss: 39257.070312\n",
      "Train Epoch: 2 [1288000/1309486 (98%)]\tLoss: 38540.796875\n",
      "Train Epoch: 2 [1296000/1309486 (99%)]\tLoss: 45483.589844\n",
      "Train Epoch: 2 [1304000/1309486 (100%)]\tLoss: 37062.769531\n",
      "Epoch 2 model saved!\n",
      "epoch train time:  788.86044765\n",
      "Train Epoch: 3 [8000/1309486 (1%)]\tLoss: 38743.640625\n",
      "Train Epoch: 3 [16000/1309486 (1%)]\tLoss: 40656.066406\n",
      "Train Epoch: 3 [24000/1309486 (2%)]\tLoss: 42124.144531\n",
      "Train Epoch: 3 [32000/1309486 (2%)]\tLoss: 38445.003906\n",
      "Train Epoch: 3 [40000/1309486 (3%)]\tLoss: 38392.437500\n",
      "Train Epoch: 3 [48000/1309486 (4%)]\tLoss: 39728.269531\n",
      "Train Epoch: 3 [56000/1309486 (4%)]\tLoss: 39687.570312\n",
      "Train Epoch: 3 [64000/1309486 (5%)]\tLoss: 37458.570312\n",
      "Train Epoch: 3 [72000/1309486 (5%)]\tLoss: 35951.585938\n",
      "Train Epoch: 3 [80000/1309486 (6%)]\tLoss: 41582.851562\n",
      "Train Epoch: 3 [88000/1309486 (7%)]\tLoss: 36611.875000\n",
      "Train Epoch: 3 [96000/1309486 (7%)]\tLoss: 37804.507812\n",
      "Train Epoch: 3 [104000/1309486 (8%)]\tLoss: 39948.757812\n",
      "Train Epoch: 3 [112000/1309486 (9%)]\tLoss: 40604.773438\n",
      "Train Epoch: 3 [120000/1309486 (9%)]\tLoss: 35940.929688\n",
      "Train Epoch: 3 [128000/1309486 (10%)]\tLoss: 37801.230469\n",
      "Train Epoch: 3 [136000/1309486 (10%)]\tLoss: 39125.593750\n",
      "Train Epoch: 3 [144000/1309486 (11%)]\tLoss: 39023.851562\n",
      "Train Epoch: 3 [152000/1309486 (12%)]\tLoss: 38409.953125\n",
      "Train Epoch: 3 [160000/1309486 (12%)]\tLoss: 38795.792969\n",
      "Train Epoch: 3 [168000/1309486 (13%)]\tLoss: 42359.843750\n",
      "Train Epoch: 3 [176000/1309486 (13%)]\tLoss: 39221.273438\n",
      "Train Epoch: 3 [184000/1309486 (14%)]\tLoss: 37796.597656\n",
      "Train Epoch: 3 [192000/1309486 (15%)]\tLoss: 37724.773438\n",
      "Train Epoch: 3 [200000/1309486 (15%)]\tLoss: 40272.230469\n",
      "Train Epoch: 3 [208000/1309486 (16%)]\tLoss: 38065.808594\n",
      "Train Epoch: 3 [216000/1309486 (16%)]\tLoss: 41513.250000\n",
      "Train Epoch: 3 [224000/1309486 (17%)]\tLoss: 37971.906250\n",
      "Train Epoch: 3 [232000/1309486 (18%)]\tLoss: 37747.972656\n",
      "Train Epoch: 3 [240000/1309486 (18%)]\tLoss: 38577.210938\n",
      "Train Epoch: 3 [248000/1309486 (19%)]\tLoss: 43549.808594\n",
      "Train Epoch: 3 [256000/1309486 (20%)]\tLoss: 38474.230469\n",
      "Train Epoch: 3 [264000/1309486 (20%)]\tLoss: 36143.132812\n",
      "Train Epoch: 3 [272000/1309486 (21%)]\tLoss: 38397.671875\n",
      "Train Epoch: 3 [280000/1309486 (21%)]\tLoss: 36625.519531\n",
      "Train Epoch: 3 [288000/1309486 (22%)]\tLoss: 37948.179688\n",
      "Train Epoch: 3 [296000/1309486 (23%)]\tLoss: 38997.085938\n",
      "Train Epoch: 3 [304000/1309486 (23%)]\tLoss: 39812.648438\n",
      "Train Epoch: 3 [312000/1309486 (24%)]\tLoss: 37566.929688\n",
      "Train Epoch: 3 [320000/1309486 (24%)]\tLoss: 38204.210938\n",
      "Train Epoch: 3 [328000/1309486 (25%)]\tLoss: 40917.609375\n",
      "Train Epoch: 3 [336000/1309486 (26%)]\tLoss: 36039.539062\n",
      "Train Epoch: 3 [344000/1309486 (26%)]\tLoss: 37020.203125\n",
      "Train Epoch: 3 [352000/1309486 (27%)]\tLoss: 36691.515625\n",
      "Train Epoch: 3 [360000/1309486 (27%)]\tLoss: 38259.468750\n",
      "Train Epoch: 3 [368000/1309486 (28%)]\tLoss: 39106.371094\n",
      "Train Epoch: 3 [376000/1309486 (29%)]\tLoss: 40939.589844\n",
      "Train Epoch: 3 [384000/1309486 (29%)]\tLoss: 36630.625000\n",
      "Train Epoch: 3 [392000/1309486 (30%)]\tLoss: 36067.222656\n",
      "Train Epoch: 3 [400000/1309486 (31%)]\tLoss: 35776.117188\n",
      "Train Epoch: 3 [408000/1309486 (31%)]\tLoss: 40156.281250\n",
      "Train Epoch: 3 [416000/1309486 (32%)]\tLoss: 39185.273438\n",
      "Train Epoch: 3 [424000/1309486 (32%)]\tLoss: 38164.156250\n",
      "Train Epoch: 3 [432000/1309486 (33%)]\tLoss: 39002.050781\n",
      "Train Epoch: 3 [440000/1309486 (34%)]\tLoss: 37050.062500\n",
      "Train Epoch: 3 [448000/1309486 (34%)]\tLoss: 36122.738281\n",
      "Train Epoch: 3 [456000/1309486 (35%)]\tLoss: 40948.511719\n",
      "Train Epoch: 3 [464000/1309486 (35%)]\tLoss: 36912.710938\n",
      "Train Epoch: 3 [472000/1309486 (36%)]\tLoss: 41357.976562\n",
      "Train Epoch: 3 [480000/1309486 (37%)]\tLoss: 40436.164062\n",
      "Train Epoch: 3 [488000/1309486 (37%)]\tLoss: 35717.460938\n",
      "Train Epoch: 3 [496000/1309486 (38%)]\tLoss: 36239.976562\n",
      "Train Epoch: 3 [504000/1309486 (38%)]\tLoss: 39106.984375\n",
      "Train Epoch: 3 [512000/1309486 (39%)]\tLoss: 40070.828125\n",
      "Train Epoch: 3 [520000/1309486 (40%)]\tLoss: 41286.156250\n",
      "Train Epoch: 3 [528000/1309486 (40%)]\tLoss: 36985.046875\n",
      "Train Epoch: 3 [536000/1309486 (41%)]\tLoss: 37206.070312\n",
      "Train Epoch: 3 [544000/1309486 (42%)]\tLoss: 37891.050781\n",
      "Train Epoch: 3 [552000/1309486 (42%)]\tLoss: 36943.996094\n",
      "Train Epoch: 3 [560000/1309486 (43%)]\tLoss: 35808.769531\n",
      "Train Epoch: 3 [568000/1309486 (43%)]\tLoss: 46289.468750\n",
      "Train Epoch: 3 [576000/1309486 (44%)]\tLoss: 35179.878906\n",
      "Train Epoch: 3 [584000/1309486 (45%)]\tLoss: 36858.132812\n",
      "Train Epoch: 3 [592000/1309486 (45%)]\tLoss: 37091.859375\n",
      "Train Epoch: 3 [600000/1309486 (46%)]\tLoss: 38430.914062\n",
      "Train Epoch: 3 [608000/1309486 (46%)]\tLoss: 36589.796875\n",
      "Train Epoch: 3 [616000/1309486 (47%)]\tLoss: 41191.324219\n",
      "Train Epoch: 3 [624000/1309486 (48%)]\tLoss: 35270.156250\n",
      "Train Epoch: 3 [632000/1309486 (48%)]\tLoss: 37545.843750\n",
      "Train Epoch: 3 [640000/1309486 (49%)]\tLoss: 36129.718750\n",
      "Train Epoch: 3 [648000/1309486 (49%)]\tLoss: 39396.312500\n",
      "Train Epoch: 3 [656000/1309486 (50%)]\tLoss: 34491.320312\n",
      "Train Epoch: 3 [664000/1309486 (51%)]\tLoss: 37981.062500\n",
      "Train Epoch: 3 [672000/1309486 (51%)]\tLoss: 39344.351562\n",
      "Train Epoch: 3 [680000/1309486 (52%)]\tLoss: 36470.179688\n",
      "Train Epoch: 3 [688000/1309486 (53%)]\tLoss: 36980.890625\n",
      "Train Epoch: 3 [696000/1309486 (53%)]\tLoss: 36885.890625\n",
      "Train Epoch: 3 [704000/1309486 (54%)]\tLoss: 40352.882812\n",
      "Train Epoch: 3 [712000/1309486 (54%)]\tLoss: 37068.718750\n",
      "Train Epoch: 3 [720000/1309486 (55%)]\tLoss: 33810.777344\n",
      "Train Epoch: 3 [728000/1309486 (56%)]\tLoss: 42591.867188\n",
      "Train Epoch: 3 [736000/1309486 (56%)]\tLoss: 36138.675781\n",
      "Train Epoch: 3 [744000/1309486 (57%)]\tLoss: 38921.484375\n",
      "Train Epoch: 3 [752000/1309486 (57%)]\tLoss: 35642.992188\n",
      "Train Epoch: 3 [760000/1309486 (58%)]\tLoss: 42717.515625\n",
      "Train Epoch: 3 [768000/1309486 (59%)]\tLoss: 34451.148438\n",
      "Train Epoch: 3 [776000/1309486 (59%)]\tLoss: 36949.757812\n",
      "Train Epoch: 3 [784000/1309486 (60%)]\tLoss: 35765.875000\n",
      "Train Epoch: 3 [792000/1309486 (60%)]\tLoss: 39037.882812\n",
      "Train Epoch: 3 [800000/1309486 (61%)]\tLoss: 39495.457031\n",
      "Train Epoch: 3 [808000/1309486 (62%)]\tLoss: 40610.226562\n",
      "Train Epoch: 3 [816000/1309486 (62%)]\tLoss: 39862.367188\n",
      "Train Epoch: 3 [824000/1309486 (63%)]\tLoss: 40056.347656\n",
      "Train Epoch: 3 [832000/1309486 (64%)]\tLoss: 38067.371094\n",
      "Train Epoch: 3 [840000/1309486 (64%)]\tLoss: 40383.570312\n",
      "Train Epoch: 3 [848000/1309486 (65%)]\tLoss: 35998.324219\n",
      "Train Epoch: 3 [856000/1309486 (65%)]\tLoss: 36959.835938\n",
      "Train Epoch: 3 [864000/1309486 (66%)]\tLoss: 41361.156250\n",
      "Train Epoch: 3 [872000/1309486 (67%)]\tLoss: 37020.781250\n",
      "Train Epoch: 3 [880000/1309486 (67%)]\tLoss: 35644.945312\n",
      "Train Epoch: 3 [888000/1309486 (68%)]\tLoss: 41598.421875\n",
      "Train Epoch: 3 [896000/1309486 (68%)]\tLoss: 39178.312500\n",
      "Train Epoch: 3 [904000/1309486 (69%)]\tLoss: 37281.070312\n",
      "Train Epoch: 3 [912000/1309486 (70%)]\tLoss: 37962.093750\n",
      "Train Epoch: 3 [920000/1309486 (70%)]\tLoss: 39947.210938\n",
      "Train Epoch: 3 [928000/1309486 (71%)]\tLoss: 41123.535156\n",
      "Train Epoch: 3 [936000/1309486 (71%)]\tLoss: 44203.132812\n",
      "Train Epoch: 3 [944000/1309486 (72%)]\tLoss: 35169.343750\n",
      "Train Epoch: 3 [952000/1309486 (73%)]\tLoss: 37908.429688\n",
      "Train Epoch: 3 [960000/1309486 (73%)]\tLoss: 37109.554688\n",
      "Train Epoch: 3 [968000/1309486 (74%)]\tLoss: 37850.714844\n",
      "Train Epoch: 3 [976000/1309486 (75%)]\tLoss: 38295.148438\n",
      "Train Epoch: 3 [984000/1309486 (75%)]\tLoss: 38767.511719\n",
      "Train Epoch: 3 [992000/1309486 (76%)]\tLoss: 33844.359375\n",
      "Train Epoch: 3 [1000000/1309486 (76%)]\tLoss: 37353.046875\n",
      "Train Epoch: 3 [1008000/1309486 (77%)]\tLoss: 36364.027344\n",
      "Train Epoch: 3 [1016000/1309486 (78%)]\tLoss: 39424.703125\n",
      "Train Epoch: 3 [1024000/1309486 (78%)]\tLoss: 35886.332031\n",
      "Train Epoch: 3 [1032000/1309486 (79%)]\tLoss: 36265.011719\n",
      "Train Epoch: 3 [1040000/1309486 (79%)]\tLoss: 33592.312500\n",
      "Train Epoch: 3 [1048000/1309486 (80%)]\tLoss: 44831.902344\n",
      "Train Epoch: 3 [1056000/1309486 (81%)]\tLoss: 36983.304688\n",
      "Train Epoch: 3 [1064000/1309486 (81%)]\tLoss: 42492.664062\n",
      "Train Epoch: 3 [1072000/1309486 (82%)]\tLoss: 41803.945312\n",
      "Train Epoch: 3 [1080000/1309486 (82%)]\tLoss: 36234.277344\n",
      "Train Epoch: 3 [1088000/1309486 (83%)]\tLoss: 37837.730469\n",
      "Train Epoch: 3 [1096000/1309486 (84%)]\tLoss: 38122.507812\n",
      "Train Epoch: 3 [1104000/1309486 (84%)]\tLoss: 37705.320312\n",
      "Train Epoch: 3 [1112000/1309486 (85%)]\tLoss: 43821.375000\n",
      "Train Epoch: 3 [1120000/1309486 (86%)]\tLoss: 38259.046875\n",
      "Train Epoch: 3 [1128000/1309486 (86%)]\tLoss: 40139.164062\n",
      "Train Epoch: 3 [1136000/1309486 (87%)]\tLoss: 40994.156250\n",
      "Train Epoch: 3 [1144000/1309486 (87%)]\tLoss: 39055.074219\n",
      "Train Epoch: 3 [1152000/1309486 (88%)]\tLoss: 39281.542969\n",
      "Train Epoch: 3 [1160000/1309486 (89%)]\tLoss: 37838.773438\n",
      "Train Epoch: 3 [1168000/1309486 (89%)]\tLoss: 38901.242188\n",
      "Train Epoch: 3 [1176000/1309486 (90%)]\tLoss: 33707.769531\n",
      "Train Epoch: 3 [1184000/1309486 (90%)]\tLoss: 39519.359375\n",
      "Train Epoch: 3 [1192000/1309486 (91%)]\tLoss: 39599.640625\n",
      "Train Epoch: 3 [1200000/1309486 (92%)]\tLoss: 38454.015625\n",
      "Train Epoch: 3 [1208000/1309486 (92%)]\tLoss: 39883.929688\n",
      "Train Epoch: 3 [1216000/1309486 (93%)]\tLoss: 35094.269531\n",
      "Train Epoch: 3 [1224000/1309486 (93%)]\tLoss: 37565.007812\n",
      "Train Epoch: 3 [1232000/1309486 (94%)]\tLoss: 40540.523438\n",
      "Train Epoch: 3 [1240000/1309486 (95%)]\tLoss: 38855.367188\n",
      "Train Epoch: 3 [1248000/1309486 (95%)]\tLoss: 39580.007812\n",
      "Train Epoch: 3 [1256000/1309486 (96%)]\tLoss: 37399.218750\n",
      "Train Epoch: 3 [1264000/1309486 (97%)]\tLoss: 34979.609375\n",
      "Train Epoch: 3 [1272000/1309486 (97%)]\tLoss: 35093.617188\n",
      "Train Epoch: 3 [1280000/1309486 (98%)]\tLoss: 37260.578125\n",
      "Train Epoch: 3 [1288000/1309486 (98%)]\tLoss: 39364.945312\n",
      "Train Epoch: 3 [1296000/1309486 (99%)]\tLoss: 35702.121094\n",
      "Train Epoch: 3 [1304000/1309486 (100%)]\tLoss: 38941.035156\n",
      "Epoch 3 model saved!\n",
      "epoch train time:  786.20360875\n",
      "Train Epoch: 4 [8000/1309486 (1%)]\tLoss: 37431.667969\n",
      "Train Epoch: 4 [16000/1309486 (1%)]\tLoss: 36336.324219\n",
      "Train Epoch: 4 [24000/1309486 (2%)]\tLoss: 39944.835938\n",
      "Train Epoch: 4 [32000/1309486 (2%)]\tLoss: 40574.003906\n",
      "Train Epoch: 4 [40000/1309486 (3%)]\tLoss: 39627.042969\n",
      "Train Epoch: 4 [48000/1309486 (4%)]\tLoss: 40377.402344\n",
      "Train Epoch: 4 [56000/1309486 (4%)]\tLoss: 38123.320312\n",
      "Train Epoch: 4 [64000/1309486 (5%)]\tLoss: 33870.308594\n",
      "Train Epoch: 4 [72000/1309486 (5%)]\tLoss: 40194.441406\n",
      "Train Epoch: 4 [80000/1309486 (6%)]\tLoss: 39777.843750\n",
      "Train Epoch: 4 [88000/1309486 (7%)]\tLoss: 38850.613281\n",
      "Train Epoch: 4 [96000/1309486 (7%)]\tLoss: 40670.476562\n",
      "Train Epoch: 4 [104000/1309486 (8%)]\tLoss: 43616.468750\n",
      "Train Epoch: 4 [112000/1309486 (9%)]\tLoss: 40223.796875\n",
      "Train Epoch: 4 [120000/1309486 (9%)]\tLoss: 37915.578125\n",
      "Train Epoch: 4 [128000/1309486 (10%)]\tLoss: 45258.699219\n",
      "Train Epoch: 4 [136000/1309486 (10%)]\tLoss: 38794.140625\n",
      "Train Epoch: 4 [144000/1309486 (11%)]\tLoss: 36960.664062\n",
      "Train Epoch: 4 [152000/1309486 (12%)]\tLoss: 37662.851562\n",
      "Train Epoch: 4 [160000/1309486 (12%)]\tLoss: 36062.054688\n",
      "Train Epoch: 4 [168000/1309486 (13%)]\tLoss: 42092.328125\n",
      "Train Epoch: 4 [176000/1309486 (13%)]\tLoss: 37461.964844\n",
      "Train Epoch: 4 [184000/1309486 (14%)]\tLoss: 36193.375000\n",
      "Train Epoch: 4 [192000/1309486 (15%)]\tLoss: 36746.500000\n",
      "Train Epoch: 4 [200000/1309486 (15%)]\tLoss: 40620.117188\n",
      "Train Epoch: 4 [208000/1309486 (16%)]\tLoss: 35326.863281\n",
      "Train Epoch: 4 [216000/1309486 (16%)]\tLoss: 35326.156250\n",
      "Train Epoch: 4 [224000/1309486 (17%)]\tLoss: 41998.625000\n",
      "Train Epoch: 4 [232000/1309486 (18%)]\tLoss: 39914.285156\n",
      "Train Epoch: 4 [240000/1309486 (18%)]\tLoss: 36532.175781\n",
      "Train Epoch: 4 [248000/1309486 (19%)]\tLoss: 36196.945312\n",
      "Train Epoch: 4 [256000/1309486 (20%)]\tLoss: 39862.953125\n",
      "Train Epoch: 4 [264000/1309486 (20%)]\tLoss: 39863.257812\n",
      "Train Epoch: 4 [272000/1309486 (21%)]\tLoss: 41753.960938\n",
      "Train Epoch: 4 [280000/1309486 (21%)]\tLoss: 38405.726562\n",
      "Train Epoch: 4 [288000/1309486 (22%)]\tLoss: 37500.648438\n",
      "Train Epoch: 4 [296000/1309486 (23%)]\tLoss: 37886.468750\n",
      "Train Epoch: 4 [304000/1309486 (23%)]\tLoss: 42275.281250\n",
      "Train Epoch: 4 [312000/1309486 (24%)]\tLoss: 37013.625000\n",
      "Train Epoch: 4 [320000/1309486 (24%)]\tLoss: 36837.054688\n",
      "Train Epoch: 4 [328000/1309486 (25%)]\tLoss: 38012.656250\n",
      "Train Epoch: 4 [336000/1309486 (26%)]\tLoss: 37273.476562\n",
      "Train Epoch: 4 [344000/1309486 (26%)]\tLoss: 39467.878906\n",
      "Train Epoch: 4 [352000/1309486 (27%)]\tLoss: 40891.859375\n",
      "Train Epoch: 4 [360000/1309486 (27%)]\tLoss: 38597.289062\n",
      "Train Epoch: 4 [368000/1309486 (28%)]\tLoss: 38684.945312\n",
      "Train Epoch: 4 [376000/1309486 (29%)]\tLoss: 36591.429688\n",
      "Train Epoch: 4 [384000/1309486 (29%)]\tLoss: 34598.339844\n",
      "Train Epoch: 4 [392000/1309486 (30%)]\tLoss: 37964.500000\n",
      "Train Epoch: 4 [400000/1309486 (31%)]\tLoss: 38723.191406\n",
      "Train Epoch: 4 [408000/1309486 (31%)]\tLoss: 40990.292969\n",
      "Train Epoch: 4 [416000/1309486 (32%)]\tLoss: 41595.292969\n",
      "Train Epoch: 4 [424000/1309486 (32%)]\tLoss: 37870.785156\n",
      "Train Epoch: 4 [432000/1309486 (33%)]\tLoss: 37872.800781\n",
      "Train Epoch: 4 [440000/1309486 (34%)]\tLoss: 38608.687500\n",
      "Train Epoch: 4 [448000/1309486 (34%)]\tLoss: 40148.613281\n",
      "Train Epoch: 4 [456000/1309486 (35%)]\tLoss: 37879.039062\n",
      "Train Epoch: 4 [464000/1309486 (35%)]\tLoss: 39746.382812\n",
      "Train Epoch: 4 [472000/1309486 (36%)]\tLoss: 42355.070312\n",
      "Train Epoch: 4 [480000/1309486 (37%)]\tLoss: 36079.507812\n",
      "Train Epoch: 4 [488000/1309486 (37%)]\tLoss: 37276.117188\n",
      "Train Epoch: 4 [496000/1309486 (38%)]\tLoss: 38212.648438\n",
      "Train Epoch: 4 [504000/1309486 (38%)]\tLoss: 39101.781250\n",
      "Train Epoch: 4 [512000/1309486 (39%)]\tLoss: 33932.406250\n",
      "Train Epoch: 4 [520000/1309486 (40%)]\tLoss: 38780.378906\n",
      "Train Epoch: 4 [528000/1309486 (40%)]\tLoss: 35850.773438\n",
      "Train Epoch: 4 [536000/1309486 (41%)]\tLoss: 37970.910156\n",
      "Train Epoch: 4 [544000/1309486 (42%)]\tLoss: 35121.160156\n",
      "Train Epoch: 4 [552000/1309486 (42%)]\tLoss: 38735.859375\n",
      "Train Epoch: 4 [560000/1309486 (43%)]\tLoss: 37983.847656\n",
      "Train Epoch: 4 [568000/1309486 (43%)]\tLoss: 38322.093750\n",
      "Train Epoch: 4 [576000/1309486 (44%)]\tLoss: 43436.628906\n",
      "Train Epoch: 4 [584000/1309486 (45%)]\tLoss: 41258.867188\n",
      "Train Epoch: 4 [592000/1309486 (45%)]\tLoss: 35459.308594\n",
      "Train Epoch: 4 [600000/1309486 (46%)]\tLoss: 38349.070312\n",
      "Train Epoch: 4 [608000/1309486 (46%)]\tLoss: 41885.039062\n",
      "Train Epoch: 4 [616000/1309486 (47%)]\tLoss: 36499.507812\n",
      "Train Epoch: 4 [624000/1309486 (48%)]\tLoss: 39574.218750\n",
      "Train Epoch: 4 [632000/1309486 (48%)]\tLoss: 38936.718750\n",
      "Train Epoch: 4 [640000/1309486 (49%)]\tLoss: 41261.773438\n",
      "Train Epoch: 4 [648000/1309486 (49%)]\tLoss: 36434.335938\n",
      "Train Epoch: 4 [656000/1309486 (50%)]\tLoss: 37379.070312\n",
      "Train Epoch: 4 [664000/1309486 (51%)]\tLoss: 41815.824219\n",
      "Train Epoch: 4 [672000/1309486 (51%)]\tLoss: 34526.554688\n",
      "Train Epoch: 4 [680000/1309486 (52%)]\tLoss: 36858.531250\n",
      "Train Epoch: 4 [688000/1309486 (53%)]\tLoss: 38939.148438\n",
      "Train Epoch: 4 [696000/1309486 (53%)]\tLoss: 35323.062500\n",
      "Train Epoch: 4 [704000/1309486 (54%)]\tLoss: 31480.378906\n",
      "Train Epoch: 4 [712000/1309486 (54%)]\tLoss: 41724.769531\n",
      "Train Epoch: 4 [720000/1309486 (55%)]\tLoss: 38355.468750\n",
      "Train Epoch: 4 [728000/1309486 (56%)]\tLoss: 39660.984375\n",
      "Train Epoch: 4 [736000/1309486 (56%)]\tLoss: 39740.101562\n",
      "Train Epoch: 4 [744000/1309486 (57%)]\tLoss: 37749.417969\n",
      "Train Epoch: 4 [752000/1309486 (57%)]\tLoss: 41341.066406\n",
      "Train Epoch: 4 [760000/1309486 (58%)]\tLoss: 38993.746094\n",
      "Train Epoch: 4 [768000/1309486 (59%)]\tLoss: 42349.355469\n",
      "Train Epoch: 4 [776000/1309486 (59%)]\tLoss: 35501.742188\n",
      "Train Epoch: 4 [784000/1309486 (60%)]\tLoss: 37909.808594\n",
      "Train Epoch: 4 [792000/1309486 (60%)]\tLoss: 36920.343750\n",
      "Train Epoch: 4 [800000/1309486 (61%)]\tLoss: 38483.597656\n",
      "Train Epoch: 4 [808000/1309486 (62%)]\tLoss: 38071.500000\n",
      "Train Epoch: 4 [816000/1309486 (62%)]\tLoss: 40924.695312\n",
      "Train Epoch: 4 [824000/1309486 (63%)]\tLoss: 39031.789062\n",
      "Train Epoch: 4 [832000/1309486 (64%)]\tLoss: 38921.441406\n",
      "Train Epoch: 4 [840000/1309486 (64%)]\tLoss: 37109.765625\n",
      "Train Epoch: 4 [848000/1309486 (65%)]\tLoss: 37303.773438\n",
      "Train Epoch: 4 [856000/1309486 (65%)]\tLoss: 41576.722656\n",
      "Train Epoch: 4 [864000/1309486 (66%)]\tLoss: 39798.089844\n",
      "Train Epoch: 4 [872000/1309486 (67%)]\tLoss: 40394.742188\n",
      "Train Epoch: 4 [880000/1309486 (67%)]\tLoss: 39580.890625\n",
      "Train Epoch: 4 [888000/1309486 (68%)]\tLoss: 40243.824219\n",
      "Train Epoch: 4 [896000/1309486 (68%)]\tLoss: 41351.242188\n",
      "Train Epoch: 4 [904000/1309486 (69%)]\tLoss: 36743.039062\n",
      "Train Epoch: 4 [912000/1309486 (70%)]\tLoss: 40753.546875\n",
      "Train Epoch: 4 [920000/1309486 (70%)]\tLoss: 37659.601562\n",
      "Train Epoch: 4 [928000/1309486 (71%)]\tLoss: 45329.531250\n",
      "Train Epoch: 4 [936000/1309486 (71%)]\tLoss: 38830.210938\n",
      "Train Epoch: 4 [944000/1309486 (72%)]\tLoss: 41772.593750\n",
      "Train Epoch: 4 [952000/1309486 (73%)]\tLoss: 36085.226562\n",
      "Train Epoch: 4 [960000/1309486 (73%)]\tLoss: 33532.789062\n",
      "Train Epoch: 4 [968000/1309486 (74%)]\tLoss: 40701.273438\n",
      "Train Epoch: 4 [976000/1309486 (75%)]\tLoss: 36292.613281\n",
      "Train Epoch: 4 [984000/1309486 (75%)]\tLoss: 41936.718750\n",
      "Train Epoch: 4 [992000/1309486 (76%)]\tLoss: 41856.679688\n",
      "Train Epoch: 4 [1000000/1309486 (76%)]\tLoss: 38604.335938\n",
      "Train Epoch: 4 [1008000/1309486 (77%)]\tLoss: 38605.558594\n",
      "Train Epoch: 4 [1016000/1309486 (78%)]\tLoss: 35696.789062\n",
      "Train Epoch: 4 [1024000/1309486 (78%)]\tLoss: 37136.320312\n",
      "Train Epoch: 4 [1032000/1309486 (79%)]\tLoss: 41022.335938\n",
      "Train Epoch: 4 [1040000/1309486 (79%)]\tLoss: 37852.894531\n",
      "Train Epoch: 4 [1048000/1309486 (80%)]\tLoss: 35089.984375\n",
      "Train Epoch: 4 [1056000/1309486 (81%)]\tLoss: 37695.578125\n",
      "Train Epoch: 4 [1064000/1309486 (81%)]\tLoss: 42039.523438\n",
      "Train Epoch: 4 [1072000/1309486 (82%)]\tLoss: 38473.585938\n",
      "Train Epoch: 4 [1080000/1309486 (82%)]\tLoss: 40591.703125\n",
      "Train Epoch: 4 [1088000/1309486 (83%)]\tLoss: 41098.773438\n",
      "Train Epoch: 4 [1096000/1309486 (84%)]\tLoss: 40953.609375\n",
      "Train Epoch: 4 [1104000/1309486 (84%)]\tLoss: 40770.535156\n",
      "Train Epoch: 4 [1112000/1309486 (85%)]\tLoss: 38967.449219\n",
      "Train Epoch: 4 [1120000/1309486 (86%)]\tLoss: 36826.867188\n",
      "Train Epoch: 4 [1128000/1309486 (86%)]\tLoss: 37055.609375\n",
      "Train Epoch: 4 [1136000/1309486 (87%)]\tLoss: 43031.562500\n",
      "Train Epoch: 4 [1144000/1309486 (87%)]\tLoss: 39075.421875\n",
      "Train Epoch: 4 [1152000/1309486 (88%)]\tLoss: 36246.816406\n",
      "Train Epoch: 4 [1160000/1309486 (89%)]\tLoss: 39614.269531\n",
      "Train Epoch: 4 [1168000/1309486 (89%)]\tLoss: 36112.132812\n",
      "Train Epoch: 4 [1176000/1309486 (90%)]\tLoss: 40288.273438\n",
      "Train Epoch: 4 [1184000/1309486 (90%)]\tLoss: 36763.359375\n",
      "Train Epoch: 4 [1192000/1309486 (91%)]\tLoss: 43376.812500\n",
      "Train Epoch: 4 [1200000/1309486 (92%)]\tLoss: 35954.929688\n",
      "Train Epoch: 4 [1208000/1309486 (92%)]\tLoss: 38520.390625\n",
      "Train Epoch: 4 [1216000/1309486 (93%)]\tLoss: 39431.562500\n",
      "Train Epoch: 4 [1224000/1309486 (93%)]\tLoss: 38842.457031\n",
      "Train Epoch: 4 [1232000/1309486 (94%)]\tLoss: 36557.601562\n",
      "Train Epoch: 4 [1240000/1309486 (95%)]\tLoss: 39954.160156\n",
      "Train Epoch: 4 [1248000/1309486 (95%)]\tLoss: 38909.117188\n",
      "Train Epoch: 4 [1256000/1309486 (96%)]\tLoss: 37564.445312\n",
      "Train Epoch: 4 [1264000/1309486 (97%)]\tLoss: 35930.296875\n",
      "Train Epoch: 4 [1272000/1309486 (97%)]\tLoss: 36578.125000\n",
      "Train Epoch: 4 [1280000/1309486 (98%)]\tLoss: 36107.710938\n",
      "Train Epoch: 4 [1288000/1309486 (98%)]\tLoss: 37515.507812\n",
      "Train Epoch: 4 [1296000/1309486 (99%)]\tLoss: 42600.171875\n",
      "Train Epoch: 4 [1304000/1309486 (100%)]\tLoss: 37582.507812\n",
      "Epoch 4 model saved!\n",
      "epoch train time:  786.51900673\n",
      "Train Epoch: 5 [8000/1309486 (1%)]\tLoss: 39116.468750\n",
      "Train Epoch: 5 [16000/1309486 (1%)]\tLoss: 36175.460938\n",
      "Train Epoch: 5 [24000/1309486 (2%)]\tLoss: 36771.562500\n",
      "Train Epoch: 5 [32000/1309486 (2%)]\tLoss: 42375.738281\n",
      "Train Epoch: 5 [40000/1309486 (3%)]\tLoss: 37698.472656\n",
      "Train Epoch: 5 [48000/1309486 (4%)]\tLoss: 37354.234375\n",
      "Train Epoch: 5 [56000/1309486 (4%)]\tLoss: 41719.492188\n",
      "Train Epoch: 5 [64000/1309486 (5%)]\tLoss: 38323.265625\n",
      "Train Epoch: 5 [72000/1309486 (5%)]\tLoss: 38570.820312\n",
      "Train Epoch: 5 [80000/1309486 (6%)]\tLoss: 38968.617188\n",
      "Train Epoch: 5 [88000/1309486 (7%)]\tLoss: 38820.226562\n",
      "Train Epoch: 5 [96000/1309486 (7%)]\tLoss: 37472.539062\n",
      "Train Epoch: 5 [104000/1309486 (8%)]\tLoss: 45092.156250\n",
      "Train Epoch: 5 [112000/1309486 (9%)]\tLoss: 35522.500000\n",
      "Train Epoch: 5 [120000/1309486 (9%)]\tLoss: 38083.511719\n",
      "Train Epoch: 5 [128000/1309486 (10%)]\tLoss: 43265.265625\n",
      "Train Epoch: 5 [136000/1309486 (10%)]\tLoss: 37858.984375\n",
      "Train Epoch: 5 [144000/1309486 (11%)]\tLoss: 36795.535156\n",
      "Train Epoch: 5 [152000/1309486 (12%)]\tLoss: 33062.335938\n",
      "Train Epoch: 5 [160000/1309486 (12%)]\tLoss: 39314.515625\n",
      "Train Epoch: 5 [168000/1309486 (13%)]\tLoss: 37031.492188\n",
      "Train Epoch: 5 [176000/1309486 (13%)]\tLoss: 35436.433594\n",
      "Train Epoch: 5 [184000/1309486 (14%)]\tLoss: 39701.101562\n",
      "Train Epoch: 5 [192000/1309486 (15%)]\tLoss: 40151.789062\n",
      "Train Epoch: 5 [200000/1309486 (15%)]\tLoss: 39019.214844\n",
      "Train Epoch: 5 [208000/1309486 (16%)]\tLoss: 37571.910156\n",
      "Train Epoch: 5 [216000/1309486 (16%)]\tLoss: 35834.171875\n",
      "Train Epoch: 5 [224000/1309486 (17%)]\tLoss: 36769.320312\n",
      "Train Epoch: 5 [232000/1309486 (18%)]\tLoss: 40899.429688\n",
      "Train Epoch: 5 [240000/1309486 (18%)]\tLoss: 38833.820312\n",
      "Train Epoch: 5 [248000/1309486 (19%)]\tLoss: 39446.980469\n",
      "Train Epoch: 5 [256000/1309486 (20%)]\tLoss: 37536.273438\n",
      "Train Epoch: 5 [264000/1309486 (20%)]\tLoss: 37965.968750\n",
      "Train Epoch: 5 [272000/1309486 (21%)]\tLoss: 38128.402344\n",
      "Train Epoch: 5 [280000/1309486 (21%)]\tLoss: 43588.839844\n",
      "Train Epoch: 5 [288000/1309486 (22%)]\tLoss: 37240.410156\n",
      "Train Epoch: 5 [296000/1309486 (23%)]\tLoss: 35800.375000\n",
      "Train Epoch: 5 [304000/1309486 (23%)]\tLoss: 37843.273438\n",
      "Train Epoch: 5 [312000/1309486 (24%)]\tLoss: 34721.996094\n",
      "Train Epoch: 5 [320000/1309486 (24%)]\tLoss: 36406.652344\n",
      "Train Epoch: 5 [328000/1309486 (25%)]\tLoss: 39601.273438\n",
      "Train Epoch: 5 [336000/1309486 (26%)]\tLoss: 46261.273438\n",
      "Train Epoch: 5 [344000/1309486 (26%)]\tLoss: 35936.125000\n",
      "Train Epoch: 5 [352000/1309486 (27%)]\tLoss: 39734.906250\n",
      "Train Epoch: 5 [360000/1309486 (27%)]\tLoss: 34843.632812\n",
      "Train Epoch: 5 [368000/1309486 (28%)]\tLoss: 38312.218750\n",
      "Train Epoch: 5 [376000/1309486 (29%)]\tLoss: 38231.515625\n",
      "Train Epoch: 5 [384000/1309486 (29%)]\tLoss: 40026.488281\n",
      "Train Epoch: 5 [392000/1309486 (30%)]\tLoss: 40348.250000\n",
      "Train Epoch: 5 [400000/1309486 (31%)]\tLoss: 40009.324219\n",
      "Train Epoch: 5 [408000/1309486 (31%)]\tLoss: 37056.656250\n",
      "Train Epoch: 5 [416000/1309486 (32%)]\tLoss: 42055.132812\n",
      "Train Epoch: 5 [424000/1309486 (32%)]\tLoss: 39256.992188\n",
      "Train Epoch: 5 [432000/1309486 (33%)]\tLoss: 40455.773438\n",
      "Train Epoch: 5 [440000/1309486 (34%)]\tLoss: 41788.796875\n",
      "Train Epoch: 5 [448000/1309486 (34%)]\tLoss: 35655.992188\n",
      "Train Epoch: 5 [456000/1309486 (35%)]\tLoss: 36098.167969\n",
      "Train Epoch: 5 [464000/1309486 (35%)]\tLoss: 39374.605469\n",
      "Train Epoch: 5 [472000/1309486 (36%)]\tLoss: 35712.789062\n",
      "Train Epoch: 5 [480000/1309486 (37%)]\tLoss: 40134.023438\n",
      "Train Epoch: 5 [488000/1309486 (37%)]\tLoss: 45655.281250\n",
      "Train Epoch: 5 [496000/1309486 (38%)]\tLoss: 40655.796875\n",
      "Train Epoch: 5 [504000/1309486 (38%)]\tLoss: 37630.132812\n",
      "Train Epoch: 5 [512000/1309486 (39%)]\tLoss: 37056.109375\n",
      "Train Epoch: 5 [520000/1309486 (40%)]\tLoss: 35057.136719\n",
      "Train Epoch: 5 [528000/1309486 (40%)]\tLoss: 39118.882812\n",
      "Train Epoch: 5 [536000/1309486 (41%)]\tLoss: 37920.242188\n",
      "Train Epoch: 5 [544000/1309486 (42%)]\tLoss: 37897.382812\n",
      "Train Epoch: 5 [552000/1309486 (42%)]\tLoss: 39923.371094\n",
      "Train Epoch: 5 [560000/1309486 (43%)]\tLoss: 43009.121094\n",
      "Train Epoch: 5 [568000/1309486 (43%)]\tLoss: 41846.820312\n",
      "Train Epoch: 5 [576000/1309486 (44%)]\tLoss: 35351.203125\n",
      "Train Epoch: 5 [584000/1309486 (45%)]\tLoss: 38271.828125\n",
      "Train Epoch: 5 [592000/1309486 (45%)]\tLoss: 41358.304688\n",
      "Train Epoch: 5 [600000/1309486 (46%)]\tLoss: 40537.289062\n",
      "Train Epoch: 5 [608000/1309486 (46%)]\tLoss: 36209.027344\n",
      "Train Epoch: 5 [616000/1309486 (47%)]\tLoss: 39135.523438\n",
      "Train Epoch: 5 [624000/1309486 (48%)]\tLoss: 40839.023438\n",
      "Train Epoch: 5 [632000/1309486 (48%)]\tLoss: 42395.101562\n",
      "Train Epoch: 5 [640000/1309486 (49%)]\tLoss: 35206.070312\n",
      "Train Epoch: 5 [648000/1309486 (49%)]\tLoss: 41442.242188\n",
      "Train Epoch: 5 [656000/1309486 (50%)]\tLoss: 35907.890625\n",
      "Train Epoch: 5 [664000/1309486 (51%)]\tLoss: 37048.277344\n",
      "Train Epoch: 5 [672000/1309486 (51%)]\tLoss: 39261.171875\n",
      "Train Epoch: 5 [680000/1309486 (52%)]\tLoss: 39283.082031\n",
      "Train Epoch: 5 [688000/1309486 (53%)]\tLoss: 37857.343750\n",
      "Train Epoch: 5 [696000/1309486 (53%)]\tLoss: 39494.726562\n",
      "Train Epoch: 5 [704000/1309486 (54%)]\tLoss: 45580.816406\n",
      "Train Epoch: 5 [712000/1309486 (54%)]\tLoss: 35361.500000\n",
      "Train Epoch: 5 [720000/1309486 (55%)]\tLoss: 37406.875000\n",
      "Train Epoch: 5 [728000/1309486 (56%)]\tLoss: 38014.679688\n",
      "Train Epoch: 5 [736000/1309486 (56%)]\tLoss: 36688.367188\n",
      "Train Epoch: 5 [744000/1309486 (57%)]\tLoss: 36162.957031\n",
      "Train Epoch: 5 [752000/1309486 (57%)]\tLoss: 40262.234375\n",
      "Train Epoch: 5 [760000/1309486 (58%)]\tLoss: 37440.617188\n",
      "Train Epoch: 5 [768000/1309486 (59%)]\tLoss: 37773.093750\n",
      "Train Epoch: 5 [776000/1309486 (59%)]\tLoss: 40338.453125\n",
      "Train Epoch: 5 [784000/1309486 (60%)]\tLoss: 36540.164062\n",
      "Train Epoch: 5 [792000/1309486 (60%)]\tLoss: 43190.125000\n",
      "Train Epoch: 5 [800000/1309486 (61%)]\tLoss: 40393.328125\n",
      "Train Epoch: 5 [808000/1309486 (62%)]\tLoss: 36731.414062\n",
      "Train Epoch: 5 [816000/1309486 (62%)]\tLoss: 37097.921875\n",
      "Train Epoch: 5 [824000/1309486 (63%)]\tLoss: 36698.898438\n",
      "Train Epoch: 5 [832000/1309486 (64%)]\tLoss: 40008.203125\n",
      "Train Epoch: 5 [840000/1309486 (64%)]\tLoss: 39528.632812\n",
      "Train Epoch: 5 [848000/1309486 (65%)]\tLoss: 40669.664062\n",
      "Train Epoch: 5 [856000/1309486 (65%)]\tLoss: 37338.179688\n",
      "Train Epoch: 5 [864000/1309486 (66%)]\tLoss: 38221.140625\n",
      "Train Epoch: 5 [872000/1309486 (67%)]\tLoss: 40944.343750\n",
      "Train Epoch: 5 [880000/1309486 (67%)]\tLoss: 40528.093750\n",
      "Train Epoch: 5 [888000/1309486 (68%)]\tLoss: 40548.363281\n",
      "Train Epoch: 5 [896000/1309486 (68%)]\tLoss: 39789.917969\n",
      "Train Epoch: 5 [904000/1309486 (69%)]\tLoss: 39077.343750\n",
      "Train Epoch: 5 [912000/1309486 (70%)]\tLoss: 36814.976562\n",
      "Train Epoch: 5 [920000/1309486 (70%)]\tLoss: 37590.367188\n",
      "Train Epoch: 5 [928000/1309486 (71%)]\tLoss: 36586.484375\n",
      "Train Epoch: 5 [936000/1309486 (71%)]\tLoss: 38201.128906\n",
      "Train Epoch: 5 [944000/1309486 (72%)]\tLoss: 36505.796875\n",
      "Train Epoch: 5 [952000/1309486 (73%)]\tLoss: 37519.296875\n",
      "Train Epoch: 5 [960000/1309486 (73%)]\tLoss: 35525.039062\n",
      "Train Epoch: 5 [968000/1309486 (74%)]\tLoss: 39101.515625\n",
      "Train Epoch: 5 [976000/1309486 (75%)]\tLoss: 39692.500000\n",
      "Train Epoch: 5 [984000/1309486 (75%)]\tLoss: 41995.894531\n",
      "Train Epoch: 5 [992000/1309486 (76%)]\tLoss: 39867.035156\n",
      "Train Epoch: 5 [1000000/1309486 (76%)]\tLoss: 39437.222656\n",
      "Train Epoch: 5 [1008000/1309486 (77%)]\tLoss: 35064.203125\n",
      "Train Epoch: 5 [1016000/1309486 (78%)]\tLoss: 37973.203125\n",
      "Train Epoch: 5 [1024000/1309486 (78%)]\tLoss: 34739.761719\n",
      "Train Epoch: 5 [1032000/1309486 (79%)]\tLoss: 37923.453125\n",
      "Train Epoch: 5 [1040000/1309486 (79%)]\tLoss: 38932.015625\n",
      "Train Epoch: 5 [1048000/1309486 (80%)]\tLoss: 38292.265625\n",
      "Train Epoch: 5 [1056000/1309486 (81%)]\tLoss: 40807.382812\n",
      "Train Epoch: 5 [1064000/1309486 (81%)]\tLoss: 39176.765625\n",
      "Train Epoch: 5 [1072000/1309486 (82%)]\tLoss: 38171.179688\n",
      "Train Epoch: 5 [1080000/1309486 (82%)]\tLoss: 35604.476562\n",
      "Train Epoch: 5 [1088000/1309486 (83%)]\tLoss: 36113.449219\n",
      "Train Epoch: 5 [1096000/1309486 (84%)]\tLoss: 42189.757812\n",
      "Train Epoch: 5 [1104000/1309486 (84%)]\tLoss: 38371.460938\n",
      "Train Epoch: 5 [1112000/1309486 (85%)]\tLoss: 31977.144531\n",
      "Train Epoch: 5 [1120000/1309486 (86%)]\tLoss: 32379.550781\n",
      "Train Epoch: 5 [1128000/1309486 (86%)]\tLoss: 36580.632812\n",
      "Train Epoch: 5 [1136000/1309486 (87%)]\tLoss: 39504.648438\n",
      "Train Epoch: 5 [1144000/1309486 (87%)]\tLoss: 37538.496094\n",
      "Train Epoch: 5 [1152000/1309486 (88%)]\tLoss: 39898.335938\n",
      "Train Epoch: 5 [1160000/1309486 (89%)]\tLoss: 34619.914062\n",
      "Train Epoch: 5 [1168000/1309486 (89%)]\tLoss: 35231.992188\n",
      "Train Epoch: 5 [1176000/1309486 (90%)]\tLoss: 34202.945312\n",
      "Train Epoch: 5 [1184000/1309486 (90%)]\tLoss: 39443.179688\n",
      "Train Epoch: 5 [1192000/1309486 (91%)]\tLoss: 36073.855469\n",
      "Train Epoch: 5 [1200000/1309486 (92%)]\tLoss: 38114.636719\n",
      "Train Epoch: 5 [1208000/1309486 (92%)]\tLoss: 34411.406250\n",
      "Train Epoch: 5 [1216000/1309486 (93%)]\tLoss: 39718.980469\n",
      "Train Epoch: 5 [1224000/1309486 (93%)]\tLoss: 39129.468750\n",
      "Train Epoch: 5 [1232000/1309486 (94%)]\tLoss: 37542.683594\n",
      "Train Epoch: 5 [1240000/1309486 (95%)]\tLoss: 41014.925781\n",
      "Train Epoch: 5 [1248000/1309486 (95%)]\tLoss: 39072.800781\n",
      "Train Epoch: 5 [1256000/1309486 (96%)]\tLoss: 36621.511719\n",
      "Train Epoch: 5 [1264000/1309486 (97%)]\tLoss: 38758.457031\n",
      "Train Epoch: 5 [1272000/1309486 (97%)]\tLoss: 33906.960938\n",
      "Train Epoch: 5 [1280000/1309486 (98%)]\tLoss: 36679.671875\n",
      "Train Epoch: 5 [1288000/1309486 (98%)]\tLoss: 37080.164062\n",
      "Train Epoch: 5 [1296000/1309486 (99%)]\tLoss: 40329.347656\n",
      "Train Epoch: 5 [1304000/1309486 (100%)]\tLoss: 36627.070312\n",
      "Epoch 5 model saved!\n",
      "epoch train time:  786.52877617\n",
      "Train Epoch: 6 [8000/1309486 (1%)]\tLoss: 40614.921875\n",
      "Train Epoch: 6 [16000/1309486 (1%)]\tLoss: 34737.527344\n",
      "Train Epoch: 6 [24000/1309486 (2%)]\tLoss: 37317.457031\n",
      "Train Epoch: 6 [32000/1309486 (2%)]\tLoss: 37792.011719\n",
      "Train Epoch: 6 [40000/1309486 (3%)]\tLoss: 38770.578125\n",
      "Train Epoch: 6 [48000/1309486 (4%)]\tLoss: 36994.941406\n",
      "Train Epoch: 6 [56000/1309486 (4%)]\tLoss: 42747.460938\n",
      "Train Epoch: 6 [64000/1309486 (5%)]\tLoss: 37809.375000\n",
      "Train Epoch: 6 [72000/1309486 (5%)]\tLoss: 39661.109375\n",
      "Train Epoch: 6 [80000/1309486 (6%)]\tLoss: 37808.988281\n",
      "Train Epoch: 6 [88000/1309486 (7%)]\tLoss: 40698.460938\n",
      "Train Epoch: 6 [96000/1309486 (7%)]\tLoss: 39180.480469\n",
      "Train Epoch: 6 [104000/1309486 (8%)]\tLoss: 36677.609375\n",
      "Train Epoch: 6 [112000/1309486 (9%)]\tLoss: 39386.468750\n",
      "Train Epoch: 6 [120000/1309486 (9%)]\tLoss: 39209.207031\n",
      "Train Epoch: 6 [128000/1309486 (10%)]\tLoss: 42362.367188\n",
      "Train Epoch: 6 [136000/1309486 (10%)]\tLoss: 37212.531250\n",
      "Train Epoch: 6 [144000/1309486 (11%)]\tLoss: 37716.460938\n",
      "Train Epoch: 6 [152000/1309486 (12%)]\tLoss: 43579.312500\n",
      "Train Epoch: 6 [160000/1309486 (12%)]\tLoss: 36743.140625\n",
      "Train Epoch: 6 [168000/1309486 (13%)]\tLoss: 40178.773438\n",
      "Train Epoch: 6 [176000/1309486 (13%)]\tLoss: 39113.203125\n",
      "Train Epoch: 6 [184000/1309486 (14%)]\tLoss: 38834.601562\n",
      "Train Epoch: 6 [192000/1309486 (15%)]\tLoss: 36497.460938\n",
      "Train Epoch: 6 [200000/1309486 (15%)]\tLoss: 37501.738281\n",
      "Train Epoch: 6 [208000/1309486 (16%)]\tLoss: 39254.937500\n",
      "Train Epoch: 6 [216000/1309486 (16%)]\tLoss: 37099.828125\n",
      "Train Epoch: 6 [224000/1309486 (17%)]\tLoss: 36090.398438\n",
      "Train Epoch: 6 [232000/1309486 (18%)]\tLoss: 39185.843750\n",
      "Train Epoch: 6 [240000/1309486 (18%)]\tLoss: 41000.949219\n",
      "Train Epoch: 6 [248000/1309486 (19%)]\tLoss: 39085.898438\n",
      "Train Epoch: 6 [256000/1309486 (20%)]\tLoss: 40280.566406\n",
      "Train Epoch: 6 [264000/1309486 (20%)]\tLoss: 39854.148438\n",
      "Train Epoch: 6 [272000/1309486 (21%)]\tLoss: 40306.656250\n",
      "Train Epoch: 6 [280000/1309486 (21%)]\tLoss: 39768.289062\n",
      "Train Epoch: 6 [288000/1309486 (22%)]\tLoss: 39707.101562\n",
      "Train Epoch: 6 [296000/1309486 (23%)]\tLoss: 39696.382812\n",
      "Train Epoch: 6 [304000/1309486 (23%)]\tLoss: 37334.343750\n",
      "Train Epoch: 6 [312000/1309486 (24%)]\tLoss: 39831.957031\n",
      "Train Epoch: 6 [320000/1309486 (24%)]\tLoss: 34532.367188\n",
      "Train Epoch: 6 [328000/1309486 (25%)]\tLoss: 38218.207031\n",
      "Train Epoch: 6 [336000/1309486 (26%)]\tLoss: 40686.789062\n",
      "Train Epoch: 6 [344000/1309486 (26%)]\tLoss: 41142.929688\n",
      "Train Epoch: 6 [352000/1309486 (27%)]\tLoss: 37384.894531\n",
      "Train Epoch: 6 [360000/1309486 (27%)]\tLoss: 36919.917969\n",
      "Train Epoch: 6 [368000/1309486 (28%)]\tLoss: 36825.437500\n",
      "Train Epoch: 6 [376000/1309486 (29%)]\tLoss: 40444.542969\n",
      "Train Epoch: 6 [384000/1309486 (29%)]\tLoss: 37531.277344\n",
      "Train Epoch: 6 [392000/1309486 (30%)]\tLoss: 37558.921875\n",
      "Train Epoch: 6 [400000/1309486 (31%)]\tLoss: 39598.984375\n",
      "Train Epoch: 6 [408000/1309486 (31%)]\tLoss: 35600.171875\n",
      "Train Epoch: 6 [416000/1309486 (32%)]\tLoss: 41755.242188\n",
      "Train Epoch: 6 [424000/1309486 (32%)]\tLoss: 36862.625000\n",
      "Train Epoch: 6 [432000/1309486 (33%)]\tLoss: 37017.046875\n",
      "Train Epoch: 6 [440000/1309486 (34%)]\tLoss: 38714.203125\n",
      "Train Epoch: 6 [448000/1309486 (34%)]\tLoss: 39727.941406\n",
      "Train Epoch: 6 [456000/1309486 (35%)]\tLoss: 38701.828125\n",
      "Train Epoch: 6 [464000/1309486 (35%)]\tLoss: 39443.941406\n",
      "Train Epoch: 6 [472000/1309486 (36%)]\tLoss: 35631.613281\n",
      "Train Epoch: 6 [480000/1309486 (37%)]\tLoss: 39858.222656\n",
      "Train Epoch: 6 [488000/1309486 (37%)]\tLoss: 39012.125000\n",
      "Train Epoch: 6 [496000/1309486 (38%)]\tLoss: 36616.617188\n",
      "Train Epoch: 6 [504000/1309486 (38%)]\tLoss: 37804.179688\n",
      "Train Epoch: 6 [512000/1309486 (39%)]\tLoss: 38564.527344\n",
      "Train Epoch: 6 [520000/1309486 (40%)]\tLoss: 39885.316406\n",
      "Train Epoch: 6 [528000/1309486 (40%)]\tLoss: 47372.757812\n",
      "Train Epoch: 6 [536000/1309486 (41%)]\tLoss: 41989.335938\n",
      "Train Epoch: 6 [544000/1309486 (42%)]\tLoss: 38305.492188\n",
      "Train Epoch: 6 [552000/1309486 (42%)]\tLoss: 34481.910156\n",
      "Train Epoch: 6 [560000/1309486 (43%)]\tLoss: 38446.269531\n",
      "Train Epoch: 6 [568000/1309486 (43%)]\tLoss: 41730.117188\n",
      "Train Epoch: 6 [576000/1309486 (44%)]\tLoss: 35260.578125\n",
      "Train Epoch: 6 [584000/1309486 (45%)]\tLoss: 40786.351562\n",
      "Train Epoch: 6 [592000/1309486 (45%)]\tLoss: 41810.039062\n",
      "Train Epoch: 6 [600000/1309486 (46%)]\tLoss: 38757.699219\n",
      "Train Epoch: 6 [608000/1309486 (46%)]\tLoss: 39089.414062\n",
      "Train Epoch: 6 [616000/1309486 (47%)]\tLoss: 37212.117188\n",
      "Train Epoch: 6 [624000/1309486 (48%)]\tLoss: 38197.546875\n",
      "Train Epoch: 6 [632000/1309486 (48%)]\tLoss: 33908.453125\n",
      "Train Epoch: 6 [640000/1309486 (49%)]\tLoss: 40227.675781\n",
      "Train Epoch: 6 [648000/1309486 (49%)]\tLoss: 37919.375000\n",
      "Train Epoch: 6 [656000/1309486 (50%)]\tLoss: 38785.878906\n",
      "Train Epoch: 6 [664000/1309486 (51%)]\tLoss: 34145.015625\n",
      "Train Epoch: 6 [672000/1309486 (51%)]\tLoss: 39731.625000\n",
      "Train Epoch: 6 [680000/1309486 (52%)]\tLoss: 41093.664062\n",
      "Train Epoch: 6 [688000/1309486 (53%)]\tLoss: 38856.105469\n",
      "Train Epoch: 6 [696000/1309486 (53%)]\tLoss: 42856.070312\n",
      "Train Epoch: 6 [704000/1309486 (54%)]\tLoss: 39173.492188\n",
      "Train Epoch: 6 [712000/1309486 (54%)]\tLoss: 39802.781250\n",
      "Train Epoch: 6 [720000/1309486 (55%)]\tLoss: 41293.179688\n",
      "Train Epoch: 6 [728000/1309486 (56%)]\tLoss: 39413.863281\n",
      "Train Epoch: 6 [736000/1309486 (56%)]\tLoss: 35935.601562\n",
      "Train Epoch: 6 [744000/1309486 (57%)]\tLoss: 37350.132812\n",
      "Train Epoch: 6 [752000/1309486 (57%)]\tLoss: 35684.234375\n",
      "Train Epoch: 6 [760000/1309486 (58%)]\tLoss: 34739.109375\n",
      "Train Epoch: 6 [768000/1309486 (59%)]\tLoss: 35819.406250\n",
      "Train Epoch: 6 [776000/1309486 (59%)]\tLoss: 41151.753906\n",
      "Train Epoch: 6 [784000/1309486 (60%)]\tLoss: 41539.929688\n",
      "Train Epoch: 6 [792000/1309486 (60%)]\tLoss: 35984.679688\n",
      "Train Epoch: 6 [800000/1309486 (61%)]\tLoss: 41021.343750\n",
      "Train Epoch: 6 [808000/1309486 (62%)]\tLoss: 44599.257812\n",
      "Train Epoch: 6 [816000/1309486 (62%)]\tLoss: 35610.308594\n",
      "Train Epoch: 6 [824000/1309486 (63%)]\tLoss: 36570.765625\n",
      "Train Epoch: 6 [832000/1309486 (64%)]\tLoss: 34267.664062\n",
      "Train Epoch: 6 [840000/1309486 (64%)]\tLoss: 42388.871094\n",
      "Train Epoch: 6 [848000/1309486 (65%)]\tLoss: 40794.148438\n",
      "Train Epoch: 6 [856000/1309486 (65%)]\tLoss: 37650.656250\n",
      "Train Epoch: 6 [864000/1309486 (66%)]\tLoss: 35691.718750\n",
      "Train Epoch: 6 [872000/1309486 (67%)]\tLoss: 34983.789062\n",
      "Train Epoch: 6 [880000/1309486 (67%)]\tLoss: 33935.765625\n",
      "Train Epoch: 6 [888000/1309486 (68%)]\tLoss: 36876.296875\n",
      "Train Epoch: 6 [896000/1309486 (68%)]\tLoss: 38826.875000\n",
      "Train Epoch: 6 [904000/1309486 (69%)]\tLoss: 40942.781250\n",
      "Train Epoch: 6 [912000/1309486 (70%)]\tLoss: 36927.234375\n",
      "Train Epoch: 6 [920000/1309486 (70%)]\tLoss: 39934.820312\n",
      "Train Epoch: 6 [928000/1309486 (71%)]\tLoss: 38582.937500\n",
      "Train Epoch: 6 [936000/1309486 (71%)]\tLoss: 40209.992188\n",
      "Train Epoch: 6 [944000/1309486 (72%)]\tLoss: 37816.796875\n",
      "Train Epoch: 6 [952000/1309486 (73%)]\tLoss: 38984.992188\n",
      "Train Epoch: 6 [960000/1309486 (73%)]\tLoss: 39901.195312\n",
      "Train Epoch: 6 [968000/1309486 (74%)]\tLoss: 41203.226562\n",
      "Train Epoch: 6 [976000/1309486 (75%)]\tLoss: 39180.691406\n",
      "Train Epoch: 6 [984000/1309486 (75%)]\tLoss: 37436.578125\n",
      "Train Epoch: 6 [992000/1309486 (76%)]\tLoss: 37847.093750\n",
      "Train Epoch: 6 [1000000/1309486 (76%)]\tLoss: 37047.878906\n",
      "Train Epoch: 6 [1008000/1309486 (77%)]\tLoss: 37771.484375\n",
      "Train Epoch: 6 [1016000/1309486 (78%)]\tLoss: 33742.371094\n",
      "Train Epoch: 6 [1024000/1309486 (78%)]\tLoss: 38891.386719\n",
      "Train Epoch: 6 [1032000/1309486 (79%)]\tLoss: 36523.390625\n",
      "Train Epoch: 6 [1040000/1309486 (79%)]\tLoss: 38640.226562\n",
      "Train Epoch: 6 [1048000/1309486 (80%)]\tLoss: 37814.835938\n",
      "Train Epoch: 6 [1056000/1309486 (81%)]\tLoss: 36470.695312\n",
      "Train Epoch: 6 [1064000/1309486 (81%)]\tLoss: 37027.425781\n",
      "Train Epoch: 6 [1072000/1309486 (82%)]\tLoss: 42264.289062\n",
      "Train Epoch: 6 [1080000/1309486 (82%)]\tLoss: 36781.195312\n",
      "Train Epoch: 6 [1088000/1309486 (83%)]\tLoss: 41467.000000\n",
      "Train Epoch: 6 [1096000/1309486 (84%)]\tLoss: 38459.652344\n",
      "Train Epoch: 6 [1104000/1309486 (84%)]\tLoss: 39991.078125\n",
      "Train Epoch: 6 [1112000/1309486 (85%)]\tLoss: 38409.554688\n",
      "Train Epoch: 6 [1120000/1309486 (86%)]\tLoss: 40450.546875\n",
      "Train Epoch: 6 [1128000/1309486 (86%)]\tLoss: 36737.449219\n",
      "Train Epoch: 6 [1136000/1309486 (87%)]\tLoss: 36696.128906\n",
      "Train Epoch: 6 [1144000/1309486 (87%)]\tLoss: 37519.726562\n",
      "Train Epoch: 6 [1152000/1309486 (88%)]\tLoss: 39718.703125\n",
      "Train Epoch: 6 [1160000/1309486 (89%)]\tLoss: 41003.515625\n",
      "Train Epoch: 6 [1168000/1309486 (89%)]\tLoss: 34087.726562\n",
      "Train Epoch: 6 [1176000/1309486 (90%)]\tLoss: 33793.718750\n",
      "Train Epoch: 6 [1184000/1309486 (90%)]\tLoss: 39698.121094\n",
      "Train Epoch: 6 [1192000/1309486 (91%)]\tLoss: 35622.582031\n",
      "Train Epoch: 6 [1200000/1309486 (92%)]\tLoss: 38181.359375\n",
      "Train Epoch: 6 [1208000/1309486 (92%)]\tLoss: 43030.078125\n",
      "Train Epoch: 6 [1216000/1309486 (93%)]\tLoss: 34631.406250\n",
      "Train Epoch: 6 [1224000/1309486 (93%)]\tLoss: 39364.671875\n",
      "Train Epoch: 6 [1232000/1309486 (94%)]\tLoss: 40215.542969\n",
      "Train Epoch: 6 [1240000/1309486 (95%)]\tLoss: 37678.828125\n",
      "Train Epoch: 6 [1248000/1309486 (95%)]\tLoss: 38759.210938\n",
      "Train Epoch: 6 [1256000/1309486 (96%)]\tLoss: 36623.664062\n",
      "Train Epoch: 6 [1264000/1309486 (97%)]\tLoss: 35829.675781\n",
      "Train Epoch: 6 [1272000/1309486 (97%)]\tLoss: 35663.773438\n",
      "Train Epoch: 6 [1280000/1309486 (98%)]\tLoss: 37792.140625\n",
      "Train Epoch: 6 [1288000/1309486 (98%)]\tLoss: 40878.542969\n",
      "Train Epoch: 6 [1296000/1309486 (99%)]\tLoss: 37324.765625\n",
      "Train Epoch: 6 [1304000/1309486 (100%)]\tLoss: 35948.953125\n",
      "Epoch 6 model saved!\n",
      "epoch train time:  786.65466428\n",
      "Train Epoch: 7 [8000/1309486 (1%)]\tLoss: 37234.289062\n",
      "Train Epoch: 7 [16000/1309486 (1%)]\tLoss: 38486.203125\n",
      "Train Epoch: 7 [24000/1309486 (2%)]\tLoss: 38393.382812\n",
      "Train Epoch: 7 [32000/1309486 (2%)]\tLoss: 41229.945312\n",
      "Train Epoch: 7 [40000/1309486 (3%)]\tLoss: 40169.921875\n",
      "Train Epoch: 7 [48000/1309486 (4%)]\tLoss: 37176.703125\n",
      "Train Epoch: 7 [56000/1309486 (4%)]\tLoss: 39367.718750\n",
      "Train Epoch: 7 [64000/1309486 (5%)]\tLoss: 38703.335938\n",
      "Train Epoch: 7 [72000/1309486 (5%)]\tLoss: 42201.691406\n",
      "Train Epoch: 7 [80000/1309486 (6%)]\tLoss: 40435.812500\n",
      "Train Epoch: 7 [88000/1309486 (7%)]\tLoss: 38118.093750\n",
      "Train Epoch: 7 [96000/1309486 (7%)]\tLoss: 38324.625000\n",
      "Train Epoch: 7 [104000/1309486 (8%)]\tLoss: 38272.019531\n",
      "Train Epoch: 7 [112000/1309486 (9%)]\tLoss: 37658.101562\n",
      "Train Epoch: 7 [120000/1309486 (9%)]\tLoss: 41810.769531\n",
      "Train Epoch: 7 [128000/1309486 (10%)]\tLoss: 35974.523438\n",
      "Train Epoch: 7 [136000/1309486 (10%)]\tLoss: 39986.734375\n",
      "Train Epoch: 7 [144000/1309486 (11%)]\tLoss: 41087.183594\n",
      "Train Epoch: 7 [152000/1309486 (12%)]\tLoss: 33567.351562\n",
      "Train Epoch: 7 [160000/1309486 (12%)]\tLoss: 40447.832031\n",
      "Train Epoch: 7 [168000/1309486 (13%)]\tLoss: 38915.179688\n",
      "Train Epoch: 7 [176000/1309486 (13%)]\tLoss: 37898.523438\n",
      "Train Epoch: 7 [184000/1309486 (14%)]\tLoss: 36650.281250\n",
      "Train Epoch: 7 [192000/1309486 (15%)]\tLoss: 37381.023438\n",
      "Train Epoch: 7 [200000/1309486 (15%)]\tLoss: 38905.656250\n",
      "Train Epoch: 7 [208000/1309486 (16%)]\tLoss: 39769.171875\n",
      "Train Epoch: 7 [216000/1309486 (16%)]\tLoss: 36314.515625\n",
      "Train Epoch: 7 [224000/1309486 (17%)]\tLoss: 43235.140625\n",
      "Train Epoch: 7 [232000/1309486 (18%)]\tLoss: 37430.152344\n",
      "Train Epoch: 7 [240000/1309486 (18%)]\tLoss: 37301.140625\n",
      "Train Epoch: 7 [248000/1309486 (19%)]\tLoss: 36472.257812\n",
      "Train Epoch: 7 [256000/1309486 (20%)]\tLoss: 38721.351562\n",
      "Train Epoch: 7 [264000/1309486 (20%)]\tLoss: 34813.039062\n",
      "Train Epoch: 7 [272000/1309486 (21%)]\tLoss: 37277.070312\n",
      "Train Epoch: 7 [280000/1309486 (21%)]\tLoss: 36597.445312\n",
      "Train Epoch: 7 [288000/1309486 (22%)]\tLoss: 38453.179688\n",
      "Train Epoch: 7 [296000/1309486 (23%)]\tLoss: 38211.015625\n",
      "Train Epoch: 7 [304000/1309486 (23%)]\tLoss: 35606.535156\n",
      "Train Epoch: 7 [312000/1309486 (24%)]\tLoss: 38298.039062\n",
      "Train Epoch: 7 [320000/1309486 (24%)]\tLoss: 35011.226562\n",
      "Train Epoch: 7 [328000/1309486 (25%)]\tLoss: 40565.085938\n",
      "Train Epoch: 7 [336000/1309486 (26%)]\tLoss: 37631.890625\n",
      "Train Epoch: 7 [344000/1309486 (26%)]\tLoss: 36652.414062\n",
      "Train Epoch: 7 [352000/1309486 (27%)]\tLoss: 36245.648438\n",
      "Train Epoch: 7 [360000/1309486 (27%)]\tLoss: 38735.250000\n",
      "Train Epoch: 7 [368000/1309486 (28%)]\tLoss: 40778.148438\n",
      "Train Epoch: 7 [376000/1309486 (29%)]\tLoss: 38758.605469\n",
      "Train Epoch: 7 [384000/1309486 (29%)]\tLoss: 39090.101562\n",
      "Train Epoch: 7 [392000/1309486 (30%)]\tLoss: 40679.746094\n",
      "Train Epoch: 7 [400000/1309486 (31%)]\tLoss: 36842.281250\n",
      "Train Epoch: 7 [408000/1309486 (31%)]\tLoss: 35745.976562\n",
      "Train Epoch: 7 [416000/1309486 (32%)]\tLoss: 37638.457031\n",
      "Train Epoch: 7 [424000/1309486 (32%)]\tLoss: 39805.820312\n",
      "Train Epoch: 7 [432000/1309486 (33%)]\tLoss: 34652.898438\n",
      "Train Epoch: 7 [440000/1309486 (34%)]\tLoss: 34724.382812\n",
      "Train Epoch: 7 [448000/1309486 (34%)]\tLoss: 39644.703125\n",
      "Train Epoch: 7 [456000/1309486 (35%)]\tLoss: 37658.414062\n",
      "Train Epoch: 7 [464000/1309486 (35%)]\tLoss: 37432.742188\n",
      "Train Epoch: 7 [472000/1309486 (36%)]\tLoss: 41095.019531\n",
      "Train Epoch: 7 [480000/1309486 (37%)]\tLoss: 39033.570312\n",
      "Train Epoch: 7 [488000/1309486 (37%)]\tLoss: 35351.281250\n",
      "Train Epoch: 7 [496000/1309486 (38%)]\tLoss: 35211.109375\n",
      "Train Epoch: 7 [504000/1309486 (38%)]\tLoss: 36513.507812\n",
      "Train Epoch: 7 [512000/1309486 (39%)]\tLoss: 38446.500000\n",
      "Train Epoch: 7 [520000/1309486 (40%)]\tLoss: 40571.046875\n",
      "Train Epoch: 7 [528000/1309486 (40%)]\tLoss: 36917.562500\n",
      "Train Epoch: 7 [536000/1309486 (41%)]\tLoss: 40463.765625\n",
      "Train Epoch: 7 [544000/1309486 (42%)]\tLoss: 42539.843750\n",
      "Train Epoch: 7 [552000/1309486 (42%)]\tLoss: 34904.179688\n",
      "Train Epoch: 7 [560000/1309486 (43%)]\tLoss: 39474.554688\n",
      "Train Epoch: 7 [568000/1309486 (43%)]\tLoss: 37976.812500\n",
      "Train Epoch: 7 [576000/1309486 (44%)]\tLoss: 37891.398438\n",
      "Train Epoch: 7 [584000/1309486 (45%)]\tLoss: 37683.359375\n",
      "Train Epoch: 7 [592000/1309486 (45%)]\tLoss: 35476.640625\n",
      "Train Epoch: 7 [600000/1309486 (46%)]\tLoss: 40658.257812\n",
      "Train Epoch: 7 [608000/1309486 (46%)]\tLoss: 38747.375000\n",
      "Train Epoch: 7 [616000/1309486 (47%)]\tLoss: 38463.070312\n",
      "Train Epoch: 7 [624000/1309486 (48%)]\tLoss: 36123.527344\n",
      "Train Epoch: 7 [632000/1309486 (48%)]\tLoss: 34920.132812\n",
      "Train Epoch: 7 [640000/1309486 (49%)]\tLoss: 39526.292969\n",
      "Train Epoch: 7 [648000/1309486 (49%)]\tLoss: 39554.664062\n",
      "Train Epoch: 7 [656000/1309486 (50%)]\tLoss: 41057.648438\n",
      "Train Epoch: 7 [664000/1309486 (51%)]\tLoss: 42306.328125\n",
      "Train Epoch: 7 [672000/1309486 (51%)]\tLoss: 37656.257812\n",
      "Train Epoch: 7 [680000/1309486 (52%)]\tLoss: 36845.695312\n",
      "Train Epoch: 7 [688000/1309486 (53%)]\tLoss: 37050.613281\n",
      "Train Epoch: 7 [696000/1309486 (53%)]\tLoss: 37018.492188\n",
      "Train Epoch: 7 [704000/1309486 (54%)]\tLoss: 36983.777344\n",
      "Train Epoch: 7 [712000/1309486 (54%)]\tLoss: 37412.945312\n",
      "Train Epoch: 7 [720000/1309486 (55%)]\tLoss: 40980.355469\n",
      "Train Epoch: 7 [728000/1309486 (56%)]\tLoss: 40651.074219\n",
      "Train Epoch: 7 [736000/1309486 (56%)]\tLoss: 40337.925781\n",
      "Train Epoch: 7 [744000/1309486 (57%)]\tLoss: 39557.515625\n",
      "Train Epoch: 7 [752000/1309486 (57%)]\tLoss: 39817.187500\n",
      "Train Epoch: 7 [760000/1309486 (58%)]\tLoss: 38849.406250\n",
      "Train Epoch: 7 [768000/1309486 (59%)]\tLoss: 39781.078125\n",
      "Train Epoch: 7 [776000/1309486 (59%)]\tLoss: 35074.058594\n",
      "Train Epoch: 7 [784000/1309486 (60%)]\tLoss: 43046.269531\n",
      "Train Epoch: 7 [792000/1309486 (60%)]\tLoss: 43640.410156\n",
      "Train Epoch: 7 [800000/1309486 (61%)]\tLoss: 36232.601562\n",
      "Train Epoch: 7 [808000/1309486 (62%)]\tLoss: 37964.132812\n",
      "Train Epoch: 7 [816000/1309486 (62%)]\tLoss: 37697.246094\n",
      "Train Epoch: 7 [824000/1309486 (63%)]\tLoss: 35577.539062\n",
      "Train Epoch: 7 [832000/1309486 (64%)]\tLoss: 41707.656250\n",
      "Train Epoch: 7 [840000/1309486 (64%)]\tLoss: 41065.398438\n",
      "Train Epoch: 7 [848000/1309486 (65%)]\tLoss: 36493.484375\n",
      "Train Epoch: 7 [856000/1309486 (65%)]\tLoss: 36390.664062\n",
      "Train Epoch: 7 [864000/1309486 (66%)]\tLoss: 42900.976562\n",
      "Train Epoch: 7 [872000/1309486 (67%)]\tLoss: 37595.683594\n",
      "Train Epoch: 7 [880000/1309486 (67%)]\tLoss: 37607.089844\n",
      "Train Epoch: 7 [888000/1309486 (68%)]\tLoss: 35576.078125\n",
      "Train Epoch: 7 [896000/1309486 (68%)]\tLoss: 44377.882812\n",
      "Train Epoch: 7 [904000/1309486 (69%)]\tLoss: 36762.023438\n",
      "Train Epoch: 7 [912000/1309486 (70%)]\tLoss: 38023.296875\n",
      "Train Epoch: 7 [920000/1309486 (70%)]\tLoss: 40684.812500\n",
      "Train Epoch: 7 [928000/1309486 (71%)]\tLoss: 38512.062500\n",
      "Train Epoch: 7 [936000/1309486 (71%)]\tLoss: 36266.921875\n",
      "Train Epoch: 7 [944000/1309486 (72%)]\tLoss: 37556.683594\n",
      "Train Epoch: 7 [952000/1309486 (73%)]\tLoss: 35542.492188\n",
      "Train Epoch: 7 [960000/1309486 (73%)]\tLoss: 37174.507812\n",
      "Train Epoch: 7 [968000/1309486 (74%)]\tLoss: 40700.886719\n",
      "Train Epoch: 7 [976000/1309486 (75%)]\tLoss: 42376.164062\n",
      "Train Epoch: 7 [984000/1309486 (75%)]\tLoss: 37131.015625\n",
      "Train Epoch: 7 [992000/1309486 (76%)]\tLoss: 37603.898438\n",
      "Train Epoch: 7 [1000000/1309486 (76%)]\tLoss: 38450.328125\n",
      "Train Epoch: 7 [1008000/1309486 (77%)]\tLoss: 39304.687500\n",
      "Train Epoch: 7 [1016000/1309486 (78%)]\tLoss: 43769.968750\n",
      "Train Epoch: 7 [1024000/1309486 (78%)]\tLoss: 36347.687500\n",
      "Train Epoch: 7 [1032000/1309486 (79%)]\tLoss: 36315.539062\n",
      "Train Epoch: 7 [1040000/1309486 (79%)]\tLoss: 39360.781250\n",
      "Train Epoch: 7 [1048000/1309486 (80%)]\tLoss: 40374.046875\n",
      "Train Epoch: 7 [1056000/1309486 (81%)]\tLoss: 41983.500000\n",
      "Train Epoch: 7 [1064000/1309486 (81%)]\tLoss: 39524.187500\n",
      "Train Epoch: 7 [1072000/1309486 (82%)]\tLoss: 38386.789062\n",
      "Train Epoch: 7 [1080000/1309486 (82%)]\tLoss: 37806.890625\n",
      "Train Epoch: 7 [1088000/1309486 (83%)]\tLoss: 37598.742188\n",
      "Train Epoch: 7 [1096000/1309486 (84%)]\tLoss: 36054.804688\n",
      "Train Epoch: 7 [1104000/1309486 (84%)]\tLoss: 37662.082031\n",
      "Train Epoch: 7 [1112000/1309486 (85%)]\tLoss: 42408.386719\n",
      "Train Epoch: 7 [1120000/1309486 (86%)]\tLoss: 38329.710938\n",
      "Train Epoch: 7 [1128000/1309486 (86%)]\tLoss: 39946.957031\n",
      "Train Epoch: 7 [1136000/1309486 (87%)]\tLoss: 44355.960938\n",
      "Train Epoch: 7 [1144000/1309486 (87%)]\tLoss: 39669.750000\n",
      "Train Epoch: 7 [1152000/1309486 (88%)]\tLoss: 42300.835938\n",
      "Train Epoch: 7 [1160000/1309486 (89%)]\tLoss: 40141.378906\n",
      "Train Epoch: 7 [1168000/1309486 (89%)]\tLoss: 37904.468750\n",
      "Train Epoch: 7 [1176000/1309486 (90%)]\tLoss: 37512.515625\n",
      "Train Epoch: 7 [1184000/1309486 (90%)]\tLoss: 38009.921875\n",
      "Train Epoch: 7 [1192000/1309486 (91%)]\tLoss: 37455.820312\n",
      "Train Epoch: 7 [1200000/1309486 (92%)]\tLoss: 34754.242188\n",
      "Train Epoch: 7 [1208000/1309486 (92%)]\tLoss: 37512.136719\n",
      "Train Epoch: 7 [1216000/1309486 (93%)]\tLoss: 38959.164062\n",
      "Train Epoch: 7 [1224000/1309486 (93%)]\tLoss: 39719.171875\n",
      "Train Epoch: 7 [1232000/1309486 (94%)]\tLoss: 40022.019531\n",
      "Train Epoch: 7 [1240000/1309486 (95%)]\tLoss: 38021.164062\n",
      "Train Epoch: 7 [1248000/1309486 (95%)]\tLoss: 37472.808594\n",
      "Train Epoch: 7 [1256000/1309486 (96%)]\tLoss: 43028.031250\n",
      "Train Epoch: 7 [1264000/1309486 (97%)]\tLoss: 38662.355469\n",
      "Train Epoch: 7 [1272000/1309486 (97%)]\tLoss: 38446.398438\n",
      "Train Epoch: 7 [1280000/1309486 (98%)]\tLoss: 37155.851562\n",
      "Train Epoch: 7 [1288000/1309486 (98%)]\tLoss: 35915.097656\n",
      "Train Epoch: 7 [1296000/1309486 (99%)]\tLoss: 39062.035156\n",
      "Train Epoch: 7 [1304000/1309486 (100%)]\tLoss: 36258.445312\n",
      "Epoch 7 model saved!\n",
      "epoch train time:  785.91289568\n",
      "Train Epoch: 8 [8000/1309486 (1%)]\tLoss: 37927.039062\n",
      "Train Epoch: 8 [16000/1309486 (1%)]\tLoss: 35959.558594\n",
      "Train Epoch: 8 [24000/1309486 (2%)]\tLoss: 37701.414062\n",
      "Train Epoch: 8 [32000/1309486 (2%)]\tLoss: 41663.210938\n",
      "Train Epoch: 8 [40000/1309486 (3%)]\tLoss: 38864.062500\n",
      "Train Epoch: 8 [48000/1309486 (4%)]\tLoss: 40299.292969\n",
      "Train Epoch: 8 [56000/1309486 (4%)]\tLoss: 37098.046875\n",
      "Train Epoch: 8 [64000/1309486 (5%)]\tLoss: 35731.437500\n",
      "Train Epoch: 8 [72000/1309486 (5%)]\tLoss: 40042.710938\n",
      "Train Epoch: 8 [80000/1309486 (6%)]\tLoss: 39276.976562\n",
      "Train Epoch: 8 [88000/1309486 (7%)]\tLoss: 41569.679688\n",
      "Train Epoch: 8 [96000/1309486 (7%)]\tLoss: 34184.640625\n",
      "Train Epoch: 8 [104000/1309486 (8%)]\tLoss: 36254.148438\n",
      "Train Epoch: 8 [112000/1309486 (9%)]\tLoss: 36486.648438\n",
      "Train Epoch: 8 [120000/1309486 (9%)]\tLoss: 41935.789062\n",
      "Train Epoch: 8 [128000/1309486 (10%)]\tLoss: 37775.828125\n",
      "Train Epoch: 8 [136000/1309486 (10%)]\tLoss: 36207.414062\n",
      "Train Epoch: 8 [144000/1309486 (11%)]\tLoss: 37990.898438\n",
      "Train Epoch: 8 [152000/1309486 (12%)]\tLoss: 39169.804688\n",
      "Train Epoch: 8 [160000/1309486 (12%)]\tLoss: 38964.914062\n",
      "Train Epoch: 8 [168000/1309486 (13%)]\tLoss: 42197.574219\n",
      "Train Epoch: 8 [176000/1309486 (13%)]\tLoss: 43531.394531\n",
      "Train Epoch: 8 [184000/1309486 (14%)]\tLoss: 39209.976562\n",
      "Train Epoch: 8 [192000/1309486 (15%)]\tLoss: 38351.093750\n",
      "Train Epoch: 8 [200000/1309486 (15%)]\tLoss: 35101.550781\n",
      "Train Epoch: 8 [208000/1309486 (16%)]\tLoss: 36593.316406\n",
      "Train Epoch: 8 [216000/1309486 (16%)]\tLoss: 34851.945312\n",
      "Train Epoch: 8 [224000/1309486 (17%)]\tLoss: 40311.898438\n",
      "Train Epoch: 8 [232000/1309486 (18%)]\tLoss: 38036.726562\n",
      "Train Epoch: 8 [240000/1309486 (18%)]\tLoss: 41051.062500\n",
      "Train Epoch: 8 [248000/1309486 (19%)]\tLoss: 36433.367188\n",
      "Train Epoch: 8 [256000/1309486 (20%)]\tLoss: 39159.472656\n",
      "Train Epoch: 8 [264000/1309486 (20%)]\tLoss: 39725.113281\n",
      "Train Epoch: 8 [272000/1309486 (21%)]\tLoss: 34126.992188\n",
      "Train Epoch: 8 [280000/1309486 (21%)]\tLoss: 38515.503906\n",
      "Train Epoch: 8 [288000/1309486 (22%)]\tLoss: 36630.148438\n",
      "Train Epoch: 8 [296000/1309486 (23%)]\tLoss: 42682.890625\n",
      "Train Epoch: 8 [304000/1309486 (23%)]\tLoss: 38221.019531\n",
      "Train Epoch: 8 [312000/1309486 (24%)]\tLoss: 45239.890625\n",
      "Train Epoch: 8 [320000/1309486 (24%)]\tLoss: 36264.882812\n",
      "Train Epoch: 8 [328000/1309486 (25%)]\tLoss: 41464.882812\n",
      "Train Epoch: 8 [336000/1309486 (26%)]\tLoss: 39067.769531\n",
      "Train Epoch: 8 [344000/1309486 (26%)]\tLoss: 33827.230469\n",
      "Train Epoch: 8 [352000/1309486 (27%)]\tLoss: 40890.570312\n",
      "Train Epoch: 8 [360000/1309486 (27%)]\tLoss: 36759.636719\n",
      "Train Epoch: 8 [368000/1309486 (28%)]\tLoss: 39185.488281\n",
      "Train Epoch: 8 [376000/1309486 (29%)]\tLoss: 40032.203125\n",
      "Train Epoch: 8 [384000/1309486 (29%)]\tLoss: 40471.902344\n",
      "Train Epoch: 8 [392000/1309486 (30%)]\tLoss: 41600.960938\n",
      "Train Epoch: 8 [400000/1309486 (31%)]\tLoss: 32945.937500\n",
      "Train Epoch: 8 [408000/1309486 (31%)]\tLoss: 42805.089844\n",
      "Train Epoch: 8 [416000/1309486 (32%)]\tLoss: 36367.015625\n",
      "Train Epoch: 8 [424000/1309486 (32%)]\tLoss: 39916.527344\n",
      "Train Epoch: 8 [432000/1309486 (33%)]\tLoss: 38022.820312\n",
      "Train Epoch: 8 [440000/1309486 (34%)]\tLoss: 41516.726562\n",
      "Train Epoch: 8 [448000/1309486 (34%)]\tLoss: 40432.699219\n",
      "Train Epoch: 8 [456000/1309486 (35%)]\tLoss: 36381.257812\n",
      "Train Epoch: 8 [464000/1309486 (35%)]\tLoss: 40534.476562\n",
      "Train Epoch: 8 [472000/1309486 (36%)]\tLoss: 38398.648438\n",
      "Train Epoch: 8 [480000/1309486 (37%)]\tLoss: 39064.867188\n",
      "Train Epoch: 8 [488000/1309486 (37%)]\tLoss: 37128.859375\n",
      "Train Epoch: 8 [496000/1309486 (38%)]\tLoss: 35738.593750\n",
      "Train Epoch: 8 [504000/1309486 (38%)]\tLoss: 35433.953125\n",
      "Train Epoch: 8 [512000/1309486 (39%)]\tLoss: 38540.828125\n",
      "Train Epoch: 8 [520000/1309486 (40%)]\tLoss: 35698.796875\n",
      "Train Epoch: 8 [528000/1309486 (40%)]\tLoss: 36612.960938\n",
      "Train Epoch: 8 [536000/1309486 (41%)]\tLoss: 37500.398438\n",
      "Train Epoch: 8 [544000/1309486 (42%)]\tLoss: 34693.648438\n",
      "Train Epoch: 8 [552000/1309486 (42%)]\tLoss: 41371.015625\n",
      "Train Epoch: 8 [560000/1309486 (43%)]\tLoss: 40836.335938\n",
      "Train Epoch: 8 [568000/1309486 (43%)]\tLoss: 37043.863281\n",
      "Train Epoch: 8 [576000/1309486 (44%)]\tLoss: 37426.347656\n",
      "Train Epoch: 8 [584000/1309486 (45%)]\tLoss: 37458.953125\n",
      "Train Epoch: 8 [592000/1309486 (45%)]\tLoss: 39753.664062\n",
      "Train Epoch: 8 [600000/1309486 (46%)]\tLoss: 35371.808594\n",
      "Train Epoch: 8 [608000/1309486 (46%)]\tLoss: 36845.218750\n",
      "Train Epoch: 8 [616000/1309486 (47%)]\tLoss: 36214.660156\n",
      "Train Epoch: 8 [624000/1309486 (48%)]\tLoss: 43015.210938\n",
      "Train Epoch: 8 [632000/1309486 (48%)]\tLoss: 39307.132812\n",
      "Train Epoch: 8 [640000/1309486 (49%)]\tLoss: 37468.804688\n",
      "Train Epoch: 8 [648000/1309486 (49%)]\tLoss: 36385.449219\n",
      "Train Epoch: 8 [656000/1309486 (50%)]\tLoss: 37535.093750\n",
      "Train Epoch: 8 [664000/1309486 (51%)]\tLoss: 40582.367188\n",
      "Train Epoch: 8 [672000/1309486 (51%)]\tLoss: 34754.132812\n",
      "Train Epoch: 8 [680000/1309486 (52%)]\tLoss: 39665.429688\n",
      "Train Epoch: 8 [688000/1309486 (53%)]\tLoss: 38708.007812\n",
      "Train Epoch: 8 [696000/1309486 (53%)]\tLoss: 40739.152344\n",
      "Train Epoch: 8 [704000/1309486 (54%)]\tLoss: 36361.730469\n",
      "Train Epoch: 8 [712000/1309486 (54%)]\tLoss: 44049.667969\n",
      "Train Epoch: 8 [720000/1309486 (55%)]\tLoss: 36740.773438\n",
      "Train Epoch: 8 [728000/1309486 (56%)]\tLoss: 39487.304688\n",
      "Train Epoch: 8 [736000/1309486 (56%)]\tLoss: 36480.617188\n",
      "Train Epoch: 8 [744000/1309486 (57%)]\tLoss: 37702.953125\n",
      "Train Epoch: 8 [752000/1309486 (57%)]\tLoss: 37550.765625\n",
      "Train Epoch: 8 [760000/1309486 (58%)]\tLoss: 36080.410156\n",
      "Train Epoch: 8 [768000/1309486 (59%)]\tLoss: 36219.281250\n",
      "Train Epoch: 8 [776000/1309486 (59%)]\tLoss: 35565.515625\n",
      "Train Epoch: 8 [784000/1309486 (60%)]\tLoss: 38921.019531\n",
      "Train Epoch: 8 [792000/1309486 (60%)]\tLoss: 34680.218750\n",
      "Train Epoch: 8 [800000/1309486 (61%)]\tLoss: 38946.218750\n",
      "Train Epoch: 8 [808000/1309486 (62%)]\tLoss: 44179.960938\n",
      "Train Epoch: 8 [816000/1309486 (62%)]\tLoss: 37446.031250\n",
      "Train Epoch: 8 [824000/1309486 (63%)]\tLoss: 40290.433594\n",
      "Train Epoch: 8 [832000/1309486 (64%)]\tLoss: 35804.359375\n",
      "Train Epoch: 8 [840000/1309486 (64%)]\tLoss: 38232.066406\n",
      "Train Epoch: 8 [848000/1309486 (65%)]\tLoss: 40758.640625\n",
      "Train Epoch: 8 [856000/1309486 (65%)]\tLoss: 39729.988281\n",
      "Train Epoch: 8 [864000/1309486 (66%)]\tLoss: 34915.867188\n",
      "Train Epoch: 8 [872000/1309486 (67%)]\tLoss: 44021.203125\n",
      "Train Epoch: 8 [880000/1309486 (67%)]\tLoss: 40628.253906\n",
      "Train Epoch: 8 [888000/1309486 (68%)]\tLoss: 32715.271484\n",
      "Train Epoch: 8 [896000/1309486 (68%)]\tLoss: 37709.390625\n",
      "Train Epoch: 8 [904000/1309486 (69%)]\tLoss: 40346.550781\n",
      "Train Epoch: 8 [912000/1309486 (70%)]\tLoss: 39741.851562\n",
      "Train Epoch: 8 [920000/1309486 (70%)]\tLoss: 38371.707031\n",
      "Train Epoch: 8 [928000/1309486 (71%)]\tLoss: 44928.664062\n",
      "Train Epoch: 8 [936000/1309486 (71%)]\tLoss: 36908.242188\n",
      "Train Epoch: 8 [944000/1309486 (72%)]\tLoss: 40902.828125\n",
      "Train Epoch: 8 [952000/1309486 (73%)]\tLoss: 38518.234375\n",
      "Train Epoch: 8 [960000/1309486 (73%)]\tLoss: 36262.789062\n",
      "Train Epoch: 8 [968000/1309486 (74%)]\tLoss: 39145.890625\n",
      "Train Epoch: 8 [976000/1309486 (75%)]\tLoss: 36394.472656\n",
      "Train Epoch: 8 [984000/1309486 (75%)]\tLoss: 40651.812500\n",
      "Train Epoch: 8 [992000/1309486 (76%)]\tLoss: 38872.304688\n",
      "Train Epoch: 8 [1000000/1309486 (76%)]\tLoss: 41219.460938\n",
      "Train Epoch: 8 [1008000/1309486 (77%)]\tLoss: 39023.492188\n",
      "Train Epoch: 8 [1016000/1309486 (78%)]\tLoss: 36289.742188\n",
      "Train Epoch: 8 [1024000/1309486 (78%)]\tLoss: 39721.304688\n",
      "Train Epoch: 8 [1032000/1309486 (79%)]\tLoss: 39561.804688\n",
      "Train Epoch: 8 [1040000/1309486 (79%)]\tLoss: 37031.187500\n",
      "Train Epoch: 8 [1048000/1309486 (80%)]\tLoss: 38219.734375\n",
      "Train Epoch: 8 [1056000/1309486 (81%)]\tLoss: 42259.261719\n",
      "Train Epoch: 8 [1064000/1309486 (81%)]\tLoss: 36517.628906\n",
      "Train Epoch: 8 [1072000/1309486 (82%)]\tLoss: 34759.457031\n",
      "Train Epoch: 8 [1080000/1309486 (82%)]\tLoss: 37416.320312\n",
      "Train Epoch: 8 [1088000/1309486 (83%)]\tLoss: 37218.414062\n",
      "Train Epoch: 8 [1096000/1309486 (84%)]\tLoss: 40113.578125\n",
      "Train Epoch: 8 [1104000/1309486 (84%)]\tLoss: 41776.238281\n",
      "Train Epoch: 8 [1112000/1309486 (85%)]\tLoss: 39912.214844\n",
      "Train Epoch: 8 [1120000/1309486 (86%)]\tLoss: 36783.132812\n",
      "Train Epoch: 8 [1128000/1309486 (86%)]\tLoss: 37794.937500\n",
      "Train Epoch: 8 [1136000/1309486 (87%)]\tLoss: 40974.476562\n",
      "Train Epoch: 8 [1144000/1309486 (87%)]\tLoss: 35368.898438\n",
      "Train Epoch: 8 [1152000/1309486 (88%)]\tLoss: 39007.828125\n",
      "Train Epoch: 8 [1160000/1309486 (89%)]\tLoss: 40632.351562\n",
      "Train Epoch: 8 [1168000/1309486 (89%)]\tLoss: 37817.562500\n",
      "Train Epoch: 8 [1176000/1309486 (90%)]\tLoss: 38860.632812\n",
      "Train Epoch: 8 [1184000/1309486 (90%)]\tLoss: 39177.468750\n",
      "Train Epoch: 8 [1192000/1309486 (91%)]\tLoss: 39833.726562\n",
      "Train Epoch: 8 [1200000/1309486 (92%)]\tLoss: 41459.882812\n",
      "Train Epoch: 8 [1208000/1309486 (92%)]\tLoss: 38868.562500\n",
      "Train Epoch: 8 [1216000/1309486 (93%)]\tLoss: 36730.085938\n",
      "Train Epoch: 8 [1224000/1309486 (93%)]\tLoss: 42902.421875\n",
      "Train Epoch: 8 [1232000/1309486 (94%)]\tLoss: 36665.625000\n",
      "Train Epoch: 8 [1240000/1309486 (95%)]\tLoss: 36785.843750\n",
      "Train Epoch: 8 [1248000/1309486 (95%)]\tLoss: 42499.132812\n",
      "Train Epoch: 8 [1256000/1309486 (96%)]\tLoss: 39970.058594\n",
      "Train Epoch: 8 [1264000/1309486 (97%)]\tLoss: 39751.546875\n",
      "Train Epoch: 8 [1272000/1309486 (97%)]\tLoss: 38184.578125\n",
      "Train Epoch: 8 [1280000/1309486 (98%)]\tLoss: 39763.414062\n",
      "Train Epoch: 8 [1288000/1309486 (98%)]\tLoss: 39837.988281\n",
      "Train Epoch: 8 [1296000/1309486 (99%)]\tLoss: 37267.367188\n",
      "Train Epoch: 8 [1304000/1309486 (100%)]\tLoss: 40141.343750\n",
      "Epoch 8 model saved!\n",
      "epoch train time:  787.84881949\n",
      "Train Epoch: 9 [8000/1309486 (1%)]\tLoss: 40362.093750\n",
      "Train Epoch: 9 [16000/1309486 (1%)]\tLoss: 36139.367188\n",
      "Train Epoch: 9 [24000/1309486 (2%)]\tLoss: 39930.085938\n",
      "Train Epoch: 9 [32000/1309486 (2%)]\tLoss: 36799.089844\n",
      "Train Epoch: 9 [40000/1309486 (3%)]\tLoss: 38310.968750\n",
      "Train Epoch: 9 [48000/1309486 (4%)]\tLoss: 37336.742188\n",
      "Train Epoch: 9 [56000/1309486 (4%)]\tLoss: 36047.640625\n",
      "Train Epoch: 9 [64000/1309486 (5%)]\tLoss: 36355.210938\n",
      "Train Epoch: 9 [72000/1309486 (5%)]\tLoss: 39844.960938\n",
      "Train Epoch: 9 [80000/1309486 (6%)]\tLoss: 39430.093750\n",
      "Train Epoch: 9 [88000/1309486 (7%)]\tLoss: 38831.968750\n",
      "Train Epoch: 9 [96000/1309486 (7%)]\tLoss: 38424.886719\n",
      "Train Epoch: 9 [104000/1309486 (8%)]\tLoss: 43308.734375\n",
      "Train Epoch: 9 [112000/1309486 (9%)]\tLoss: 36345.023438\n",
      "Train Epoch: 9 [120000/1309486 (9%)]\tLoss: 39052.101562\n",
      "Train Epoch: 9 [128000/1309486 (10%)]\tLoss: 33500.734375\n",
      "Train Epoch: 9 [136000/1309486 (10%)]\tLoss: 36992.636719\n",
      "Train Epoch: 9 [144000/1309486 (11%)]\tLoss: 39018.105469\n",
      "Train Epoch: 9 [152000/1309486 (12%)]\tLoss: 41710.679688\n",
      "Train Epoch: 9 [160000/1309486 (12%)]\tLoss: 36698.390625\n",
      "Train Epoch: 9 [168000/1309486 (13%)]\tLoss: 38009.292969\n",
      "Train Epoch: 9 [176000/1309486 (13%)]\tLoss: 39227.375000\n",
      "Train Epoch: 9 [184000/1309486 (14%)]\tLoss: 39162.074219\n",
      "Train Epoch: 9 [192000/1309486 (15%)]\tLoss: 39784.007812\n",
      "Train Epoch: 9 [200000/1309486 (15%)]\tLoss: 35018.828125\n",
      "Train Epoch: 9 [208000/1309486 (16%)]\tLoss: 40820.449219\n",
      "Train Epoch: 9 [216000/1309486 (16%)]\tLoss: 37629.203125\n",
      "Train Epoch: 9 [224000/1309486 (17%)]\tLoss: 36476.757812\n",
      "Train Epoch: 9 [232000/1309486 (18%)]\tLoss: 41312.476562\n",
      "Train Epoch: 9 [240000/1309486 (18%)]\tLoss: 40754.468750\n",
      "Train Epoch: 9 [248000/1309486 (19%)]\tLoss: 38483.023438\n",
      "Train Epoch: 9 [256000/1309486 (20%)]\tLoss: 36584.050781\n",
      "Train Epoch: 9 [264000/1309486 (20%)]\tLoss: 34904.328125\n",
      "Train Epoch: 9 [272000/1309486 (21%)]\tLoss: 36886.996094\n",
      "Train Epoch: 9 [280000/1309486 (21%)]\tLoss: 37945.425781\n",
      "Train Epoch: 9 [288000/1309486 (22%)]\tLoss: 38911.312500\n",
      "Train Epoch: 9 [296000/1309486 (23%)]\tLoss: 41748.851562\n",
      "Train Epoch: 9 [304000/1309486 (23%)]\tLoss: 36623.488281\n",
      "Train Epoch: 9 [312000/1309486 (24%)]\tLoss: 38066.976562\n",
      "Train Epoch: 9 [320000/1309486 (24%)]\tLoss: 45537.000000\n",
      "Train Epoch: 9 [328000/1309486 (25%)]\tLoss: 36281.156250\n",
      "Train Epoch: 9 [336000/1309486 (26%)]\tLoss: 36222.480469\n",
      "Train Epoch: 9 [344000/1309486 (26%)]\tLoss: 38295.625000\n",
      "Train Epoch: 9 [352000/1309486 (27%)]\tLoss: 41535.957031\n",
      "Train Epoch: 9 [360000/1309486 (27%)]\tLoss: 37536.761719\n",
      "Train Epoch: 9 [368000/1309486 (28%)]\tLoss: 42336.167969\n",
      "Train Epoch: 9 [376000/1309486 (29%)]\tLoss: 37842.042969\n",
      "Train Epoch: 9 [384000/1309486 (29%)]\tLoss: 36342.867188\n",
      "Train Epoch: 9 [392000/1309486 (30%)]\tLoss: 37964.878906\n",
      "Train Epoch: 9 [400000/1309486 (31%)]\tLoss: 38886.968750\n",
      "Train Epoch: 9 [408000/1309486 (31%)]\tLoss: 40178.312500\n",
      "Train Epoch: 9 [416000/1309486 (32%)]\tLoss: 41219.292969\n",
      "Train Epoch: 9 [424000/1309486 (32%)]\tLoss: 41680.949219\n",
      "Train Epoch: 9 [432000/1309486 (33%)]\tLoss: 39202.000000\n",
      "Train Epoch: 9 [440000/1309486 (34%)]\tLoss: 37668.839844\n",
      "Train Epoch: 9 [448000/1309486 (34%)]\tLoss: 41937.156250\n",
      "Train Epoch: 9 [456000/1309486 (35%)]\tLoss: 35903.554688\n",
      "Train Epoch: 9 [464000/1309486 (35%)]\tLoss: 42562.992188\n",
      "Train Epoch: 9 [472000/1309486 (36%)]\tLoss: 39798.332031\n",
      "Train Epoch: 9 [480000/1309486 (37%)]\tLoss: 39687.496094\n",
      "Train Epoch: 9 [488000/1309486 (37%)]\tLoss: 39779.492188\n",
      "Train Epoch: 9 [496000/1309486 (38%)]\tLoss: 40599.890625\n",
      "Train Epoch: 9 [504000/1309486 (38%)]\tLoss: 37134.414062\n",
      "Train Epoch: 9 [512000/1309486 (39%)]\tLoss: 38274.148438\n",
      "Train Epoch: 9 [520000/1309486 (40%)]\tLoss: 38367.554688\n",
      "Train Epoch: 9 [528000/1309486 (40%)]\tLoss: 38703.218750\n",
      "Train Epoch: 9 [536000/1309486 (41%)]\tLoss: 41143.371094\n",
      "Train Epoch: 9 [544000/1309486 (42%)]\tLoss: 34411.562500\n",
      "Train Epoch: 9 [552000/1309486 (42%)]\tLoss: 37765.402344\n",
      "Train Epoch: 9 [560000/1309486 (43%)]\tLoss: 38331.609375\n",
      "Train Epoch: 9 [568000/1309486 (43%)]\tLoss: 40130.101562\n",
      "Train Epoch: 9 [576000/1309486 (44%)]\tLoss: 39524.019531\n",
      "Train Epoch: 9 [584000/1309486 (45%)]\tLoss: 36916.492188\n",
      "Train Epoch: 9 [592000/1309486 (45%)]\tLoss: 42745.437500\n",
      "Train Epoch: 9 [600000/1309486 (46%)]\tLoss: 37161.109375\n",
      "Train Epoch: 9 [608000/1309486 (46%)]\tLoss: 39459.863281\n",
      "Train Epoch: 9 [616000/1309486 (47%)]\tLoss: 35068.527344\n",
      "Train Epoch: 9 [624000/1309486 (48%)]\tLoss: 39017.492188\n",
      "Train Epoch: 9 [632000/1309486 (48%)]\tLoss: 38132.203125\n",
      "Train Epoch: 9 [640000/1309486 (49%)]\tLoss: 42659.523438\n",
      "Train Epoch: 9 [648000/1309486 (49%)]\tLoss: 37593.964844\n",
      "Train Epoch: 9 [656000/1309486 (50%)]\tLoss: 39116.648438\n",
      "Train Epoch: 9 [664000/1309486 (51%)]\tLoss: 38865.382812\n",
      "Train Epoch: 9 [672000/1309486 (51%)]\tLoss: 38453.453125\n",
      "Train Epoch: 9 [680000/1309486 (52%)]\tLoss: 35993.921875\n",
      "Train Epoch: 9 [688000/1309486 (53%)]\tLoss: 38961.851562\n",
      "Train Epoch: 9 [696000/1309486 (53%)]\tLoss: 37843.445312\n",
      "Train Epoch: 9 [704000/1309486 (54%)]\tLoss: 42424.695312\n",
      "Train Epoch: 9 [712000/1309486 (54%)]\tLoss: 38954.765625\n",
      "Train Epoch: 9 [720000/1309486 (55%)]\tLoss: 39095.277344\n",
      "Train Epoch: 9 [728000/1309486 (56%)]\tLoss: 42311.800781\n",
      "Train Epoch: 9 [736000/1309486 (56%)]\tLoss: 35788.773438\n",
      "Train Epoch: 9 [744000/1309486 (57%)]\tLoss: 39040.765625\n",
      "Train Epoch: 9 [752000/1309486 (57%)]\tLoss: 36737.906250\n",
      "Train Epoch: 9 [760000/1309486 (58%)]\tLoss: 36868.187500\n",
      "Train Epoch: 9 [768000/1309486 (59%)]\tLoss: 36305.933594\n",
      "Train Epoch: 9 [776000/1309486 (59%)]\tLoss: 34443.976562\n",
      "Train Epoch: 9 [784000/1309486 (60%)]\tLoss: 40126.828125\n",
      "Train Epoch: 9 [792000/1309486 (60%)]\tLoss: 36343.199219\n",
      "Train Epoch: 9 [800000/1309486 (61%)]\tLoss: 39857.863281\n",
      "Train Epoch: 9 [808000/1309486 (62%)]\tLoss: 38887.078125\n",
      "Train Epoch: 9 [816000/1309486 (62%)]\tLoss: 42263.042969\n",
      "Train Epoch: 9 [824000/1309486 (63%)]\tLoss: 36722.335938\n",
      "Train Epoch: 9 [832000/1309486 (64%)]\tLoss: 37416.484375\n",
      "Train Epoch: 9 [840000/1309486 (64%)]\tLoss: 41789.390625\n",
      "Train Epoch: 9 [848000/1309486 (65%)]\tLoss: 39500.960938\n",
      "Train Epoch: 9 [856000/1309486 (65%)]\tLoss: 38444.429688\n",
      "Train Epoch: 9 [864000/1309486 (66%)]\tLoss: 40341.164062\n",
      "Train Epoch: 9 [872000/1309486 (67%)]\tLoss: 41728.417969\n",
      "Train Epoch: 9 [880000/1309486 (67%)]\tLoss: 38212.632812\n",
      "Train Epoch: 9 [888000/1309486 (68%)]\tLoss: 33438.085938\n",
      "Train Epoch: 9 [896000/1309486 (68%)]\tLoss: 40251.765625\n",
      "Train Epoch: 9 [904000/1309486 (69%)]\tLoss: 38317.421875\n",
      "Train Epoch: 9 [912000/1309486 (70%)]\tLoss: 39334.343750\n",
      "Train Epoch: 9 [920000/1309486 (70%)]\tLoss: 37158.187500\n",
      "Train Epoch: 9 [928000/1309486 (71%)]\tLoss: 35555.031250\n",
      "Train Epoch: 9 [936000/1309486 (71%)]\tLoss: 36354.265625\n",
      "Train Epoch: 9 [944000/1309486 (72%)]\tLoss: 37356.496094\n",
      "Train Epoch: 9 [952000/1309486 (73%)]\tLoss: 39759.886719\n",
      "Train Epoch: 9 [960000/1309486 (73%)]\tLoss: 42435.984375\n",
      "Train Epoch: 9 [968000/1309486 (74%)]\tLoss: 35226.714844\n",
      "Train Epoch: 9 [976000/1309486 (75%)]\tLoss: 37825.074219\n",
      "Train Epoch: 9 [984000/1309486 (75%)]\tLoss: 38104.761719\n",
      "Train Epoch: 9 [992000/1309486 (76%)]\tLoss: 38600.210938\n",
      "Train Epoch: 9 [1000000/1309486 (76%)]\tLoss: 39324.894531\n",
      "Train Epoch: 9 [1008000/1309486 (77%)]\tLoss: 36656.582031\n",
      "Train Epoch: 9 [1016000/1309486 (78%)]\tLoss: 40049.910156\n",
      "Train Epoch: 9 [1024000/1309486 (78%)]\tLoss: 43424.617188\n",
      "Train Epoch: 9 [1032000/1309486 (79%)]\tLoss: 41156.683594\n",
      "Train Epoch: 9 [1040000/1309486 (79%)]\tLoss: 39800.285156\n",
      "Train Epoch: 9 [1048000/1309486 (80%)]\tLoss: 37865.812500\n",
      "Train Epoch: 9 [1056000/1309486 (81%)]\tLoss: 39366.437500\n",
      "Train Epoch: 9 [1064000/1309486 (81%)]\tLoss: 44933.953125\n",
      "Train Epoch: 9 [1072000/1309486 (82%)]\tLoss: 34318.390625\n",
      "Train Epoch: 9 [1080000/1309486 (82%)]\tLoss: 38099.429688\n",
      "Train Epoch: 9 [1088000/1309486 (83%)]\tLoss: 36445.371094\n",
      "Train Epoch: 9 [1096000/1309486 (84%)]\tLoss: 41516.976562\n",
      "Train Epoch: 9 [1104000/1309486 (84%)]\tLoss: 39745.847656\n",
      "Train Epoch: 9 [1112000/1309486 (85%)]\tLoss: 38813.742188\n",
      "Train Epoch: 9 [1120000/1309486 (86%)]\tLoss: 42277.109375\n",
      "Train Epoch: 9 [1128000/1309486 (86%)]\tLoss: 38020.742188\n",
      "Train Epoch: 9 [1136000/1309486 (87%)]\tLoss: 39413.421875\n",
      "Train Epoch: 9 [1144000/1309486 (87%)]\tLoss: 34882.660156\n",
      "Train Epoch: 9 [1152000/1309486 (88%)]\tLoss: 38262.468750\n",
      "Train Epoch: 9 [1160000/1309486 (89%)]\tLoss: 41730.386719\n",
      "Train Epoch: 9 [1168000/1309486 (89%)]\tLoss: 39057.226562\n",
      "Train Epoch: 9 [1176000/1309486 (90%)]\tLoss: 38723.019531\n",
      "Train Epoch: 9 [1184000/1309486 (90%)]\tLoss: 41311.070312\n",
      "Train Epoch: 9 [1192000/1309486 (91%)]\tLoss: 37312.656250\n",
      "Train Epoch: 9 [1200000/1309486 (92%)]\tLoss: 38083.789062\n",
      "Train Epoch: 9 [1208000/1309486 (92%)]\tLoss: 40244.371094\n",
      "Train Epoch: 9 [1216000/1309486 (93%)]\tLoss: 40132.359375\n",
      "Train Epoch: 9 [1224000/1309486 (93%)]\tLoss: 38705.777344\n",
      "Train Epoch: 9 [1232000/1309486 (94%)]\tLoss: 39587.000000\n",
      "Train Epoch: 9 [1240000/1309486 (95%)]\tLoss: 36009.339844\n",
      "Train Epoch: 9 [1248000/1309486 (95%)]\tLoss: 35859.132812\n",
      "Train Epoch: 9 [1256000/1309486 (96%)]\tLoss: 39733.921875\n",
      "Train Epoch: 9 [1264000/1309486 (97%)]\tLoss: 38810.898438\n",
      "Train Epoch: 9 [1272000/1309486 (97%)]\tLoss: 33250.781250\n",
      "Train Epoch: 9 [1280000/1309486 (98%)]\tLoss: 43422.546875\n",
      "Train Epoch: 9 [1288000/1309486 (98%)]\tLoss: 37043.546875\n",
      "Train Epoch: 9 [1296000/1309486 (99%)]\tLoss: 37318.261719\n",
      "Train Epoch: 9 [1304000/1309486 (100%)]\tLoss: 38567.191406\n",
      "Epoch 9 model saved!\n",
      "epoch train time:  785.09730339\n",
      "Train Epoch: 10 [8000/1309486 (1%)]\tLoss: 38387.996094\n",
      "Train Epoch: 10 [16000/1309486 (1%)]\tLoss: 38696.308594\n",
      "Train Epoch: 10 [24000/1309486 (2%)]\tLoss: 38260.734375\n",
      "Train Epoch: 10 [32000/1309486 (2%)]\tLoss: 35748.164062\n",
      "Train Epoch: 10 [40000/1309486 (3%)]\tLoss: 39882.546875\n",
      "Train Epoch: 10 [48000/1309486 (4%)]\tLoss: 36331.875000\n",
      "Train Epoch: 10 [56000/1309486 (4%)]\tLoss: 39901.031250\n",
      "Train Epoch: 10 [64000/1309486 (5%)]\tLoss: 37352.777344\n",
      "Train Epoch: 10 [72000/1309486 (5%)]\tLoss: 39174.984375\n",
      "Train Epoch: 10 [80000/1309486 (6%)]\tLoss: 37470.136719\n",
      "Train Epoch: 10 [88000/1309486 (7%)]\tLoss: 35501.250000\n",
      "Train Epoch: 10 [96000/1309486 (7%)]\tLoss: 38126.609375\n",
      "Train Epoch: 10 [104000/1309486 (8%)]\tLoss: 39047.269531\n",
      "Train Epoch: 10 [112000/1309486 (9%)]\tLoss: 38190.765625\n",
      "Train Epoch: 10 [120000/1309486 (9%)]\tLoss: 39203.121094\n",
      "Train Epoch: 10 [128000/1309486 (10%)]\tLoss: 38452.320312\n",
      "Train Epoch: 10 [136000/1309486 (10%)]\tLoss: 41127.894531\n",
      "Train Epoch: 10 [144000/1309486 (11%)]\tLoss: 37381.914062\n",
      "Train Epoch: 10 [152000/1309486 (12%)]\tLoss: 38478.648438\n",
      "Train Epoch: 10 [160000/1309486 (12%)]\tLoss: 39696.464844\n",
      "Train Epoch: 10 [168000/1309486 (13%)]\tLoss: 39425.726562\n",
      "Train Epoch: 10 [176000/1309486 (13%)]\tLoss: 38410.875000\n",
      "Train Epoch: 10 [184000/1309486 (14%)]\tLoss: 37128.906250\n",
      "Train Epoch: 10 [192000/1309486 (15%)]\tLoss: 39615.390625\n",
      "Train Epoch: 10 [200000/1309486 (15%)]\tLoss: 36680.609375\n",
      "Train Epoch: 10 [208000/1309486 (16%)]\tLoss: 36458.355469\n",
      "Train Epoch: 10 [216000/1309486 (16%)]\tLoss: 39826.773438\n",
      "Train Epoch: 10 [224000/1309486 (17%)]\tLoss: 37979.835938\n",
      "Train Epoch: 10 [232000/1309486 (18%)]\tLoss: 40937.007812\n",
      "Train Epoch: 10 [240000/1309486 (18%)]\tLoss: 36949.343750\n",
      "Train Epoch: 10 [248000/1309486 (19%)]\tLoss: 38242.421875\n",
      "Train Epoch: 10 [256000/1309486 (20%)]\tLoss: 38850.003906\n",
      "Train Epoch: 10 [264000/1309486 (20%)]\tLoss: 41575.273438\n",
      "Train Epoch: 10 [272000/1309486 (21%)]\tLoss: 41109.300781\n",
      "Train Epoch: 10 [280000/1309486 (21%)]\tLoss: 37190.738281\n",
      "Train Epoch: 10 [288000/1309486 (22%)]\tLoss: 37574.820312\n",
      "Train Epoch: 10 [296000/1309486 (23%)]\tLoss: 36843.035156\n",
      "Train Epoch: 10 [304000/1309486 (23%)]\tLoss: 43079.968750\n",
      "Train Epoch: 10 [312000/1309486 (24%)]\tLoss: 43862.308594\n",
      "Train Epoch: 10 [320000/1309486 (24%)]\tLoss: 41314.132812\n",
      "Train Epoch: 10 [328000/1309486 (25%)]\tLoss: 41428.703125\n",
      "Train Epoch: 10 [336000/1309486 (26%)]\tLoss: 42580.535156\n",
      "Train Epoch: 10 [344000/1309486 (26%)]\tLoss: 40313.390625\n",
      "Train Epoch: 10 [352000/1309486 (27%)]\tLoss: 36692.929688\n",
      "Train Epoch: 10 [360000/1309486 (27%)]\tLoss: 39094.585938\n",
      "Train Epoch: 10 [368000/1309486 (28%)]\tLoss: 38581.898438\n",
      "Train Epoch: 10 [376000/1309486 (29%)]\tLoss: 39320.304688\n",
      "Train Epoch: 10 [384000/1309486 (29%)]\tLoss: 36367.503906\n",
      "Train Epoch: 10 [392000/1309486 (30%)]\tLoss: 39120.429688\n",
      "Train Epoch: 10 [400000/1309486 (31%)]\tLoss: 39102.609375\n",
      "Train Epoch: 10 [408000/1309486 (31%)]\tLoss: 36370.003906\n",
      "Train Epoch: 10 [416000/1309486 (32%)]\tLoss: 34765.562500\n",
      "Train Epoch: 10 [424000/1309486 (32%)]\tLoss: 35077.593750\n",
      "Train Epoch: 10 [432000/1309486 (33%)]\tLoss: 42987.328125\n",
      "Train Epoch: 10 [440000/1309486 (34%)]\tLoss: 35511.500000\n",
      "Train Epoch: 10 [448000/1309486 (34%)]\tLoss: 38424.890625\n",
      "Train Epoch: 10 [456000/1309486 (35%)]\tLoss: 37840.531250\n",
      "Train Epoch: 10 [464000/1309486 (35%)]\tLoss: 40915.667969\n",
      "Train Epoch: 10 [472000/1309486 (36%)]\tLoss: 39787.593750\n",
      "Train Epoch: 10 [480000/1309486 (37%)]\tLoss: 36384.742188\n",
      "Train Epoch: 10 [488000/1309486 (37%)]\tLoss: 39543.136719\n",
      "Train Epoch: 10 [496000/1309486 (38%)]\tLoss: 36975.226562\n",
      "Train Epoch: 10 [504000/1309486 (38%)]\tLoss: 40717.816406\n",
      "Train Epoch: 10 [512000/1309486 (39%)]\tLoss: 39833.585938\n",
      "Train Epoch: 10 [520000/1309486 (40%)]\tLoss: 41444.312500\n",
      "Train Epoch: 10 [528000/1309486 (40%)]\tLoss: 39358.089844\n",
      "Train Epoch: 10 [536000/1309486 (41%)]\tLoss: 39926.855469\n",
      "Train Epoch: 10 [544000/1309486 (42%)]\tLoss: 41833.218750\n",
      "Train Epoch: 10 [552000/1309486 (42%)]\tLoss: 40719.152344\n",
      "Train Epoch: 10 [560000/1309486 (43%)]\tLoss: 37462.132812\n",
      "Train Epoch: 10 [568000/1309486 (43%)]\tLoss: 36459.886719\n",
      "Train Epoch: 10 [576000/1309486 (44%)]\tLoss: 40359.183594\n",
      "Train Epoch: 10 [584000/1309486 (45%)]\tLoss: 37400.546875\n",
      "Train Epoch: 10 [592000/1309486 (45%)]\tLoss: 37720.398438\n",
      "Train Epoch: 10 [600000/1309486 (46%)]\tLoss: 38612.636719\n",
      "Train Epoch: 10 [608000/1309486 (46%)]\tLoss: 36348.980469\n",
      "Train Epoch: 10 [616000/1309486 (47%)]\tLoss: 35194.710938\n",
      "Train Epoch: 10 [624000/1309486 (48%)]\tLoss: 37583.203125\n",
      "Train Epoch: 10 [632000/1309486 (48%)]\tLoss: 42249.648438\n",
      "Train Epoch: 10 [640000/1309486 (49%)]\tLoss: 38250.980469\n",
      "Train Epoch: 10 [648000/1309486 (49%)]\tLoss: 43327.042969\n",
      "Train Epoch: 10 [656000/1309486 (50%)]\tLoss: 39902.636719\n",
      "Train Epoch: 10 [664000/1309486 (51%)]\tLoss: 33681.542969\n",
      "Train Epoch: 10 [672000/1309486 (51%)]\tLoss: 38273.414062\n",
      "Train Epoch: 10 [680000/1309486 (52%)]\tLoss: 36812.460938\n",
      "Train Epoch: 10 [688000/1309486 (53%)]\tLoss: 36273.335938\n",
      "Train Epoch: 10 [696000/1309486 (53%)]\tLoss: 37693.976562\n",
      "Train Epoch: 10 [704000/1309486 (54%)]\tLoss: 34687.140625\n",
      "Train Epoch: 10 [712000/1309486 (54%)]\tLoss: 39635.765625\n",
      "Train Epoch: 10 [720000/1309486 (55%)]\tLoss: 39660.140625\n",
      "Train Epoch: 10 [728000/1309486 (56%)]\tLoss: 36299.140625\n",
      "Train Epoch: 10 [736000/1309486 (56%)]\tLoss: 37745.609375\n",
      "Train Epoch: 10 [744000/1309486 (57%)]\tLoss: 37076.187500\n",
      "Train Epoch: 10 [752000/1309486 (57%)]\tLoss: 40034.058594\n",
      "Train Epoch: 10 [760000/1309486 (58%)]\tLoss: 39906.609375\n",
      "Train Epoch: 10 [768000/1309486 (59%)]\tLoss: 36052.578125\n",
      "Train Epoch: 10 [776000/1309486 (59%)]\tLoss: 41034.039062\n",
      "Train Epoch: 10 [784000/1309486 (60%)]\tLoss: 38076.628906\n",
      "Train Epoch: 10 [792000/1309486 (60%)]\tLoss: 36666.593750\n",
      "Train Epoch: 10 [800000/1309486 (61%)]\tLoss: 36709.218750\n",
      "Train Epoch: 10 [808000/1309486 (62%)]\tLoss: 38790.292969\n",
      "Train Epoch: 10 [816000/1309486 (62%)]\tLoss: 37353.140625\n",
      "Train Epoch: 10 [824000/1309486 (63%)]\tLoss: 38603.164062\n",
      "Train Epoch: 10 [832000/1309486 (64%)]\tLoss: 33376.156250\n",
      "Train Epoch: 10 [840000/1309486 (64%)]\tLoss: 35682.835938\n",
      "Train Epoch: 10 [848000/1309486 (65%)]\tLoss: 35537.898438\n",
      "Train Epoch: 10 [856000/1309486 (65%)]\tLoss: 38269.644531\n",
      "Train Epoch: 10 [864000/1309486 (66%)]\tLoss: 40693.421875\n",
      "Train Epoch: 10 [872000/1309486 (67%)]\tLoss: 38810.945312\n",
      "Train Epoch: 10 [880000/1309486 (67%)]\tLoss: 39250.136719\n",
      "Train Epoch: 10 [888000/1309486 (68%)]\tLoss: 38867.406250\n",
      "Train Epoch: 10 [896000/1309486 (68%)]\tLoss: 39634.250000\n",
      "Train Epoch: 10 [904000/1309486 (69%)]\tLoss: 38116.718750\n",
      "Train Epoch: 10 [912000/1309486 (70%)]\tLoss: 38542.570312\n",
      "Train Epoch: 10 [920000/1309486 (70%)]\tLoss: 37921.949219\n",
      "Train Epoch: 10 [928000/1309486 (71%)]\tLoss: 38392.117188\n",
      "Train Epoch: 10 [936000/1309486 (71%)]\tLoss: 38761.617188\n",
      "Train Epoch: 10 [944000/1309486 (72%)]\tLoss: 36388.820312\n",
      "Train Epoch: 10 [952000/1309486 (73%)]\tLoss: 34917.160156\n",
      "Train Epoch: 10 [960000/1309486 (73%)]\tLoss: 37138.296875\n",
      "Train Epoch: 10 [968000/1309486 (74%)]\tLoss: 39319.125000\n",
      "Train Epoch: 10 [976000/1309486 (75%)]\tLoss: 38030.066406\n",
      "Train Epoch: 10 [984000/1309486 (75%)]\tLoss: 38279.140625\n",
      "Train Epoch: 10 [992000/1309486 (76%)]\tLoss: 39145.320312\n",
      "Train Epoch: 10 [1000000/1309486 (76%)]\tLoss: 38870.226562\n",
      "Train Epoch: 10 [1008000/1309486 (77%)]\tLoss: 40458.003906\n",
      "Train Epoch: 10 [1016000/1309486 (78%)]\tLoss: 38365.660156\n",
      "Train Epoch: 10 [1024000/1309486 (78%)]\tLoss: 42158.527344\n",
      "Train Epoch: 10 [1032000/1309486 (79%)]\tLoss: 39082.253906\n",
      "Train Epoch: 10 [1040000/1309486 (79%)]\tLoss: 37434.789062\n",
      "Train Epoch: 10 [1048000/1309486 (80%)]\tLoss: 42798.957031\n",
      "Train Epoch: 10 [1056000/1309486 (81%)]\tLoss: 44301.148438\n",
      "Train Epoch: 10 [1064000/1309486 (81%)]\tLoss: 40488.628906\n",
      "Train Epoch: 10 [1072000/1309486 (82%)]\tLoss: 34119.125000\n",
      "Train Epoch: 10 [1080000/1309486 (82%)]\tLoss: 38810.152344\n",
      "Train Epoch: 10 [1088000/1309486 (83%)]\tLoss: 37747.382812\n",
      "Train Epoch: 10 [1096000/1309486 (84%)]\tLoss: 37982.429688\n",
      "Train Epoch: 10 [1104000/1309486 (84%)]\tLoss: 43045.136719\n",
      "Train Epoch: 10 [1112000/1309486 (85%)]\tLoss: 38999.398438\n",
      "Train Epoch: 10 [1120000/1309486 (86%)]\tLoss: 36120.316406\n",
      "Train Epoch: 10 [1128000/1309486 (86%)]\tLoss: 40342.230469\n",
      "Train Epoch: 10 [1136000/1309486 (87%)]\tLoss: 37706.132812\n",
      "Train Epoch: 10 [1144000/1309486 (87%)]\tLoss: 37468.261719\n",
      "Train Epoch: 10 [1152000/1309486 (88%)]\tLoss: 36253.414062\n",
      "Train Epoch: 10 [1160000/1309486 (89%)]\tLoss: 37710.371094\n",
      "Train Epoch: 10 [1168000/1309486 (89%)]\tLoss: 37171.859375\n",
      "Train Epoch: 10 [1176000/1309486 (90%)]\tLoss: 36208.292969\n",
      "Train Epoch: 10 [1184000/1309486 (90%)]\tLoss: 42696.699219\n",
      "Train Epoch: 10 [1192000/1309486 (91%)]\tLoss: 41791.500000\n",
      "Train Epoch: 10 [1200000/1309486 (92%)]\tLoss: 36001.343750\n",
      "Train Epoch: 10 [1208000/1309486 (92%)]\tLoss: 37834.789062\n",
      "Train Epoch: 10 [1216000/1309486 (93%)]\tLoss: 40934.953125\n",
      "Train Epoch: 10 [1224000/1309486 (93%)]\tLoss: 37213.980469\n",
      "Train Epoch: 10 [1232000/1309486 (94%)]\tLoss: 38300.171875\n",
      "Train Epoch: 10 [1240000/1309486 (95%)]\tLoss: 43318.917969\n",
      "Train Epoch: 10 [1248000/1309486 (95%)]\tLoss: 36814.097656\n",
      "Train Epoch: 10 [1256000/1309486 (96%)]\tLoss: 37095.417969\n",
      "Train Epoch: 10 [1264000/1309486 (97%)]\tLoss: 36977.289062\n",
      "Train Epoch: 10 [1272000/1309486 (97%)]\tLoss: 37831.480469\n",
      "Train Epoch: 10 [1280000/1309486 (98%)]\tLoss: 36186.375000\n",
      "Train Epoch: 10 [1288000/1309486 (98%)]\tLoss: 38379.453125\n",
      "Train Epoch: 10 [1296000/1309486 (99%)]\tLoss: 39364.335938\n",
      "Train Epoch: 10 [1304000/1309486 (100%)]\tLoss: 34213.968750\n",
      "Epoch 10 model saved!\n",
      "epoch train time:  785.67957544\n",
      "Train Epoch: 11 [8000/1309486 (1%)]\tLoss: 39658.722656\n",
      "Train Epoch: 11 [16000/1309486 (1%)]\tLoss: 38200.453125\n",
      "Train Epoch: 11 [24000/1309486 (2%)]\tLoss: 34026.433594\n",
      "Train Epoch: 11 [32000/1309486 (2%)]\tLoss: 38225.949219\n",
      "Train Epoch: 11 [40000/1309486 (3%)]\tLoss: 41653.929688\n",
      "Train Epoch: 11 [48000/1309486 (4%)]\tLoss: 38129.359375\n",
      "Train Epoch: 11 [56000/1309486 (4%)]\tLoss: 41698.253906\n",
      "Train Epoch: 11 [64000/1309486 (5%)]\tLoss: 35462.855469\n",
      "Train Epoch: 11 [72000/1309486 (5%)]\tLoss: 39061.531250\n",
      "Train Epoch: 11 [80000/1309486 (6%)]\tLoss: 39023.230469\n",
      "Train Epoch: 11 [88000/1309486 (7%)]\tLoss: 37910.351562\n",
      "Train Epoch: 11 [96000/1309486 (7%)]\tLoss: 39432.570312\n",
      "Train Epoch: 11 [104000/1309486 (8%)]\tLoss: 38371.214844\n",
      "Train Epoch: 11 [112000/1309486 (9%)]\tLoss: 42024.488281\n",
      "Train Epoch: 11 [120000/1309486 (9%)]\tLoss: 38357.753906\n",
      "Train Epoch: 11 [128000/1309486 (10%)]\tLoss: 34878.468750\n",
      "Train Epoch: 11 [136000/1309486 (10%)]\tLoss: 36581.605469\n",
      "Train Epoch: 11 [144000/1309486 (11%)]\tLoss: 37136.601562\n",
      "Train Epoch: 11 [152000/1309486 (12%)]\tLoss: 36176.578125\n",
      "Train Epoch: 11 [160000/1309486 (12%)]\tLoss: 43560.082031\n",
      "Train Epoch: 11 [168000/1309486 (13%)]\tLoss: 42144.351562\n",
      "Train Epoch: 11 [176000/1309486 (13%)]\tLoss: 40958.984375\n",
      "Train Epoch: 11 [184000/1309486 (14%)]\tLoss: 37182.011719\n",
      "Train Epoch: 11 [192000/1309486 (15%)]\tLoss: 40229.531250\n",
      "Train Epoch: 11 [200000/1309486 (15%)]\tLoss: 42331.968750\n",
      "Train Epoch: 11 [208000/1309486 (16%)]\tLoss: 39714.000000\n",
      "Train Epoch: 11 [216000/1309486 (16%)]\tLoss: 33654.371094\n",
      "Train Epoch: 11 [224000/1309486 (17%)]\tLoss: 35542.148438\n",
      "Train Epoch: 11 [232000/1309486 (18%)]\tLoss: 39019.406250\n",
      "Train Epoch: 11 [240000/1309486 (18%)]\tLoss: 40810.539062\n",
      "Train Epoch: 11 [248000/1309486 (19%)]\tLoss: 35085.781250\n",
      "Train Epoch: 11 [256000/1309486 (20%)]\tLoss: 38812.308594\n",
      "Train Epoch: 11 [264000/1309486 (20%)]\tLoss: 35745.429688\n",
      "Train Epoch: 11 [272000/1309486 (21%)]\tLoss: 39465.359375\n",
      "Train Epoch: 11 [280000/1309486 (21%)]\tLoss: 36894.859375\n",
      "Train Epoch: 11 [288000/1309486 (22%)]\tLoss: 38775.710938\n",
      "Train Epoch: 11 [296000/1309486 (23%)]\tLoss: 38973.429688\n",
      "Train Epoch: 11 [304000/1309486 (23%)]\tLoss: 38657.738281\n",
      "Train Epoch: 11 [312000/1309486 (24%)]\tLoss: 39447.265625\n",
      "Train Epoch: 11 [320000/1309486 (24%)]\tLoss: 40490.261719\n",
      "Train Epoch: 11 [328000/1309486 (25%)]\tLoss: 40163.523438\n",
      "Train Epoch: 11 [336000/1309486 (26%)]\tLoss: 39922.242188\n",
      "Train Epoch: 11 [344000/1309486 (26%)]\tLoss: 38738.054688\n",
      "Train Epoch: 11 [352000/1309486 (27%)]\tLoss: 39914.023438\n",
      "Train Epoch: 11 [360000/1309486 (27%)]\tLoss: 41987.257812\n",
      "Train Epoch: 11 [368000/1309486 (28%)]\tLoss: 38328.527344\n",
      "Train Epoch: 11 [376000/1309486 (29%)]\tLoss: 39212.148438\n",
      "Train Epoch: 11 [384000/1309486 (29%)]\tLoss: 35915.453125\n",
      "Train Epoch: 11 [392000/1309486 (30%)]\tLoss: 43106.398438\n",
      "Train Epoch: 11 [400000/1309486 (31%)]\tLoss: 39872.484375\n",
      "Train Epoch: 11 [408000/1309486 (31%)]\tLoss: 33218.570312\n",
      "Train Epoch: 11 [416000/1309486 (32%)]\tLoss: 39117.742188\n",
      "Train Epoch: 11 [424000/1309486 (32%)]\tLoss: 40059.351562\n",
      "Train Epoch: 11 [432000/1309486 (33%)]\tLoss: 37532.664062\n",
      "Train Epoch: 11 [440000/1309486 (34%)]\tLoss: 41936.746094\n",
      "Train Epoch: 11 [448000/1309486 (34%)]\tLoss: 37417.144531\n",
      "Train Epoch: 11 [456000/1309486 (35%)]\tLoss: 36203.746094\n",
      "Train Epoch: 11 [464000/1309486 (35%)]\tLoss: 39211.312500\n",
      "Train Epoch: 11 [472000/1309486 (36%)]\tLoss: 40282.101562\n",
      "Train Epoch: 11 [480000/1309486 (37%)]\tLoss: 39313.984375\n",
      "Train Epoch: 11 [488000/1309486 (37%)]\tLoss: 33990.023438\n",
      "Train Epoch: 11 [496000/1309486 (38%)]\tLoss: 37732.257812\n",
      "Train Epoch: 11 [504000/1309486 (38%)]\tLoss: 36888.343750\n",
      "Train Epoch: 11 [512000/1309486 (39%)]\tLoss: 37088.039062\n",
      "Train Epoch: 11 [520000/1309486 (40%)]\tLoss: 37003.140625\n",
      "Train Epoch: 11 [528000/1309486 (40%)]\tLoss: 34389.796875\n",
      "Train Epoch: 11 [536000/1309486 (41%)]\tLoss: 35416.742188\n",
      "Train Epoch: 11 [544000/1309486 (42%)]\tLoss: 37563.832031\n",
      "Train Epoch: 11 [552000/1309486 (42%)]\tLoss: 34834.015625\n",
      "Train Epoch: 11 [560000/1309486 (43%)]\tLoss: 38355.164062\n",
      "Train Epoch: 11 [568000/1309486 (43%)]\tLoss: 39526.839844\n",
      "Train Epoch: 11 [576000/1309486 (44%)]\tLoss: 35898.382812\n",
      "Train Epoch: 11 [584000/1309486 (45%)]\tLoss: 39846.800781\n",
      "Train Epoch: 11 [592000/1309486 (45%)]\tLoss: 38036.718750\n",
      "Train Epoch: 11 [600000/1309486 (46%)]\tLoss: 38543.535156\n",
      "Train Epoch: 11 [608000/1309486 (46%)]\tLoss: 38802.062500\n",
      "Train Epoch: 11 [616000/1309486 (47%)]\tLoss: 39480.355469\n",
      "Train Epoch: 11 [624000/1309486 (48%)]\tLoss: 44251.703125\n",
      "Train Epoch: 11 [632000/1309486 (48%)]\tLoss: 42860.941406\n",
      "Train Epoch: 11 [640000/1309486 (49%)]\tLoss: 38134.246094\n",
      "Train Epoch: 11 [648000/1309486 (49%)]\tLoss: 36874.050781\n",
      "Train Epoch: 11 [656000/1309486 (50%)]\tLoss: 39963.125000\n",
      "Train Epoch: 11 [664000/1309486 (51%)]\tLoss: 40116.875000\n",
      "Train Epoch: 11 [672000/1309486 (51%)]\tLoss: 38658.085938\n",
      "Train Epoch: 11 [680000/1309486 (52%)]\tLoss: 39637.902344\n",
      "Train Epoch: 11 [688000/1309486 (53%)]\tLoss: 42631.226562\n",
      "Train Epoch: 11 [696000/1309486 (53%)]\tLoss: 43071.320312\n",
      "Train Epoch: 11 [704000/1309486 (54%)]\tLoss: 38474.457031\n",
      "Train Epoch: 11 [712000/1309486 (54%)]\tLoss: 41401.511719\n",
      "Train Epoch: 11 [720000/1309486 (55%)]\tLoss: 39178.769531\n",
      "Train Epoch: 11 [728000/1309486 (56%)]\tLoss: 36150.679688\n",
      "Train Epoch: 11 [736000/1309486 (56%)]\tLoss: 35472.734375\n",
      "Train Epoch: 11 [744000/1309486 (57%)]\tLoss: 36437.218750\n",
      "Train Epoch: 11 [752000/1309486 (57%)]\tLoss: 46303.906250\n",
      "Train Epoch: 11 [760000/1309486 (58%)]\tLoss: 41943.417969\n",
      "Train Epoch: 11 [768000/1309486 (59%)]\tLoss: 38182.421875\n",
      "Train Epoch: 11 [776000/1309486 (59%)]\tLoss: 40274.156250\n",
      "Train Epoch: 11 [784000/1309486 (60%)]\tLoss: 42053.417969\n",
      "Train Epoch: 11 [792000/1309486 (60%)]\tLoss: 37988.976562\n",
      "Train Epoch: 11 [800000/1309486 (61%)]\tLoss: 38072.835938\n",
      "Train Epoch: 11 [808000/1309486 (62%)]\tLoss: 35281.582031\n",
      "Train Epoch: 11 [816000/1309486 (62%)]\tLoss: 35801.515625\n",
      "Train Epoch: 11 [824000/1309486 (63%)]\tLoss: 35984.550781\n",
      "Train Epoch: 11 [832000/1309486 (64%)]\tLoss: 41314.648438\n",
      "Train Epoch: 11 [840000/1309486 (64%)]\tLoss: 38250.914062\n",
      "Train Epoch: 11 [848000/1309486 (65%)]\tLoss: 35427.390625\n",
      "Train Epoch: 11 [856000/1309486 (65%)]\tLoss: 38226.699219\n",
      "Train Epoch: 11 [864000/1309486 (66%)]\tLoss: 37071.734375\n",
      "Train Epoch: 11 [872000/1309486 (67%)]\tLoss: 34731.335938\n",
      "Train Epoch: 11 [880000/1309486 (67%)]\tLoss: 36613.820312\n",
      "Train Epoch: 11 [888000/1309486 (68%)]\tLoss: 31942.537109\n",
      "Train Epoch: 11 [896000/1309486 (68%)]\tLoss: 34476.648438\n",
      "Train Epoch: 11 [904000/1309486 (69%)]\tLoss: 37081.480469\n",
      "Train Epoch: 11 [912000/1309486 (70%)]\tLoss: 36370.808594\n",
      "Train Epoch: 11 [920000/1309486 (70%)]\tLoss: 41501.515625\n",
      "Train Epoch: 11 [928000/1309486 (71%)]\tLoss: 35851.359375\n",
      "Train Epoch: 11 [936000/1309486 (71%)]\tLoss: 42257.273438\n",
      "Train Epoch: 11 [944000/1309486 (72%)]\tLoss: 35670.875000\n",
      "Train Epoch: 11 [952000/1309486 (73%)]\tLoss: 38918.433594\n",
      "Train Epoch: 11 [960000/1309486 (73%)]\tLoss: 42722.847656\n",
      "Train Epoch: 11 [968000/1309486 (74%)]\tLoss: 36641.585938\n",
      "Train Epoch: 11 [976000/1309486 (75%)]\tLoss: 36502.300781\n",
      "Train Epoch: 11 [984000/1309486 (75%)]\tLoss: 39174.730469\n",
      "Train Epoch: 11 [992000/1309486 (76%)]\tLoss: 41243.437500\n",
      "Train Epoch: 11 [1000000/1309486 (76%)]\tLoss: 39639.527344\n",
      "Train Epoch: 11 [1008000/1309486 (77%)]\tLoss: 41863.820312\n",
      "Train Epoch: 11 [1016000/1309486 (78%)]\tLoss: 44187.921875\n",
      "Train Epoch: 11 [1024000/1309486 (78%)]\tLoss: 37186.042969\n",
      "Train Epoch: 11 [1032000/1309486 (79%)]\tLoss: 41419.515625\n",
      "Train Epoch: 11 [1040000/1309486 (79%)]\tLoss: 41348.539062\n",
      "Train Epoch: 11 [1048000/1309486 (80%)]\tLoss: 35742.968750\n",
      "Train Epoch: 11 [1056000/1309486 (81%)]\tLoss: 36374.792969\n",
      "Train Epoch: 11 [1064000/1309486 (81%)]\tLoss: 41717.968750\n",
      "Train Epoch: 11 [1072000/1309486 (82%)]\tLoss: 40047.007812\n",
      "Train Epoch: 11 [1080000/1309486 (82%)]\tLoss: 41505.648438\n",
      "Train Epoch: 11 [1088000/1309486 (83%)]\tLoss: 42373.183594\n",
      "Train Epoch: 11 [1096000/1309486 (84%)]\tLoss: 39093.203125\n",
      "Train Epoch: 11 [1104000/1309486 (84%)]\tLoss: 36428.117188\n",
      "Train Epoch: 11 [1112000/1309486 (85%)]\tLoss: 39742.453125\n",
      "Train Epoch: 11 [1120000/1309486 (86%)]\tLoss: 42071.597656\n",
      "Train Epoch: 11 [1128000/1309486 (86%)]\tLoss: 35340.308594\n",
      "Train Epoch: 11 [1136000/1309486 (87%)]\tLoss: 37054.187500\n",
      "Train Epoch: 11 [1144000/1309486 (87%)]\tLoss: 38373.070312\n",
      "Train Epoch: 11 [1152000/1309486 (88%)]\tLoss: 40164.289062\n",
      "Train Epoch: 11 [1160000/1309486 (89%)]\tLoss: 37976.148438\n",
      "Train Epoch: 11 [1168000/1309486 (89%)]\tLoss: 36995.328125\n",
      "Train Epoch: 11 [1176000/1309486 (90%)]\tLoss: 38802.984375\n",
      "Train Epoch: 11 [1184000/1309486 (90%)]\tLoss: 40155.332031\n",
      "Train Epoch: 11 [1192000/1309486 (91%)]\tLoss: 35907.800781\n",
      "Train Epoch: 11 [1200000/1309486 (92%)]\tLoss: 34977.355469\n",
      "Train Epoch: 11 [1208000/1309486 (92%)]\tLoss: 38696.132812\n",
      "Train Epoch: 11 [1216000/1309486 (93%)]\tLoss: 37234.574219\n",
      "Train Epoch: 11 [1224000/1309486 (93%)]\tLoss: 41762.203125\n",
      "Train Epoch: 11 [1232000/1309486 (94%)]\tLoss: 41748.460938\n",
      "Train Epoch: 11 [1240000/1309486 (95%)]\tLoss: 38869.171875\n",
      "Train Epoch: 11 [1248000/1309486 (95%)]\tLoss: 41896.015625\n",
      "Train Epoch: 11 [1256000/1309486 (96%)]\tLoss: 39388.460938\n",
      "Train Epoch: 11 [1264000/1309486 (97%)]\tLoss: 43000.375000\n",
      "Train Epoch: 11 [1272000/1309486 (97%)]\tLoss: 36944.828125\n",
      "Train Epoch: 11 [1280000/1309486 (98%)]\tLoss: 37177.015625\n",
      "Train Epoch: 11 [1288000/1309486 (98%)]\tLoss: 35768.554688\n",
      "Train Epoch: 11 [1296000/1309486 (99%)]\tLoss: 42319.683594\n",
      "Train Epoch: 11 [1304000/1309486 (100%)]\tLoss: 37518.984375\n",
      "Epoch 11 model saved!\n",
      "epoch train time:  784.65809870\n",
      "Train Epoch: 12 [8000/1309486 (1%)]\tLoss: 37454.894531\n",
      "Train Epoch: 12 [16000/1309486 (1%)]\tLoss: 39375.441406\n",
      "Train Epoch: 12 [24000/1309486 (2%)]\tLoss: 36416.453125\n",
      "Train Epoch: 12 [32000/1309486 (2%)]\tLoss: 37897.628906\n",
      "Train Epoch: 12 [40000/1309486 (3%)]\tLoss: 40492.648438\n",
      "Train Epoch: 12 [48000/1309486 (4%)]\tLoss: 38686.726562\n",
      "Train Epoch: 12 [56000/1309486 (4%)]\tLoss: 36129.656250\n",
      "Train Epoch: 12 [64000/1309486 (5%)]\tLoss: 36176.515625\n",
      "Train Epoch: 12 [72000/1309486 (5%)]\tLoss: 38694.187500\n",
      "Train Epoch: 12 [80000/1309486 (6%)]\tLoss: 39806.554688\n",
      "Train Epoch: 12 [88000/1309486 (7%)]\tLoss: 37071.464844\n",
      "Train Epoch: 12 [96000/1309486 (7%)]\tLoss: 37691.187500\n",
      "Train Epoch: 12 [104000/1309486 (8%)]\tLoss: 40788.820312\n",
      "Train Epoch: 12 [112000/1309486 (9%)]\tLoss: 39746.265625\n",
      "Train Epoch: 12 [120000/1309486 (9%)]\tLoss: 36175.507812\n",
      "Train Epoch: 12 [128000/1309486 (10%)]\tLoss: 36085.078125\n",
      "Train Epoch: 12 [136000/1309486 (10%)]\tLoss: 44262.664062\n",
      "Train Epoch: 12 [144000/1309486 (11%)]\tLoss: 37368.089844\n",
      "Train Epoch: 12 [152000/1309486 (12%)]\tLoss: 41258.050781\n",
      "Train Epoch: 12 [160000/1309486 (12%)]\tLoss: 38842.136719\n",
      "Train Epoch: 12 [168000/1309486 (13%)]\tLoss: 36690.031250\n",
      "Train Epoch: 12 [176000/1309486 (13%)]\tLoss: 36655.367188\n",
      "Train Epoch: 12 [184000/1309486 (14%)]\tLoss: 36772.937500\n",
      "Train Epoch: 12 [192000/1309486 (15%)]\tLoss: 40722.269531\n",
      "Train Epoch: 12 [200000/1309486 (15%)]\tLoss: 34434.144531\n",
      "Train Epoch: 12 [208000/1309486 (16%)]\tLoss: 36674.421875\n",
      "Train Epoch: 12 [216000/1309486 (16%)]\tLoss: 39752.316406\n",
      "Train Epoch: 12 [224000/1309486 (17%)]\tLoss: 40098.621094\n",
      "Train Epoch: 12 [232000/1309486 (18%)]\tLoss: 40731.546875\n",
      "Train Epoch: 12 [240000/1309486 (18%)]\tLoss: 37837.328125\n",
      "Train Epoch: 12 [248000/1309486 (19%)]\tLoss: 38912.890625\n",
      "Train Epoch: 12 [256000/1309486 (20%)]\tLoss: 39863.156250\n",
      "Train Epoch: 12 [264000/1309486 (20%)]\tLoss: 35668.730469\n",
      "Train Epoch: 12 [272000/1309486 (21%)]\tLoss: 36213.058594\n",
      "Train Epoch: 12 [280000/1309486 (21%)]\tLoss: 36893.585938\n",
      "Train Epoch: 12 [288000/1309486 (22%)]\tLoss: 38059.421875\n",
      "Train Epoch: 12 [296000/1309486 (23%)]\tLoss: 42608.179688\n",
      "Train Epoch: 12 [304000/1309486 (23%)]\tLoss: 37993.570312\n",
      "Train Epoch: 12 [312000/1309486 (24%)]\tLoss: 35446.414062\n",
      "Train Epoch: 12 [320000/1309486 (24%)]\tLoss: 40114.968750\n",
      "Train Epoch: 12 [328000/1309486 (25%)]\tLoss: 44046.765625\n",
      "Train Epoch: 12 [336000/1309486 (26%)]\tLoss: 35750.437500\n",
      "Train Epoch: 12 [344000/1309486 (26%)]\tLoss: 40865.937500\n",
      "Train Epoch: 12 [352000/1309486 (27%)]\tLoss: 34257.054688\n",
      "Train Epoch: 12 [360000/1309486 (27%)]\tLoss: 38553.625000\n",
      "Train Epoch: 12 [368000/1309486 (28%)]\tLoss: 44931.328125\n",
      "Train Epoch: 12 [376000/1309486 (29%)]\tLoss: 35506.832031\n",
      "Train Epoch: 12 [384000/1309486 (29%)]\tLoss: 37938.105469\n",
      "Train Epoch: 12 [392000/1309486 (30%)]\tLoss: 38916.601562\n",
      "Train Epoch: 12 [400000/1309486 (31%)]\tLoss: 43534.050781\n",
      "Train Epoch: 12 [408000/1309486 (31%)]\tLoss: 43607.789062\n",
      "Train Epoch: 12 [416000/1309486 (32%)]\tLoss: 33675.097656\n",
      "Train Epoch: 12 [424000/1309486 (32%)]\tLoss: 38089.703125\n",
      "Train Epoch: 12 [432000/1309486 (33%)]\tLoss: 38138.976562\n",
      "Train Epoch: 12 [440000/1309486 (34%)]\tLoss: 41133.531250\n",
      "Train Epoch: 12 [448000/1309486 (34%)]\tLoss: 35521.417969\n",
      "Train Epoch: 12 [456000/1309486 (35%)]\tLoss: 38603.914062\n",
      "Train Epoch: 12 [464000/1309486 (35%)]\tLoss: 36606.441406\n",
      "Train Epoch: 12 [472000/1309486 (36%)]\tLoss: 39067.656250\n",
      "Train Epoch: 12 [480000/1309486 (37%)]\tLoss: 36234.214844\n",
      "Train Epoch: 12 [488000/1309486 (37%)]\tLoss: 39219.316406\n",
      "Train Epoch: 12 [496000/1309486 (38%)]\tLoss: 35068.273438\n",
      "Train Epoch: 12 [504000/1309486 (38%)]\tLoss: 39240.113281\n",
      "Train Epoch: 12 [512000/1309486 (39%)]\tLoss: 38187.968750\n",
      "Train Epoch: 12 [520000/1309486 (40%)]\tLoss: 47891.929688\n",
      "Train Epoch: 12 [528000/1309486 (40%)]\tLoss: 41112.835938\n",
      "Train Epoch: 12 [536000/1309486 (41%)]\tLoss: 37447.554688\n",
      "Train Epoch: 12 [544000/1309486 (42%)]\tLoss: 39991.148438\n",
      "Train Epoch: 12 [552000/1309486 (42%)]\tLoss: 34577.046875\n",
      "Train Epoch: 12 [560000/1309486 (43%)]\tLoss: 43577.460938\n",
      "Train Epoch: 12 [568000/1309486 (43%)]\tLoss: 41361.988281\n",
      "Train Epoch: 12 [576000/1309486 (44%)]\tLoss: 42281.914062\n",
      "Train Epoch: 12 [584000/1309486 (45%)]\tLoss: 35882.777344\n",
      "Train Epoch: 12 [592000/1309486 (45%)]\tLoss: 37908.695312\n",
      "Train Epoch: 12 [600000/1309486 (46%)]\tLoss: 39205.328125\n",
      "Train Epoch: 12 [608000/1309486 (46%)]\tLoss: 37280.808594\n",
      "Train Epoch: 12 [616000/1309486 (47%)]\tLoss: 38861.687500\n",
      "Train Epoch: 12 [624000/1309486 (48%)]\tLoss: 37672.476562\n",
      "Train Epoch: 12 [632000/1309486 (48%)]\tLoss: 33294.039062\n",
      "Train Epoch: 12 [640000/1309486 (49%)]\tLoss: 37406.140625\n",
      "Train Epoch: 12 [648000/1309486 (49%)]\tLoss: 38649.125000\n",
      "Train Epoch: 12 [656000/1309486 (50%)]\tLoss: 37453.261719\n",
      "Train Epoch: 12 [664000/1309486 (51%)]\tLoss: 38194.511719\n",
      "Train Epoch: 12 [672000/1309486 (51%)]\tLoss: 37709.855469\n",
      "Train Epoch: 12 [680000/1309486 (52%)]\tLoss: 38529.531250\n",
      "Train Epoch: 12 [688000/1309486 (53%)]\tLoss: 41893.898438\n",
      "Train Epoch: 12 [696000/1309486 (53%)]\tLoss: 37170.062500\n",
      "Train Epoch: 12 [704000/1309486 (54%)]\tLoss: 38574.109375\n",
      "Train Epoch: 12 [712000/1309486 (54%)]\tLoss: 40041.117188\n",
      "Train Epoch: 12 [720000/1309486 (55%)]\tLoss: 40852.367188\n",
      "Train Epoch: 12 [728000/1309486 (56%)]\tLoss: 38407.441406\n",
      "Train Epoch: 12 [736000/1309486 (56%)]\tLoss: 36289.937500\n",
      "Train Epoch: 12 [744000/1309486 (57%)]\tLoss: 40005.277344\n",
      "Train Epoch: 12 [752000/1309486 (57%)]\tLoss: 36084.484375\n",
      "Train Epoch: 12 [760000/1309486 (58%)]\tLoss: 39204.476562\n",
      "Train Epoch: 12 [768000/1309486 (59%)]\tLoss: 37531.390625\n",
      "Train Epoch: 12 [776000/1309486 (59%)]\tLoss: 37100.667969\n",
      "Train Epoch: 12 [784000/1309486 (60%)]\tLoss: 40972.925781\n",
      "Train Epoch: 12 [792000/1309486 (60%)]\tLoss: 39194.695312\n",
      "Train Epoch: 12 [800000/1309486 (61%)]\tLoss: 39198.500000\n",
      "Train Epoch: 12 [808000/1309486 (62%)]\tLoss: 40685.671875\n",
      "Train Epoch: 12 [816000/1309486 (62%)]\tLoss: 37728.460938\n",
      "Train Epoch: 12 [824000/1309486 (63%)]\tLoss: 41149.031250\n",
      "Train Epoch: 12 [832000/1309486 (64%)]\tLoss: 39465.382812\n",
      "Train Epoch: 12 [840000/1309486 (64%)]\tLoss: 36667.277344\n",
      "Train Epoch: 12 [848000/1309486 (65%)]\tLoss: 38544.328125\n",
      "Train Epoch: 12 [856000/1309486 (65%)]\tLoss: 38416.093750\n",
      "Train Epoch: 12 [864000/1309486 (66%)]\tLoss: 37170.312500\n",
      "Train Epoch: 12 [872000/1309486 (67%)]\tLoss: 38917.468750\n",
      "Train Epoch: 12 [880000/1309486 (67%)]\tLoss: 36763.769531\n",
      "Train Epoch: 12 [888000/1309486 (68%)]\tLoss: 36897.121094\n",
      "Train Epoch: 12 [896000/1309486 (68%)]\tLoss: 42524.773438\n",
      "Train Epoch: 12 [904000/1309486 (69%)]\tLoss: 42425.164062\n",
      "Train Epoch: 12 [912000/1309486 (70%)]\tLoss: 35995.972656\n",
      "Train Epoch: 12 [920000/1309486 (70%)]\tLoss: 38818.664062\n",
      "Train Epoch: 12 [928000/1309486 (71%)]\tLoss: 38324.363281\n",
      "Train Epoch: 12 [936000/1309486 (71%)]\tLoss: 38200.312500\n",
      "Train Epoch: 12 [944000/1309486 (72%)]\tLoss: 39284.320312\n",
      "Train Epoch: 12 [952000/1309486 (73%)]\tLoss: 41747.820312\n",
      "Train Epoch: 12 [960000/1309486 (73%)]\tLoss: 38715.425781\n",
      "Train Epoch: 12 [968000/1309486 (74%)]\tLoss: 35794.226562\n",
      "Train Epoch: 12 [976000/1309486 (75%)]\tLoss: 37221.550781\n",
      "Train Epoch: 12 [984000/1309486 (75%)]\tLoss: 42287.164062\n",
      "Train Epoch: 12 [992000/1309486 (76%)]\tLoss: 39958.261719\n",
      "Train Epoch: 12 [1000000/1309486 (76%)]\tLoss: 42445.832031\n",
      "Train Epoch: 12 [1008000/1309486 (77%)]\tLoss: 39800.507812\n",
      "Train Epoch: 12 [1016000/1309486 (78%)]\tLoss: 39512.726562\n",
      "Train Epoch: 12 [1024000/1309486 (78%)]\tLoss: 38676.960938\n",
      "Train Epoch: 12 [1032000/1309486 (79%)]\tLoss: 36476.351562\n",
      "Train Epoch: 12 [1040000/1309486 (79%)]\tLoss: 41001.574219\n",
      "Train Epoch: 12 [1048000/1309486 (80%)]\tLoss: 36937.382812\n",
      "Train Epoch: 12 [1056000/1309486 (81%)]\tLoss: 38374.554688\n",
      "Train Epoch: 12 [1064000/1309486 (81%)]\tLoss: 38668.613281\n",
      "Train Epoch: 12 [1072000/1309486 (82%)]\tLoss: 37191.695312\n",
      "Train Epoch: 12 [1080000/1309486 (82%)]\tLoss: 31761.871094\n",
      "Train Epoch: 12 [1088000/1309486 (83%)]\tLoss: 43123.453125\n",
      "Train Epoch: 12 [1096000/1309486 (84%)]\tLoss: 36814.292969\n",
      "Train Epoch: 12 [1104000/1309486 (84%)]\tLoss: 39403.781250\n",
      "Train Epoch: 12 [1112000/1309486 (85%)]\tLoss: 40028.421875\n",
      "Train Epoch: 12 [1120000/1309486 (86%)]\tLoss: 38277.929688\n",
      "Train Epoch: 12 [1128000/1309486 (86%)]\tLoss: 31651.664062\n",
      "Train Epoch: 12 [1136000/1309486 (87%)]\tLoss: 40653.375000\n",
      "Train Epoch: 12 [1144000/1309486 (87%)]\tLoss: 40563.007812\n",
      "Train Epoch: 12 [1152000/1309486 (88%)]\tLoss: 41655.699219\n",
      "Train Epoch: 12 [1160000/1309486 (89%)]\tLoss: 40850.375000\n",
      "Train Epoch: 12 [1168000/1309486 (89%)]\tLoss: 39857.550781\n",
      "Train Epoch: 12 [1176000/1309486 (90%)]\tLoss: 36496.812500\n",
      "Train Epoch: 12 [1184000/1309486 (90%)]\tLoss: 35453.863281\n",
      "Train Epoch: 12 [1192000/1309486 (91%)]\tLoss: 35385.617188\n",
      "Train Epoch: 12 [1200000/1309486 (92%)]\tLoss: 33692.789062\n",
      "Train Epoch: 12 [1208000/1309486 (92%)]\tLoss: 42038.652344\n",
      "Train Epoch: 12 [1216000/1309486 (93%)]\tLoss: 40583.210938\n",
      "Train Epoch: 12 [1224000/1309486 (93%)]\tLoss: 43789.718750\n",
      "Train Epoch: 12 [1232000/1309486 (94%)]\tLoss: 36582.921875\n",
      "Train Epoch: 12 [1240000/1309486 (95%)]\tLoss: 41608.820312\n",
      "Train Epoch: 12 [1248000/1309486 (95%)]\tLoss: 35492.609375\n",
      "Train Epoch: 12 [1256000/1309486 (96%)]\tLoss: 40541.757812\n",
      "Train Epoch: 12 [1264000/1309486 (97%)]\tLoss: 38795.207031\n",
      "Train Epoch: 12 [1272000/1309486 (97%)]\tLoss: 37084.570312\n",
      "Train Epoch: 12 [1280000/1309486 (98%)]\tLoss: 36998.140625\n",
      "Train Epoch: 12 [1288000/1309486 (98%)]\tLoss: 41173.343750\n",
      "Train Epoch: 12 [1296000/1309486 (99%)]\tLoss: 35537.839844\n",
      "Train Epoch: 12 [1304000/1309486 (100%)]\tLoss: 43406.937500\n",
      "Epoch 12 model saved!\n",
      "epoch train time:  785.60394120\n",
      "Train Epoch: 13 [8000/1309486 (1%)]\tLoss: 43185.570312\n",
      "Train Epoch: 13 [16000/1309486 (1%)]\tLoss: 39829.164062\n",
      "Train Epoch: 13 [24000/1309486 (2%)]\tLoss: 33699.617188\n",
      "Train Epoch: 13 [32000/1309486 (2%)]\tLoss: 35053.417969\n",
      "Train Epoch: 13 [40000/1309486 (3%)]\tLoss: 35757.250000\n",
      "Train Epoch: 13 [48000/1309486 (4%)]\tLoss: 39860.054688\n",
      "Train Epoch: 13 [56000/1309486 (4%)]\tLoss: 40481.156250\n",
      "Train Epoch: 13 [64000/1309486 (5%)]\tLoss: 37331.195312\n",
      "Train Epoch: 13 [72000/1309486 (5%)]\tLoss: 35339.000000\n",
      "Train Epoch: 13 [80000/1309486 (6%)]\tLoss: 37575.117188\n",
      "Train Epoch: 13 [88000/1309486 (7%)]\tLoss: 35068.859375\n",
      "Train Epoch: 13 [96000/1309486 (7%)]\tLoss: 38703.621094\n",
      "Train Epoch: 13 [104000/1309486 (8%)]\tLoss: 36214.468750\n",
      "Train Epoch: 13 [112000/1309486 (9%)]\tLoss: 37383.039062\n",
      "Train Epoch: 13 [120000/1309486 (9%)]\tLoss: 39299.648438\n",
      "Train Epoch: 13 [128000/1309486 (10%)]\tLoss: 37260.464844\n",
      "Train Epoch: 13 [136000/1309486 (10%)]\tLoss: 37557.871094\n",
      "Train Epoch: 13 [144000/1309486 (11%)]\tLoss: 34239.132812\n",
      "Train Epoch: 13 [152000/1309486 (12%)]\tLoss: 40523.468750\n",
      "Train Epoch: 13 [160000/1309486 (12%)]\tLoss: 34598.414062\n",
      "Train Epoch: 13 [168000/1309486 (13%)]\tLoss: 41208.835938\n",
      "Train Epoch: 13 [176000/1309486 (13%)]\tLoss: 39240.617188\n",
      "Train Epoch: 13 [184000/1309486 (14%)]\tLoss: 37301.335938\n",
      "Train Epoch: 13 [192000/1309486 (15%)]\tLoss: 40011.281250\n",
      "Train Epoch: 13 [200000/1309486 (15%)]\tLoss: 33534.945312\n",
      "Train Epoch: 13 [208000/1309486 (16%)]\tLoss: 36189.292969\n",
      "Train Epoch: 13 [216000/1309486 (16%)]\tLoss: 37515.015625\n",
      "Train Epoch: 13 [224000/1309486 (17%)]\tLoss: 40681.285156\n",
      "Train Epoch: 13 [232000/1309486 (18%)]\tLoss: 40972.773438\n",
      "Train Epoch: 13 [240000/1309486 (18%)]\tLoss: 39157.042969\n",
      "Train Epoch: 13 [248000/1309486 (19%)]\tLoss: 38382.082031\n",
      "Train Epoch: 13 [256000/1309486 (20%)]\tLoss: 37180.707031\n",
      "Train Epoch: 13 [264000/1309486 (20%)]\tLoss: 40580.496094\n",
      "Train Epoch: 13 [272000/1309486 (21%)]\tLoss: 38787.660156\n",
      "Train Epoch: 13 [280000/1309486 (21%)]\tLoss: 36911.949219\n",
      "Train Epoch: 13 [288000/1309486 (22%)]\tLoss: 38464.351562\n",
      "Train Epoch: 13 [296000/1309486 (23%)]\tLoss: 40741.242188\n",
      "Train Epoch: 13 [304000/1309486 (23%)]\tLoss: 37154.882812\n",
      "Train Epoch: 13 [312000/1309486 (24%)]\tLoss: 36828.417969\n",
      "Train Epoch: 13 [320000/1309486 (24%)]\tLoss: 37797.183594\n",
      "Train Epoch: 13 [328000/1309486 (25%)]\tLoss: 42614.207031\n",
      "Train Epoch: 13 [336000/1309486 (26%)]\tLoss: 39248.851562\n",
      "Train Epoch: 13 [344000/1309486 (26%)]\tLoss: 36494.652344\n",
      "Train Epoch: 13 [352000/1309486 (27%)]\tLoss: 36651.265625\n",
      "Train Epoch: 13 [360000/1309486 (27%)]\tLoss: 38282.878906\n",
      "Train Epoch: 13 [368000/1309486 (28%)]\tLoss: 38689.402344\n",
      "Train Epoch: 13 [376000/1309486 (29%)]\tLoss: 41351.390625\n",
      "Train Epoch: 13 [384000/1309486 (29%)]\tLoss: 40647.757812\n",
      "Train Epoch: 13 [392000/1309486 (30%)]\tLoss: 37423.406250\n",
      "Train Epoch: 13 [400000/1309486 (31%)]\tLoss: 39856.066406\n",
      "Train Epoch: 13 [408000/1309486 (31%)]\tLoss: 42830.484375\n",
      "Train Epoch: 13 [416000/1309486 (32%)]\tLoss: 39803.238281\n",
      "Train Epoch: 13 [424000/1309486 (32%)]\tLoss: 38975.824219\n",
      "Train Epoch: 13 [432000/1309486 (33%)]\tLoss: 36556.960938\n",
      "Train Epoch: 13 [440000/1309486 (34%)]\tLoss: 41845.437500\n",
      "Train Epoch: 13 [448000/1309486 (34%)]\tLoss: 41628.757812\n",
      "Train Epoch: 13 [456000/1309486 (35%)]\tLoss: 43216.914062\n",
      "Train Epoch: 13 [464000/1309486 (35%)]\tLoss: 36080.257812\n",
      "Train Epoch: 13 [472000/1309486 (36%)]\tLoss: 41684.367188\n",
      "Train Epoch: 13 [480000/1309486 (37%)]\tLoss: 36817.750000\n",
      "Train Epoch: 13 [488000/1309486 (37%)]\tLoss: 43196.007812\n",
      "Train Epoch: 13 [496000/1309486 (38%)]\tLoss: 40083.503906\n",
      "Train Epoch: 13 [504000/1309486 (38%)]\tLoss: 40226.398438\n",
      "Train Epoch: 13 [512000/1309486 (39%)]\tLoss: 41936.531250\n",
      "Train Epoch: 13 [520000/1309486 (40%)]\tLoss: 38702.570312\n",
      "Train Epoch: 13 [528000/1309486 (40%)]\tLoss: 43319.718750\n",
      "Train Epoch: 13 [536000/1309486 (41%)]\tLoss: 43520.699219\n",
      "Train Epoch: 13 [544000/1309486 (42%)]\tLoss: 36448.156250\n",
      "Train Epoch: 13 [552000/1309486 (42%)]\tLoss: 35728.855469\n",
      "Train Epoch: 13 [560000/1309486 (43%)]\tLoss: 39688.445312\n",
      "Train Epoch: 13 [568000/1309486 (43%)]\tLoss: 35837.117188\n",
      "Train Epoch: 13 [576000/1309486 (44%)]\tLoss: 42807.968750\n",
      "Train Epoch: 13 [584000/1309486 (45%)]\tLoss: 35812.542969\n",
      "Train Epoch: 13 [592000/1309486 (45%)]\tLoss: 34710.710938\n",
      "Train Epoch: 13 [600000/1309486 (46%)]\tLoss: 36145.761719\n",
      "Train Epoch: 13 [608000/1309486 (46%)]\tLoss: 37456.347656\n",
      "Train Epoch: 13 [616000/1309486 (47%)]\tLoss: 42314.929688\n",
      "Train Epoch: 13 [624000/1309486 (48%)]\tLoss: 38085.562500\n",
      "Train Epoch: 13 [632000/1309486 (48%)]\tLoss: 37343.015625\n",
      "Train Epoch: 13 [640000/1309486 (49%)]\tLoss: 39266.031250\n",
      "Train Epoch: 13 [648000/1309486 (49%)]\tLoss: 38617.355469\n",
      "Train Epoch: 13 [656000/1309486 (50%)]\tLoss: 40833.695312\n",
      "Train Epoch: 13 [664000/1309486 (51%)]\tLoss: 35664.117188\n",
      "Train Epoch: 13 [672000/1309486 (51%)]\tLoss: 39152.781250\n",
      "Train Epoch: 13 [680000/1309486 (52%)]\tLoss: 36915.992188\n",
      "Train Epoch: 13 [688000/1309486 (53%)]\tLoss: 37349.691406\n",
      "Train Epoch: 13 [696000/1309486 (53%)]\tLoss: 38716.820312\n",
      "Train Epoch: 13 [704000/1309486 (54%)]\tLoss: 37743.609375\n",
      "Train Epoch: 13 [712000/1309486 (54%)]\tLoss: 38285.929688\n",
      "Train Epoch: 13 [720000/1309486 (55%)]\tLoss: 38739.195312\n",
      "Train Epoch: 13 [728000/1309486 (56%)]\tLoss: 40700.460938\n",
      "Train Epoch: 13 [736000/1309486 (56%)]\tLoss: 37756.335938\n",
      "Train Epoch: 13 [744000/1309486 (57%)]\tLoss: 40056.609375\n",
      "Train Epoch: 13 [752000/1309486 (57%)]\tLoss: 36560.437500\n",
      "Train Epoch: 13 [760000/1309486 (58%)]\tLoss: 37915.210938\n",
      "Train Epoch: 13 [768000/1309486 (59%)]\tLoss: 37158.292969\n",
      "Train Epoch: 13 [776000/1309486 (59%)]\tLoss: 37340.351562\n",
      "Train Epoch: 13 [784000/1309486 (60%)]\tLoss: 37951.613281\n",
      "Train Epoch: 13 [792000/1309486 (60%)]\tLoss: 39105.023438\n",
      "Train Epoch: 13 [800000/1309486 (61%)]\tLoss: 39589.867188\n",
      "Train Epoch: 13 [808000/1309486 (62%)]\tLoss: 31573.113281\n",
      "Train Epoch: 13 [816000/1309486 (62%)]\tLoss: 37471.679688\n",
      "Train Epoch: 13 [824000/1309486 (63%)]\tLoss: 35647.023438\n",
      "Train Epoch: 13 [832000/1309486 (64%)]\tLoss: 37005.703125\n",
      "Train Epoch: 13 [840000/1309486 (64%)]\tLoss: 42030.441406\n",
      "Train Epoch: 13 [848000/1309486 (65%)]\tLoss: 39105.882812\n",
      "Train Epoch: 13 [856000/1309486 (65%)]\tLoss: 38622.015625\n",
      "Train Epoch: 13 [864000/1309486 (66%)]\tLoss: 36564.812500\n",
      "Train Epoch: 13 [872000/1309486 (67%)]\tLoss: 37182.851562\n",
      "Train Epoch: 13 [880000/1309486 (67%)]\tLoss: 41033.304688\n",
      "Train Epoch: 13 [888000/1309486 (68%)]\tLoss: 34683.171875\n",
      "Train Epoch: 13 [896000/1309486 (68%)]\tLoss: 38890.500000\n",
      "Train Epoch: 13 [904000/1309486 (69%)]\tLoss: 40513.574219\n",
      "Train Epoch: 13 [912000/1309486 (70%)]\tLoss: 42903.195312\n",
      "Train Epoch: 13 [920000/1309486 (70%)]\tLoss: 39787.941406\n",
      "Train Epoch: 13 [928000/1309486 (71%)]\tLoss: 39035.203125\n",
      "Train Epoch: 13 [936000/1309486 (71%)]\tLoss: 36898.273438\n",
      "Train Epoch: 13 [944000/1309486 (72%)]\tLoss: 37683.476562\n",
      "Train Epoch: 13 [952000/1309486 (73%)]\tLoss: 41922.984375\n",
      "Train Epoch: 13 [960000/1309486 (73%)]\tLoss: 37887.890625\n",
      "Train Epoch: 13 [968000/1309486 (74%)]\tLoss: 34695.910156\n",
      "Train Epoch: 13 [976000/1309486 (75%)]\tLoss: 36516.187500\n",
      "Train Epoch: 13 [984000/1309486 (75%)]\tLoss: 36778.953125\n",
      "Train Epoch: 13 [992000/1309486 (76%)]\tLoss: 39802.312500\n",
      "Train Epoch: 13 [1000000/1309486 (76%)]\tLoss: 36997.652344\n",
      "Train Epoch: 13 [1008000/1309486 (77%)]\tLoss: 34363.429688\n",
      "Train Epoch: 13 [1016000/1309486 (78%)]\tLoss: 39670.636719\n",
      "Train Epoch: 13 [1024000/1309486 (78%)]\tLoss: 39827.457031\n",
      "Train Epoch: 13 [1032000/1309486 (79%)]\tLoss: 39690.609375\n",
      "Train Epoch: 13 [1040000/1309486 (79%)]\tLoss: 39285.589844\n",
      "Train Epoch: 13 [1048000/1309486 (80%)]\tLoss: 37623.671875\n",
      "Train Epoch: 13 [1056000/1309486 (81%)]\tLoss: 43004.574219\n",
      "Train Epoch: 13 [1064000/1309486 (81%)]\tLoss: 37002.109375\n",
      "Train Epoch: 13 [1072000/1309486 (82%)]\tLoss: 41066.644531\n",
      "Train Epoch: 13 [1080000/1309486 (82%)]\tLoss: 41277.339844\n",
      "Train Epoch: 13 [1088000/1309486 (83%)]\tLoss: 38575.460938\n",
      "Train Epoch: 13 [1096000/1309486 (84%)]\tLoss: 37973.394531\n",
      "Train Epoch: 13 [1104000/1309486 (84%)]\tLoss: 37158.757812\n",
      "Train Epoch: 13 [1112000/1309486 (85%)]\tLoss: 39067.847656\n",
      "Train Epoch: 13 [1120000/1309486 (86%)]\tLoss: 38174.234375\n",
      "Train Epoch: 13 [1128000/1309486 (86%)]\tLoss: 37822.437500\n",
      "Train Epoch: 13 [1136000/1309486 (87%)]\tLoss: 35923.125000\n",
      "Train Epoch: 13 [1144000/1309486 (87%)]\tLoss: 36397.179688\n",
      "Train Epoch: 13 [1152000/1309486 (88%)]\tLoss: 38323.984375\n",
      "Train Epoch: 13 [1160000/1309486 (89%)]\tLoss: 37953.566406\n",
      "Train Epoch: 13 [1168000/1309486 (89%)]\tLoss: 35031.667969\n",
      "Train Epoch: 13 [1176000/1309486 (90%)]\tLoss: 36458.464844\n",
      "Train Epoch: 13 [1184000/1309486 (90%)]\tLoss: 35497.445312\n",
      "Train Epoch: 13 [1192000/1309486 (91%)]\tLoss: 40021.171875\n",
      "Train Epoch: 13 [1200000/1309486 (92%)]\tLoss: 41047.222656\n",
      "Train Epoch: 13 [1208000/1309486 (92%)]\tLoss: 37815.214844\n",
      "Train Epoch: 13 [1216000/1309486 (93%)]\tLoss: 37095.335938\n",
      "Train Epoch: 13 [1224000/1309486 (93%)]\tLoss: 39801.703125\n",
      "Train Epoch: 13 [1232000/1309486 (94%)]\tLoss: 35379.554688\n",
      "Train Epoch: 13 [1240000/1309486 (95%)]\tLoss: 44745.875000\n",
      "Train Epoch: 13 [1248000/1309486 (95%)]\tLoss: 38259.828125\n",
      "Train Epoch: 13 [1256000/1309486 (96%)]\tLoss: 42330.554688\n",
      "Train Epoch: 13 [1264000/1309486 (97%)]\tLoss: 41893.164062\n",
      "Train Epoch: 13 [1272000/1309486 (97%)]\tLoss: 39224.839844\n",
      "Train Epoch: 13 [1280000/1309486 (98%)]\tLoss: 40000.984375\n",
      "Train Epoch: 13 [1288000/1309486 (98%)]\tLoss: 38588.726562\n",
      "Train Epoch: 13 [1296000/1309486 (99%)]\tLoss: 43849.738281\n",
      "Train Epoch: 13 [1304000/1309486 (100%)]\tLoss: 38125.890625\n",
      "Epoch 13 model saved!\n",
      "epoch train time:  785.10363865\n",
      "Train Epoch: 14 [8000/1309486 (1%)]\tLoss: 36263.500000\n",
      "Train Epoch: 14 [16000/1309486 (1%)]\tLoss: 36207.265625\n",
      "Train Epoch: 14 [24000/1309486 (2%)]\tLoss: 38904.339844\n",
      "Train Epoch: 14 [32000/1309486 (2%)]\tLoss: 39854.816406\n",
      "Train Epoch: 14 [40000/1309486 (3%)]\tLoss: 34097.179688\n",
      "Train Epoch: 14 [48000/1309486 (4%)]\tLoss: 36031.679688\n",
      "Train Epoch: 14 [56000/1309486 (4%)]\tLoss: 41960.648438\n",
      "Train Epoch: 14 [64000/1309486 (5%)]\tLoss: 37655.242188\n",
      "Train Epoch: 14 [72000/1309486 (5%)]\tLoss: 39256.710938\n",
      "Train Epoch: 14 [80000/1309486 (6%)]\tLoss: 40367.195312\n",
      "Train Epoch: 14 [88000/1309486 (7%)]\tLoss: 38616.398438\n",
      "Train Epoch: 14 [96000/1309486 (7%)]\tLoss: 41869.953125\n",
      "Train Epoch: 14 [104000/1309486 (8%)]\tLoss: 37591.406250\n",
      "Train Epoch: 14 [112000/1309486 (9%)]\tLoss: 38763.578125\n",
      "Train Epoch: 14 [120000/1309486 (9%)]\tLoss: 37928.710938\n",
      "Train Epoch: 14 [128000/1309486 (10%)]\tLoss: 40831.054688\n",
      "Train Epoch: 14 [136000/1309486 (10%)]\tLoss: 40928.656250\n",
      "Train Epoch: 14 [144000/1309486 (11%)]\tLoss: 38432.898438\n",
      "Train Epoch: 14 [152000/1309486 (12%)]\tLoss: 33401.804688\n",
      "Train Epoch: 14 [160000/1309486 (12%)]\tLoss: 43519.191406\n",
      "Train Epoch: 14 [168000/1309486 (13%)]\tLoss: 36932.273438\n",
      "Train Epoch: 14 [176000/1309486 (13%)]\tLoss: 37448.539062\n",
      "Train Epoch: 14 [184000/1309486 (14%)]\tLoss: 37176.863281\n",
      "Train Epoch: 14 [192000/1309486 (15%)]\tLoss: 38519.828125\n",
      "Train Epoch: 14 [200000/1309486 (15%)]\tLoss: 40348.039062\n",
      "Train Epoch: 14 [208000/1309486 (16%)]\tLoss: 38980.242188\n",
      "Train Epoch: 14 [216000/1309486 (16%)]\tLoss: 37108.539062\n",
      "Train Epoch: 14 [224000/1309486 (17%)]\tLoss: 40340.242188\n",
      "Train Epoch: 14 [232000/1309486 (18%)]\tLoss: 39632.414062\n",
      "Train Epoch: 14 [240000/1309486 (18%)]\tLoss: 39562.941406\n",
      "Train Epoch: 14 [248000/1309486 (19%)]\tLoss: 35953.867188\n",
      "Train Epoch: 14 [256000/1309486 (20%)]\tLoss: 39268.914062\n",
      "Train Epoch: 14 [264000/1309486 (20%)]\tLoss: 42366.945312\n",
      "Train Epoch: 14 [272000/1309486 (21%)]\tLoss: 40029.667969\n",
      "Train Epoch: 14 [280000/1309486 (21%)]\tLoss: 37874.234375\n",
      "Train Epoch: 14 [288000/1309486 (22%)]\tLoss: 33248.046875\n",
      "Train Epoch: 14 [296000/1309486 (23%)]\tLoss: 33650.683594\n",
      "Train Epoch: 14 [304000/1309486 (23%)]\tLoss: 36954.183594\n",
      "Train Epoch: 14 [312000/1309486 (24%)]\tLoss: 38975.238281\n",
      "Train Epoch: 14 [320000/1309486 (24%)]\tLoss: 40724.445312\n",
      "Train Epoch: 14 [328000/1309486 (25%)]\tLoss: 43019.332031\n",
      "Train Epoch: 14 [336000/1309486 (26%)]\tLoss: 38707.312500\n",
      "Train Epoch: 14 [344000/1309486 (26%)]\tLoss: 35937.375000\n",
      "Train Epoch: 14 [352000/1309486 (27%)]\tLoss: 37559.617188\n",
      "Train Epoch: 14 [360000/1309486 (27%)]\tLoss: 35150.414062\n",
      "Train Epoch: 14 [368000/1309486 (28%)]\tLoss: 39986.765625\n",
      "Train Epoch: 14 [376000/1309486 (29%)]\tLoss: 41440.414062\n",
      "Train Epoch: 14 [384000/1309486 (29%)]\tLoss: 39028.757812\n",
      "Train Epoch: 14 [392000/1309486 (30%)]\tLoss: 33839.718750\n",
      "Train Epoch: 14 [400000/1309486 (31%)]\tLoss: 41339.058594\n",
      "Train Epoch: 14 [408000/1309486 (31%)]\tLoss: 34653.718750\n",
      "Train Epoch: 14 [416000/1309486 (32%)]\tLoss: 38360.125000\n",
      "Train Epoch: 14 [424000/1309486 (32%)]\tLoss: 36221.812500\n",
      "Train Epoch: 14 [432000/1309486 (33%)]\tLoss: 38306.250000\n",
      "Train Epoch: 14 [440000/1309486 (34%)]\tLoss: 39783.007812\n",
      "Train Epoch: 14 [448000/1309486 (34%)]\tLoss: 37534.851562\n",
      "Train Epoch: 14 [456000/1309486 (35%)]\tLoss: 36118.867188\n",
      "Train Epoch: 14 [464000/1309486 (35%)]\tLoss: 38264.996094\n",
      "Train Epoch: 14 [472000/1309486 (36%)]\tLoss: 36117.929688\n",
      "Train Epoch: 14 [480000/1309486 (37%)]\tLoss: 39430.640625\n",
      "Train Epoch: 14 [488000/1309486 (37%)]\tLoss: 39644.644531\n",
      "Train Epoch: 14 [496000/1309486 (38%)]\tLoss: 41669.734375\n",
      "Train Epoch: 14 [504000/1309486 (38%)]\tLoss: 36469.828125\n",
      "Train Epoch: 14 [512000/1309486 (39%)]\tLoss: 41310.312500\n",
      "Train Epoch: 14 [520000/1309486 (40%)]\tLoss: 37108.300781\n",
      "Train Epoch: 14 [528000/1309486 (40%)]\tLoss: 38445.156250\n",
      "Train Epoch: 14 [536000/1309486 (41%)]\tLoss: 41586.007812\n",
      "Train Epoch: 14 [544000/1309486 (42%)]\tLoss: 37243.125000\n",
      "Train Epoch: 14 [552000/1309486 (42%)]\tLoss: 39269.281250\n",
      "Train Epoch: 14 [560000/1309486 (43%)]\tLoss: 38733.992188\n",
      "Train Epoch: 14 [568000/1309486 (43%)]\tLoss: 33000.667969\n",
      "Train Epoch: 14 [576000/1309486 (44%)]\tLoss: 38559.109375\n",
      "Train Epoch: 14 [584000/1309486 (45%)]\tLoss: 44086.968750\n",
      "Train Epoch: 14 [592000/1309486 (45%)]\tLoss: 37754.429688\n",
      "Train Epoch: 14 [600000/1309486 (46%)]\tLoss: 38379.406250\n",
      "Train Epoch: 14 [608000/1309486 (46%)]\tLoss: 37788.796875\n",
      "Train Epoch: 14 [616000/1309486 (47%)]\tLoss: 37977.437500\n",
      "Train Epoch: 14 [624000/1309486 (48%)]\tLoss: 36606.875000\n",
      "Train Epoch: 14 [632000/1309486 (48%)]\tLoss: 39866.421875\n",
      "Train Epoch: 14 [640000/1309486 (49%)]\tLoss: 41013.843750\n",
      "Train Epoch: 14 [648000/1309486 (49%)]\tLoss: 38331.042969\n",
      "Train Epoch: 14 [656000/1309486 (50%)]\tLoss: 34129.582031\n",
      "Train Epoch: 14 [664000/1309486 (51%)]\tLoss: 40831.718750\n",
      "Train Epoch: 14 [672000/1309486 (51%)]\tLoss: 38147.929688\n",
      "Train Epoch: 14 [680000/1309486 (52%)]\tLoss: 38099.386719\n",
      "Train Epoch: 14 [688000/1309486 (53%)]\tLoss: 40092.742188\n",
      "Train Epoch: 14 [696000/1309486 (53%)]\tLoss: 38097.679688\n",
      "Train Epoch: 14 [704000/1309486 (54%)]\tLoss: 36596.238281\n",
      "Train Epoch: 14 [712000/1309486 (54%)]\tLoss: 37470.824219\n",
      "Train Epoch: 14 [720000/1309486 (55%)]\tLoss: 34793.187500\n",
      "Train Epoch: 14 [728000/1309486 (56%)]\tLoss: 39917.671875\n",
      "Train Epoch: 14 [736000/1309486 (56%)]\tLoss: 39036.453125\n",
      "Train Epoch: 14 [744000/1309486 (57%)]\tLoss: 35738.214844\n",
      "Train Epoch: 14 [752000/1309486 (57%)]\tLoss: 37964.789062\n",
      "Train Epoch: 14 [760000/1309486 (58%)]\tLoss: 38275.171875\n",
      "Train Epoch: 14 [768000/1309486 (59%)]\tLoss: 35669.375000\n",
      "Train Epoch: 14 [776000/1309486 (59%)]\tLoss: 40231.570312\n",
      "Train Epoch: 14 [784000/1309486 (60%)]\tLoss: 37930.476562\n",
      "Train Epoch: 14 [792000/1309486 (60%)]\tLoss: 39381.437500\n",
      "Train Epoch: 14 [800000/1309486 (61%)]\tLoss: 40781.859375\n",
      "Train Epoch: 14 [808000/1309486 (62%)]\tLoss: 38185.921875\n",
      "Train Epoch: 14 [816000/1309486 (62%)]\tLoss: 39535.410156\n",
      "Train Epoch: 14 [824000/1309486 (63%)]\tLoss: 40565.664062\n",
      "Train Epoch: 14 [832000/1309486 (64%)]\tLoss: 35759.839844\n",
      "Train Epoch: 14 [840000/1309486 (64%)]\tLoss: 38914.769531\n",
      "Train Epoch: 14 [848000/1309486 (65%)]\tLoss: 39705.761719\n",
      "Train Epoch: 14 [856000/1309486 (65%)]\tLoss: 34826.554688\n",
      "Train Epoch: 14 [864000/1309486 (66%)]\tLoss: 37793.062500\n",
      "Train Epoch: 14 [872000/1309486 (67%)]\tLoss: 39642.726562\n",
      "Train Epoch: 14 [880000/1309486 (67%)]\tLoss: 39205.078125\n",
      "Train Epoch: 14 [888000/1309486 (68%)]\tLoss: 36030.449219\n",
      "Train Epoch: 14 [896000/1309486 (68%)]\tLoss: 39032.031250\n",
      "Train Epoch: 14 [904000/1309486 (69%)]\tLoss: 36747.757812\n",
      "Train Epoch: 14 [912000/1309486 (70%)]\tLoss: 40479.515625\n",
      "Train Epoch: 14 [920000/1309486 (70%)]\tLoss: 37027.609375\n",
      "Train Epoch: 14 [928000/1309486 (71%)]\tLoss: 39564.210938\n",
      "Train Epoch: 14 [936000/1309486 (71%)]\tLoss: 41101.152344\n",
      "Train Epoch: 14 [944000/1309486 (72%)]\tLoss: 38294.132812\n",
      "Train Epoch: 14 [952000/1309486 (73%)]\tLoss: 36846.996094\n",
      "Train Epoch: 14 [960000/1309486 (73%)]\tLoss: 37855.867188\n",
      "Train Epoch: 14 [968000/1309486 (74%)]\tLoss: 39992.230469\n",
      "Train Epoch: 14 [976000/1309486 (75%)]\tLoss: 35999.355469\n",
      "Train Epoch: 14 [984000/1309486 (75%)]\tLoss: 39023.714844\n",
      "Train Epoch: 14 [992000/1309486 (76%)]\tLoss: 37664.667969\n",
      "Train Epoch: 14 [1000000/1309486 (76%)]\tLoss: 41835.476562\n",
      "Train Epoch: 14 [1008000/1309486 (77%)]\tLoss: 43425.632812\n",
      "Train Epoch: 14 [1016000/1309486 (78%)]\tLoss: 38046.492188\n",
      "Train Epoch: 14 [1024000/1309486 (78%)]\tLoss: 39584.683594\n",
      "Train Epoch: 14 [1032000/1309486 (79%)]\tLoss: 45214.625000\n",
      "Train Epoch: 14 [1040000/1309486 (79%)]\tLoss: 39260.578125\n",
      "Train Epoch: 14 [1048000/1309486 (80%)]\tLoss: 40306.632812\n",
      "Train Epoch: 14 [1056000/1309486 (81%)]\tLoss: 43118.800781\n",
      "Train Epoch: 14 [1064000/1309486 (81%)]\tLoss: 41825.230469\n",
      "Train Epoch: 14 [1072000/1309486 (82%)]\tLoss: 38196.687500\n",
      "Train Epoch: 14 [1080000/1309486 (82%)]\tLoss: 38863.343750\n",
      "Train Epoch: 14 [1088000/1309486 (83%)]\tLoss: 41272.585938\n",
      "Train Epoch: 14 [1096000/1309486 (84%)]\tLoss: 36941.933594\n",
      "Train Epoch: 14 [1104000/1309486 (84%)]\tLoss: 38309.929688\n",
      "Train Epoch: 14 [1112000/1309486 (85%)]\tLoss: 42207.234375\n",
      "Train Epoch: 14 [1120000/1309486 (86%)]\tLoss: 38897.902344\n",
      "Train Epoch: 14 [1128000/1309486 (86%)]\tLoss: 39746.570312\n",
      "Train Epoch: 14 [1136000/1309486 (87%)]\tLoss: 39717.621094\n",
      "Train Epoch: 14 [1144000/1309486 (87%)]\tLoss: 37664.765625\n",
      "Train Epoch: 14 [1152000/1309486 (88%)]\tLoss: 41657.335938\n",
      "Train Epoch: 14 [1160000/1309486 (89%)]\tLoss: 37312.468750\n",
      "Train Epoch: 14 [1168000/1309486 (89%)]\tLoss: 39801.593750\n",
      "Train Epoch: 14 [1176000/1309486 (90%)]\tLoss: 38370.894531\n",
      "Train Epoch: 14 [1184000/1309486 (90%)]\tLoss: 37315.402344\n",
      "Train Epoch: 14 [1192000/1309486 (91%)]\tLoss: 37200.695312\n",
      "Train Epoch: 14 [1200000/1309486 (92%)]\tLoss: 39000.105469\n",
      "Train Epoch: 14 [1208000/1309486 (92%)]\tLoss: 41253.808594\n",
      "Train Epoch: 14 [1216000/1309486 (93%)]\tLoss: 42527.750000\n",
      "Train Epoch: 14 [1224000/1309486 (93%)]\tLoss: 40102.250000\n",
      "Train Epoch: 14 [1232000/1309486 (94%)]\tLoss: 36094.140625\n",
      "Train Epoch: 14 [1240000/1309486 (95%)]\tLoss: 43290.976562\n",
      "Train Epoch: 14 [1248000/1309486 (95%)]\tLoss: 39825.210938\n",
      "Train Epoch: 14 [1256000/1309486 (96%)]\tLoss: 41662.332031\n",
      "Train Epoch: 14 [1264000/1309486 (97%)]\tLoss: 39039.757812\n",
      "Train Epoch: 14 [1272000/1309486 (97%)]\tLoss: 36178.988281\n",
      "Train Epoch: 14 [1280000/1309486 (98%)]\tLoss: 36403.691406\n",
      "Train Epoch: 14 [1288000/1309486 (98%)]\tLoss: 38754.460938\n",
      "Train Epoch: 14 [1296000/1309486 (99%)]\tLoss: 37883.320312\n",
      "Train Epoch: 14 [1304000/1309486 (100%)]\tLoss: 40632.257812\n",
      "Epoch 14 model saved!\n",
      "epoch train time:  784.13679028\n",
      "Train Epoch: 15 [8000/1309486 (1%)]\tLoss: 40943.898438\n",
      "Train Epoch: 15 [16000/1309486 (1%)]\tLoss: 37911.039062\n",
      "Train Epoch: 15 [24000/1309486 (2%)]\tLoss: 38367.796875\n",
      "Train Epoch: 15 [32000/1309486 (2%)]\tLoss: 39723.593750\n",
      "Train Epoch: 15 [40000/1309486 (3%)]\tLoss: 36878.929688\n",
      "Train Epoch: 15 [48000/1309486 (4%)]\tLoss: 37502.550781\n",
      "Train Epoch: 15 [56000/1309486 (4%)]\tLoss: 39697.253906\n",
      "Train Epoch: 15 [64000/1309486 (5%)]\tLoss: 39034.218750\n",
      "Train Epoch: 15 [72000/1309486 (5%)]\tLoss: 39749.898438\n",
      "Train Epoch: 15 [80000/1309486 (6%)]\tLoss: 35133.226562\n",
      "Train Epoch: 15 [88000/1309486 (7%)]\tLoss: 33356.480469\n",
      "Train Epoch: 15 [96000/1309486 (7%)]\tLoss: 40323.394531\n",
      "Train Epoch: 15 [104000/1309486 (8%)]\tLoss: 40256.339844\n",
      "Train Epoch: 15 [112000/1309486 (9%)]\tLoss: 37074.714844\n",
      "Train Epoch: 15 [120000/1309486 (9%)]\tLoss: 34940.929688\n",
      "Train Epoch: 15 [128000/1309486 (10%)]\tLoss: 36291.867188\n",
      "Train Epoch: 15 [136000/1309486 (10%)]\tLoss: 35557.640625\n",
      "Train Epoch: 15 [144000/1309486 (11%)]\tLoss: 35507.847656\n",
      "Train Epoch: 15 [152000/1309486 (12%)]\tLoss: 36482.843750\n",
      "Train Epoch: 15 [160000/1309486 (12%)]\tLoss: 43042.125000\n",
      "Train Epoch: 15 [168000/1309486 (13%)]\tLoss: 39654.273438\n",
      "Train Epoch: 15 [176000/1309486 (13%)]\tLoss: 36801.722656\n",
      "Train Epoch: 15 [184000/1309486 (14%)]\tLoss: 37425.773438\n",
      "Train Epoch: 15 [192000/1309486 (15%)]\tLoss: 40312.386719\n",
      "Train Epoch: 15 [200000/1309486 (15%)]\tLoss: 43363.039062\n",
      "Train Epoch: 15 [208000/1309486 (16%)]\tLoss: 42590.273438\n",
      "Train Epoch: 15 [216000/1309486 (16%)]\tLoss: 39344.527344\n",
      "Train Epoch: 15 [224000/1309486 (17%)]\tLoss: 39374.230469\n",
      "Train Epoch: 15 [232000/1309486 (18%)]\tLoss: 37175.453125\n",
      "Train Epoch: 15 [240000/1309486 (18%)]\tLoss: 40682.363281\n",
      "Train Epoch: 15 [248000/1309486 (19%)]\tLoss: 39827.343750\n",
      "Train Epoch: 15 [256000/1309486 (20%)]\tLoss: 41024.937500\n",
      "Train Epoch: 15 [264000/1309486 (20%)]\tLoss: 41749.382812\n",
      "Train Epoch: 15 [272000/1309486 (21%)]\tLoss: 41812.082031\n",
      "Train Epoch: 15 [280000/1309486 (21%)]\tLoss: 37185.902344\n",
      "Train Epoch: 15 [288000/1309486 (22%)]\tLoss: 36553.148438\n",
      "Train Epoch: 15 [296000/1309486 (23%)]\tLoss: 37593.757812\n",
      "Train Epoch: 15 [304000/1309486 (23%)]\tLoss: 38861.753906\n",
      "Train Epoch: 15 [312000/1309486 (24%)]\tLoss: 38442.398438\n",
      "Train Epoch: 15 [320000/1309486 (24%)]\tLoss: 35695.613281\n",
      "Train Epoch: 15 [328000/1309486 (25%)]\tLoss: 38614.687500\n",
      "Train Epoch: 15 [336000/1309486 (26%)]\tLoss: 39342.101562\n",
      "Train Epoch: 15 [344000/1309486 (26%)]\tLoss: 38023.851562\n",
      "Train Epoch: 15 [352000/1309486 (27%)]\tLoss: 38779.914062\n",
      "Train Epoch: 15 [360000/1309486 (27%)]\tLoss: 38525.816406\n",
      "Train Epoch: 15 [368000/1309486 (28%)]\tLoss: 41799.156250\n",
      "Train Epoch: 15 [376000/1309486 (29%)]\tLoss: 38821.609375\n",
      "Train Epoch: 15 [384000/1309486 (29%)]\tLoss: 39446.492188\n",
      "Train Epoch: 15 [392000/1309486 (30%)]\tLoss: 40943.835938\n",
      "Train Epoch: 15 [400000/1309486 (31%)]\tLoss: 39488.097656\n",
      "Train Epoch: 15 [408000/1309486 (31%)]\tLoss: 38533.082031\n",
      "Train Epoch: 15 [416000/1309486 (32%)]\tLoss: 37675.906250\n",
      "Train Epoch: 15 [424000/1309486 (32%)]\tLoss: 36969.386719\n",
      "Train Epoch: 15 [432000/1309486 (33%)]\tLoss: 38471.609375\n",
      "Train Epoch: 15 [440000/1309486 (34%)]\tLoss: 36359.148438\n",
      "Train Epoch: 15 [448000/1309486 (34%)]\tLoss: 39658.015625\n",
      "Train Epoch: 15 [456000/1309486 (35%)]\tLoss: 41619.406250\n",
      "Train Epoch: 15 [464000/1309486 (35%)]\tLoss: 35805.382812\n",
      "Train Epoch: 15 [472000/1309486 (36%)]\tLoss: 37458.125000\n",
      "Train Epoch: 15 [480000/1309486 (37%)]\tLoss: 36179.375000\n",
      "Train Epoch: 15 [488000/1309486 (37%)]\tLoss: 42620.914062\n",
      "Train Epoch: 15 [496000/1309486 (38%)]\tLoss: 43092.507812\n",
      "Train Epoch: 15 [504000/1309486 (38%)]\tLoss: 37776.898438\n",
      "Train Epoch: 15 [512000/1309486 (39%)]\tLoss: 41125.531250\n",
      "Train Epoch: 15 [520000/1309486 (40%)]\tLoss: 36336.351562\n",
      "Train Epoch: 15 [528000/1309486 (40%)]\tLoss: 37950.312500\n",
      "Train Epoch: 15 [536000/1309486 (41%)]\tLoss: 37113.578125\n",
      "Train Epoch: 15 [544000/1309486 (42%)]\tLoss: 38829.128906\n",
      "Train Epoch: 15 [552000/1309486 (42%)]\tLoss: 39892.265625\n",
      "Train Epoch: 15 [560000/1309486 (43%)]\tLoss: 38087.546875\n",
      "Train Epoch: 15 [568000/1309486 (43%)]\tLoss: 37899.527344\n",
      "Train Epoch: 15 [576000/1309486 (44%)]\tLoss: 38554.140625\n",
      "Train Epoch: 15 [584000/1309486 (45%)]\tLoss: 38668.140625\n",
      "Train Epoch: 15 [592000/1309486 (45%)]\tLoss: 42575.804688\n",
      "Train Epoch: 15 [600000/1309486 (46%)]\tLoss: 39614.644531\n",
      "Train Epoch: 15 [608000/1309486 (46%)]\tLoss: 41302.328125\n",
      "Train Epoch: 15 [616000/1309486 (47%)]\tLoss: 39313.625000\n",
      "Train Epoch: 15 [624000/1309486 (48%)]\tLoss: 40433.750000\n",
      "Train Epoch: 15 [632000/1309486 (48%)]\tLoss: 38991.687500\n",
      "Train Epoch: 15 [640000/1309486 (49%)]\tLoss: 42857.023438\n",
      "Train Epoch: 15 [648000/1309486 (49%)]\tLoss: 38699.859375\n",
      "Train Epoch: 15 [656000/1309486 (50%)]\tLoss: 39807.906250\n",
      "Train Epoch: 15 [664000/1309486 (51%)]\tLoss: 40013.218750\n",
      "Train Epoch: 15 [672000/1309486 (51%)]\tLoss: 38446.539062\n",
      "Train Epoch: 15 [680000/1309486 (52%)]\tLoss: 38825.003906\n",
      "Train Epoch: 15 [688000/1309486 (53%)]\tLoss: 40373.265625\n",
      "Train Epoch: 15 [696000/1309486 (53%)]\tLoss: 36150.769531\n",
      "Train Epoch: 15 [704000/1309486 (54%)]\tLoss: 42293.960938\n",
      "Train Epoch: 15 [712000/1309486 (54%)]\tLoss: 34948.468750\n",
      "Train Epoch: 15 [720000/1309486 (55%)]\tLoss: 40249.203125\n",
      "Train Epoch: 15 [728000/1309486 (56%)]\tLoss: 35228.308594\n",
      "Train Epoch: 15 [736000/1309486 (56%)]\tLoss: 36068.253906\n",
      "Train Epoch: 15 [744000/1309486 (57%)]\tLoss: 37576.882812\n",
      "Train Epoch: 15 [752000/1309486 (57%)]\tLoss: 33968.898438\n",
      "Train Epoch: 15 [760000/1309486 (58%)]\tLoss: 42738.171875\n",
      "Train Epoch: 15 [768000/1309486 (59%)]\tLoss: 38963.722656\n",
      "Train Epoch: 15 [776000/1309486 (59%)]\tLoss: 36616.031250\n",
      "Train Epoch: 15 [784000/1309486 (60%)]\tLoss: 40658.585938\n",
      "Train Epoch: 15 [792000/1309486 (60%)]\tLoss: 39928.531250\n",
      "Train Epoch: 15 [800000/1309486 (61%)]\tLoss: 39992.421875\n",
      "Train Epoch: 15 [808000/1309486 (62%)]\tLoss: 40525.523438\n",
      "Train Epoch: 15 [816000/1309486 (62%)]\tLoss: 40082.960938\n",
      "Train Epoch: 15 [824000/1309486 (63%)]\tLoss: 35600.976562\n",
      "Train Epoch: 15 [832000/1309486 (64%)]\tLoss: 39095.226562\n",
      "Train Epoch: 15 [840000/1309486 (64%)]\tLoss: 33534.742188\n",
      "Train Epoch: 15 [848000/1309486 (65%)]\tLoss: 40077.398438\n",
      "Train Epoch: 15 [856000/1309486 (65%)]\tLoss: 36641.789062\n",
      "Train Epoch: 15 [864000/1309486 (66%)]\tLoss: 38695.921875\n",
      "Train Epoch: 15 [872000/1309486 (67%)]\tLoss: 37831.472656\n",
      "Train Epoch: 15 [880000/1309486 (67%)]\tLoss: 36931.125000\n",
      "Train Epoch: 15 [888000/1309486 (68%)]\tLoss: 39933.917969\n",
      "Train Epoch: 15 [896000/1309486 (68%)]\tLoss: 38933.093750\n",
      "Train Epoch: 15 [904000/1309486 (69%)]\tLoss: 39600.593750\n",
      "Train Epoch: 15 [912000/1309486 (70%)]\tLoss: 39728.539062\n",
      "Train Epoch: 15 [920000/1309486 (70%)]\tLoss: 38413.234375\n",
      "Train Epoch: 15 [928000/1309486 (71%)]\tLoss: 41027.023438\n",
      "Train Epoch: 15 [936000/1309486 (71%)]\tLoss: 36802.578125\n",
      "Train Epoch: 15 [944000/1309486 (72%)]\tLoss: 35344.300781\n",
      "Train Epoch: 15 [952000/1309486 (73%)]\tLoss: 34270.437500\n",
      "Train Epoch: 15 [960000/1309486 (73%)]\tLoss: 38571.558594\n",
      "Train Epoch: 15 [968000/1309486 (74%)]\tLoss: 36470.792969\n",
      "Train Epoch: 15 [976000/1309486 (75%)]\tLoss: 39046.285156\n",
      "Train Epoch: 15 [984000/1309486 (75%)]\tLoss: 40761.726562\n",
      "Train Epoch: 15 [992000/1309486 (76%)]\tLoss: 40006.796875\n",
      "Train Epoch: 15 [1000000/1309486 (76%)]\tLoss: 33765.152344\n",
      "Train Epoch: 15 [1008000/1309486 (77%)]\tLoss: 37366.726562\n",
      "Train Epoch: 15 [1016000/1309486 (78%)]\tLoss: 37571.312500\n",
      "Train Epoch: 15 [1024000/1309486 (78%)]\tLoss: 40759.046875\n",
      "Train Epoch: 15 [1032000/1309486 (79%)]\tLoss: 38691.750000\n",
      "Train Epoch: 15 [1040000/1309486 (79%)]\tLoss: 40591.726562\n",
      "Train Epoch: 15 [1048000/1309486 (80%)]\tLoss: 38171.265625\n",
      "Train Epoch: 15 [1056000/1309486 (81%)]\tLoss: 38349.515625\n",
      "Train Epoch: 15 [1064000/1309486 (81%)]\tLoss: 38033.621094\n",
      "Train Epoch: 15 [1072000/1309486 (82%)]\tLoss: 34251.976562\n",
      "Train Epoch: 15 [1080000/1309486 (82%)]\tLoss: 41725.597656\n",
      "Train Epoch: 15 [1088000/1309486 (83%)]\tLoss: 41451.386719\n",
      "Train Epoch: 15 [1096000/1309486 (84%)]\tLoss: 38030.207031\n",
      "Train Epoch: 15 [1104000/1309486 (84%)]\tLoss: 38897.953125\n",
      "Train Epoch: 15 [1112000/1309486 (85%)]\tLoss: 40822.511719\n",
      "Train Epoch: 15 [1120000/1309486 (86%)]\tLoss: 36408.238281\n",
      "Train Epoch: 15 [1128000/1309486 (86%)]\tLoss: 40056.550781\n",
      "Train Epoch: 15 [1136000/1309486 (87%)]\tLoss: 38620.300781\n",
      "Train Epoch: 15 [1144000/1309486 (87%)]\tLoss: 36716.242188\n",
      "Train Epoch: 15 [1152000/1309486 (88%)]\tLoss: 39400.023438\n",
      "Train Epoch: 15 [1160000/1309486 (89%)]\tLoss: 42574.714844\n",
      "Train Epoch: 15 [1168000/1309486 (89%)]\tLoss: 37194.480469\n",
      "Train Epoch: 15 [1176000/1309486 (90%)]\tLoss: 39786.320312\n",
      "Train Epoch: 15 [1184000/1309486 (90%)]\tLoss: 38519.070312\n",
      "Train Epoch: 15 [1192000/1309486 (91%)]\tLoss: 39481.140625\n",
      "Train Epoch: 15 [1200000/1309486 (92%)]\tLoss: 38544.710938\n",
      "Train Epoch: 15 [1208000/1309486 (92%)]\tLoss: 41011.019531\n",
      "Train Epoch: 15 [1216000/1309486 (93%)]\tLoss: 39931.265625\n",
      "Train Epoch: 15 [1224000/1309486 (93%)]\tLoss: 38111.691406\n",
      "Train Epoch: 15 [1232000/1309486 (94%)]\tLoss: 39641.097656\n",
      "Train Epoch: 15 [1240000/1309486 (95%)]\tLoss: 38203.484375\n",
      "Train Epoch: 15 [1248000/1309486 (95%)]\tLoss: 39210.507812\n",
      "Train Epoch: 15 [1256000/1309486 (96%)]\tLoss: 40436.414062\n",
      "Train Epoch: 15 [1264000/1309486 (97%)]\tLoss: 36106.640625\n",
      "Train Epoch: 15 [1272000/1309486 (97%)]\tLoss: 37171.929688\n",
      "Train Epoch: 15 [1280000/1309486 (98%)]\tLoss: 38215.476562\n",
      "Train Epoch: 15 [1288000/1309486 (98%)]\tLoss: 41584.562500\n",
      "Train Epoch: 15 [1296000/1309486 (99%)]\tLoss: 38030.492188\n",
      "Train Epoch: 15 [1304000/1309486 (100%)]\tLoss: 37146.875000\n",
      "Epoch 15 model saved!\n",
      "epoch train time:  783.18148017\n",
      "Train Epoch: 16 [8000/1309486 (1%)]\tLoss: 37766.410156\n",
      "Train Epoch: 16 [16000/1309486 (1%)]\tLoss: 40512.851562\n",
      "Train Epoch: 16 [24000/1309486 (2%)]\tLoss: 39256.019531\n",
      "Train Epoch: 16 [32000/1309486 (2%)]\tLoss: 40398.476562\n",
      "Train Epoch: 16 [40000/1309486 (3%)]\tLoss: 38639.652344\n",
      "Train Epoch: 16 [48000/1309486 (4%)]\tLoss: 36131.558594\n",
      "Train Epoch: 16 [56000/1309486 (4%)]\tLoss: 39419.039062\n",
      "Train Epoch: 16 [64000/1309486 (5%)]\tLoss: 39574.027344\n",
      "Train Epoch: 16 [72000/1309486 (5%)]\tLoss: 39161.984375\n",
      "Train Epoch: 16 [80000/1309486 (6%)]\tLoss: 41957.898438\n",
      "Train Epoch: 16 [88000/1309486 (7%)]\tLoss: 37855.312500\n",
      "Train Epoch: 16 [96000/1309486 (7%)]\tLoss: 36769.566406\n",
      "Train Epoch: 16 [104000/1309486 (8%)]\tLoss: 39049.089844\n",
      "Train Epoch: 16 [112000/1309486 (9%)]\tLoss: 36898.250000\n",
      "Train Epoch: 16 [120000/1309486 (9%)]\tLoss: 37342.515625\n",
      "Train Epoch: 16 [128000/1309486 (10%)]\tLoss: 38998.148438\n",
      "Train Epoch: 16 [136000/1309486 (10%)]\tLoss: 35810.445312\n",
      "Train Epoch: 16 [144000/1309486 (11%)]\tLoss: 40141.500000\n",
      "Train Epoch: 16 [152000/1309486 (12%)]\tLoss: 40616.414062\n",
      "Train Epoch: 16 [160000/1309486 (12%)]\tLoss: 37816.453125\n",
      "Train Epoch: 16 [168000/1309486 (13%)]\tLoss: 41714.175781\n",
      "Train Epoch: 16 [176000/1309486 (13%)]\tLoss: 37260.031250\n",
      "Train Epoch: 16 [184000/1309486 (14%)]\tLoss: 38961.296875\n",
      "Train Epoch: 16 [192000/1309486 (15%)]\tLoss: 36438.554688\n",
      "Train Epoch: 16 [200000/1309486 (15%)]\tLoss: 36583.601562\n",
      "Train Epoch: 16 [208000/1309486 (16%)]\tLoss: 42194.433594\n",
      "Train Epoch: 16 [216000/1309486 (16%)]\tLoss: 35170.226562\n",
      "Train Epoch: 16 [224000/1309486 (17%)]\tLoss: 35775.554688\n",
      "Train Epoch: 16 [232000/1309486 (18%)]\tLoss: 37570.765625\n",
      "Train Epoch: 16 [240000/1309486 (18%)]\tLoss: 36464.703125\n",
      "Train Epoch: 16 [248000/1309486 (19%)]\tLoss: 41432.601562\n",
      "Train Epoch: 16 [256000/1309486 (20%)]\tLoss: 36327.234375\n",
      "Train Epoch: 16 [264000/1309486 (20%)]\tLoss: 37744.343750\n",
      "Train Epoch: 16 [272000/1309486 (21%)]\tLoss: 34557.675781\n",
      "Train Epoch: 16 [280000/1309486 (21%)]\tLoss: 39511.335938\n",
      "Train Epoch: 16 [288000/1309486 (22%)]\tLoss: 38117.253906\n",
      "Train Epoch: 16 [296000/1309486 (23%)]\tLoss: 39503.468750\n",
      "Train Epoch: 16 [304000/1309486 (23%)]\tLoss: 34234.062500\n",
      "Train Epoch: 16 [312000/1309486 (24%)]\tLoss: 37497.703125\n",
      "Train Epoch: 16 [320000/1309486 (24%)]\tLoss: 42029.511719\n",
      "Train Epoch: 16 [328000/1309486 (25%)]\tLoss: 41459.132812\n",
      "Train Epoch: 16 [336000/1309486 (26%)]\tLoss: 39378.937500\n",
      "Train Epoch: 16 [344000/1309486 (26%)]\tLoss: 41603.097656\n",
      "Train Epoch: 16 [352000/1309486 (27%)]\tLoss: 38847.378906\n",
      "Train Epoch: 16 [360000/1309486 (27%)]\tLoss: 41138.550781\n",
      "Train Epoch: 16 [368000/1309486 (28%)]\tLoss: 38019.082031\n",
      "Train Epoch: 16 [376000/1309486 (29%)]\tLoss: 37416.132812\n",
      "Train Epoch: 16 [384000/1309486 (29%)]\tLoss: 43461.859375\n",
      "Train Epoch: 16 [392000/1309486 (30%)]\tLoss: 37761.582031\n",
      "Train Epoch: 16 [400000/1309486 (31%)]\tLoss: 38333.007812\n",
      "Train Epoch: 16 [408000/1309486 (31%)]\tLoss: 35626.585938\n",
      "Train Epoch: 16 [416000/1309486 (32%)]\tLoss: 36465.765625\n",
      "Train Epoch: 16 [424000/1309486 (32%)]\tLoss: 40058.066406\n",
      "Train Epoch: 16 [432000/1309486 (33%)]\tLoss: 37747.558594\n",
      "Train Epoch: 16 [440000/1309486 (34%)]\tLoss: 36256.613281\n",
      "Train Epoch: 16 [448000/1309486 (34%)]\tLoss: 39599.011719\n",
      "Train Epoch: 16 [456000/1309486 (35%)]\tLoss: 35469.023438\n",
      "Train Epoch: 16 [464000/1309486 (35%)]\tLoss: 37604.617188\n",
      "Train Epoch: 16 [472000/1309486 (36%)]\tLoss: 36020.023438\n",
      "Train Epoch: 16 [480000/1309486 (37%)]\tLoss: 35779.898438\n",
      "Train Epoch: 16 [488000/1309486 (37%)]\tLoss: 35840.335938\n",
      "Train Epoch: 16 [496000/1309486 (38%)]\tLoss: 40181.691406\n",
      "Train Epoch: 16 [504000/1309486 (38%)]\tLoss: 36535.398438\n",
      "Train Epoch: 16 [512000/1309486 (39%)]\tLoss: 38990.183594\n",
      "Train Epoch: 16 [520000/1309486 (40%)]\tLoss: 39250.765625\n",
      "Train Epoch: 16 [528000/1309486 (40%)]\tLoss: 34710.128906\n",
      "Train Epoch: 16 [536000/1309486 (41%)]\tLoss: 34918.265625\n",
      "Train Epoch: 16 [544000/1309486 (42%)]\tLoss: 41021.644531\n",
      "Train Epoch: 16 [552000/1309486 (42%)]\tLoss: 39828.394531\n",
      "Train Epoch: 16 [560000/1309486 (43%)]\tLoss: 37591.453125\n",
      "Train Epoch: 16 [568000/1309486 (43%)]\tLoss: 40407.289062\n",
      "Train Epoch: 16 [576000/1309486 (44%)]\tLoss: 40486.804688\n",
      "Train Epoch: 16 [584000/1309486 (45%)]\tLoss: 40135.039062\n",
      "Train Epoch: 16 [592000/1309486 (45%)]\tLoss: 37151.738281\n",
      "Train Epoch: 16 [600000/1309486 (46%)]\tLoss: 37736.972656\n",
      "Train Epoch: 16 [608000/1309486 (46%)]\tLoss: 38531.468750\n",
      "Train Epoch: 16 [616000/1309486 (47%)]\tLoss: 39019.531250\n",
      "Train Epoch: 16 [624000/1309486 (48%)]\tLoss: 35454.484375\n",
      "Train Epoch: 16 [632000/1309486 (48%)]\tLoss: 38758.132812\n",
      "Train Epoch: 16 [640000/1309486 (49%)]\tLoss: 38542.273438\n",
      "Train Epoch: 16 [648000/1309486 (49%)]\tLoss: 42051.929688\n",
      "Train Epoch: 16 [656000/1309486 (50%)]\tLoss: 40902.292969\n",
      "Train Epoch: 16 [664000/1309486 (51%)]\tLoss: 35627.976562\n",
      "Train Epoch: 16 [672000/1309486 (51%)]\tLoss: 39629.742188\n",
      "Train Epoch: 16 [680000/1309486 (52%)]\tLoss: 35395.671875\n",
      "Train Epoch: 16 [688000/1309486 (53%)]\tLoss: 43351.515625\n",
      "Train Epoch: 16 [696000/1309486 (53%)]\tLoss: 39846.855469\n",
      "Train Epoch: 16 [704000/1309486 (54%)]\tLoss: 37078.671875\n",
      "Train Epoch: 16 [712000/1309486 (54%)]\tLoss: 36465.632812\n",
      "Train Epoch: 16 [720000/1309486 (55%)]\tLoss: 38149.703125\n",
      "Train Epoch: 16 [728000/1309486 (56%)]\tLoss: 35875.703125\n",
      "Train Epoch: 16 [736000/1309486 (56%)]\tLoss: 41096.824219\n",
      "Train Epoch: 16 [744000/1309486 (57%)]\tLoss: 37298.203125\n",
      "Train Epoch: 16 [752000/1309486 (57%)]\tLoss: 39338.625000\n",
      "Train Epoch: 16 [760000/1309486 (58%)]\tLoss: 35657.753906\n",
      "Train Epoch: 16 [768000/1309486 (59%)]\tLoss: 42108.246094\n",
      "Train Epoch: 16 [776000/1309486 (59%)]\tLoss: 36302.195312\n",
      "Train Epoch: 16 [784000/1309486 (60%)]\tLoss: 39983.757812\n",
      "Train Epoch: 16 [792000/1309486 (60%)]\tLoss: 37215.496094\n",
      "Train Epoch: 16 [800000/1309486 (61%)]\tLoss: 38999.371094\n",
      "Train Epoch: 16 [808000/1309486 (62%)]\tLoss: 39419.464844\n",
      "Train Epoch: 16 [816000/1309486 (62%)]\tLoss: 38778.468750\n",
      "Train Epoch: 16 [824000/1309486 (63%)]\tLoss: 37734.605469\n",
      "Train Epoch: 16 [832000/1309486 (64%)]\tLoss: 38068.730469\n",
      "Train Epoch: 16 [840000/1309486 (64%)]\tLoss: 39883.042969\n",
      "Train Epoch: 16 [848000/1309486 (65%)]\tLoss: 35322.929688\n",
      "Train Epoch: 16 [856000/1309486 (65%)]\tLoss: 37147.136719\n",
      "Train Epoch: 16 [864000/1309486 (66%)]\tLoss: 41874.312500\n",
      "Train Epoch: 16 [872000/1309486 (67%)]\tLoss: 44912.710938\n",
      "Train Epoch: 16 [880000/1309486 (67%)]\tLoss: 37682.414062\n",
      "Train Epoch: 16 [888000/1309486 (68%)]\tLoss: 36363.175781\n",
      "Train Epoch: 16 [896000/1309486 (68%)]\tLoss: 41309.500000\n",
      "Train Epoch: 16 [904000/1309486 (69%)]\tLoss: 37479.414062\n",
      "Train Epoch: 16 [912000/1309486 (70%)]\tLoss: 40244.710938\n",
      "Train Epoch: 16 [920000/1309486 (70%)]\tLoss: 34534.472656\n",
      "Train Epoch: 16 [928000/1309486 (71%)]\tLoss: 36051.835938\n",
      "Train Epoch: 16 [936000/1309486 (71%)]\tLoss: 33772.597656\n",
      "Train Epoch: 16 [944000/1309486 (72%)]\tLoss: 37211.941406\n",
      "Train Epoch: 16 [952000/1309486 (73%)]\tLoss: 37382.792969\n",
      "Train Epoch: 16 [960000/1309486 (73%)]\tLoss: 37480.550781\n",
      "Train Epoch: 16 [968000/1309486 (74%)]\tLoss: 39981.398438\n",
      "Train Epoch: 16 [976000/1309486 (75%)]\tLoss: 39527.085938\n",
      "Train Epoch: 16 [984000/1309486 (75%)]\tLoss: 38752.292969\n",
      "Train Epoch: 16 [992000/1309486 (76%)]\tLoss: 41112.406250\n",
      "Train Epoch: 16 [1000000/1309486 (76%)]\tLoss: 37306.562500\n",
      "Train Epoch: 16 [1008000/1309486 (77%)]\tLoss: 38697.785156\n",
      "Train Epoch: 16 [1016000/1309486 (78%)]\tLoss: 36906.441406\n",
      "Train Epoch: 16 [1024000/1309486 (78%)]\tLoss: 40634.085938\n",
      "Train Epoch: 16 [1032000/1309486 (79%)]\tLoss: 35157.851562\n",
      "Train Epoch: 16 [1040000/1309486 (79%)]\tLoss: 40275.343750\n",
      "Train Epoch: 16 [1048000/1309486 (80%)]\tLoss: 41819.964844\n",
      "Train Epoch: 16 [1056000/1309486 (81%)]\tLoss: 34574.406250\n",
      "Train Epoch: 16 [1064000/1309486 (81%)]\tLoss: 38664.910156\n",
      "Train Epoch: 16 [1072000/1309486 (82%)]\tLoss: 37883.140625\n",
      "Train Epoch: 16 [1080000/1309486 (82%)]\tLoss: 39417.574219\n",
      "Train Epoch: 16 [1088000/1309486 (83%)]\tLoss: 36284.695312\n",
      "Train Epoch: 16 [1096000/1309486 (84%)]\tLoss: 36851.414062\n",
      "Train Epoch: 16 [1104000/1309486 (84%)]\tLoss: 38125.480469\n",
      "Train Epoch: 16 [1112000/1309486 (85%)]\tLoss: 36793.167969\n",
      "Train Epoch: 16 [1120000/1309486 (86%)]\tLoss: 37255.476562\n",
      "Train Epoch: 16 [1128000/1309486 (86%)]\tLoss: 37159.757812\n",
      "Train Epoch: 16 [1136000/1309486 (87%)]\tLoss: 38372.914062\n",
      "Train Epoch: 16 [1144000/1309486 (87%)]\tLoss: 35872.523438\n",
      "Train Epoch: 16 [1152000/1309486 (88%)]\tLoss: 37567.992188\n",
      "Train Epoch: 16 [1160000/1309486 (89%)]\tLoss: 38086.785156\n",
      "Train Epoch: 16 [1168000/1309486 (89%)]\tLoss: 35227.433594\n",
      "Train Epoch: 16 [1176000/1309486 (90%)]\tLoss: 37977.425781\n",
      "Train Epoch: 16 [1184000/1309486 (90%)]\tLoss: 37892.054688\n",
      "Train Epoch: 16 [1192000/1309486 (91%)]\tLoss: 37468.453125\n",
      "Train Epoch: 16 [1200000/1309486 (92%)]\tLoss: 40939.335938\n",
      "Train Epoch: 16 [1208000/1309486 (92%)]\tLoss: 38307.625000\n",
      "Train Epoch: 16 [1216000/1309486 (93%)]\tLoss: 40390.835938\n",
      "Train Epoch: 16 [1224000/1309486 (93%)]\tLoss: 35396.796875\n",
      "Train Epoch: 16 [1232000/1309486 (94%)]\tLoss: 41500.636719\n",
      "Train Epoch: 16 [1240000/1309486 (95%)]\tLoss: 38457.910156\n",
      "Train Epoch: 16 [1248000/1309486 (95%)]\tLoss: 40946.117188\n",
      "Train Epoch: 16 [1256000/1309486 (96%)]\tLoss: 38136.394531\n",
      "Train Epoch: 16 [1264000/1309486 (97%)]\tLoss: 43049.914062\n",
      "Train Epoch: 16 [1272000/1309486 (97%)]\tLoss: 36689.832031\n",
      "Train Epoch: 16 [1280000/1309486 (98%)]\tLoss: 38080.882812\n",
      "Train Epoch: 16 [1288000/1309486 (98%)]\tLoss: 37375.765625\n",
      "Train Epoch: 16 [1296000/1309486 (99%)]\tLoss: 36749.625000\n",
      "Train Epoch: 16 [1304000/1309486 (100%)]\tLoss: 34214.867188\n",
      "Epoch 16 model saved!\n",
      "epoch train time:  784.56104517\n",
      "Train Epoch: 17 [8000/1309486 (1%)]\tLoss: 38193.937500\n",
      "Train Epoch: 17 [16000/1309486 (1%)]\tLoss: 41615.578125\n",
      "Train Epoch: 17 [24000/1309486 (2%)]\tLoss: 37580.386719\n",
      "Train Epoch: 17 [32000/1309486 (2%)]\tLoss: 38067.140625\n",
      "Train Epoch: 17 [40000/1309486 (3%)]\tLoss: 43077.246094\n",
      "Train Epoch: 17 [48000/1309486 (4%)]\tLoss: 40985.445312\n",
      "Train Epoch: 17 [56000/1309486 (4%)]\tLoss: 38765.414062\n",
      "Train Epoch: 17 [64000/1309486 (5%)]\tLoss: 41021.390625\n",
      "Train Epoch: 17 [72000/1309486 (5%)]\tLoss: 36213.429688\n",
      "Train Epoch: 17 [80000/1309486 (6%)]\tLoss: 35532.675781\n",
      "Train Epoch: 17 [88000/1309486 (7%)]\tLoss: 35954.117188\n",
      "Train Epoch: 17 [96000/1309486 (7%)]\tLoss: 39603.273438\n",
      "Train Epoch: 17 [104000/1309486 (8%)]\tLoss: 36660.484375\n",
      "Train Epoch: 17 [112000/1309486 (9%)]\tLoss: 41972.320312\n",
      "Train Epoch: 17 [120000/1309486 (9%)]\tLoss: 38188.996094\n",
      "Train Epoch: 17 [128000/1309486 (10%)]\tLoss: 37227.164062\n",
      "Train Epoch: 17 [136000/1309486 (10%)]\tLoss: 41127.292969\n",
      "Train Epoch: 17 [144000/1309486 (11%)]\tLoss: 37273.140625\n",
      "Train Epoch: 17 [152000/1309486 (12%)]\tLoss: 34145.753906\n",
      "Train Epoch: 17 [160000/1309486 (12%)]\tLoss: 39761.937500\n",
      "Train Epoch: 17 [168000/1309486 (13%)]\tLoss: 41073.632812\n",
      "Train Epoch: 17 [176000/1309486 (13%)]\tLoss: 40220.242188\n",
      "Train Epoch: 17 [184000/1309486 (14%)]\tLoss: 41739.234375\n",
      "Train Epoch: 17 [192000/1309486 (15%)]\tLoss: 37942.105469\n",
      "Train Epoch: 17 [200000/1309486 (15%)]\tLoss: 37846.324219\n",
      "Train Epoch: 17 [208000/1309486 (16%)]\tLoss: 36607.531250\n",
      "Train Epoch: 17 [216000/1309486 (16%)]\tLoss: 40557.351562\n",
      "Train Epoch: 17 [224000/1309486 (17%)]\tLoss: 40274.203125\n",
      "Train Epoch: 17 [232000/1309486 (18%)]\tLoss: 36043.140625\n",
      "Train Epoch: 17 [240000/1309486 (18%)]\tLoss: 36017.621094\n",
      "Train Epoch: 17 [248000/1309486 (19%)]\tLoss: 36245.589844\n",
      "Train Epoch: 17 [256000/1309486 (20%)]\tLoss: 38536.132812\n",
      "Train Epoch: 17 [264000/1309486 (20%)]\tLoss: 36507.882812\n",
      "Train Epoch: 17 [272000/1309486 (21%)]\tLoss: 37689.828125\n",
      "Train Epoch: 17 [280000/1309486 (21%)]\tLoss: 41153.820312\n",
      "Train Epoch: 17 [288000/1309486 (22%)]\tLoss: 41729.632812\n",
      "Train Epoch: 17 [296000/1309486 (23%)]\tLoss: 38831.597656\n",
      "Train Epoch: 17 [304000/1309486 (23%)]\tLoss: 39766.375000\n",
      "Train Epoch: 17 [312000/1309486 (24%)]\tLoss: 36823.023438\n",
      "Train Epoch: 17 [320000/1309486 (24%)]\tLoss: 39961.148438\n",
      "Train Epoch: 17 [328000/1309486 (25%)]\tLoss: 39446.750000\n",
      "Train Epoch: 17 [336000/1309486 (26%)]\tLoss: 40126.824219\n",
      "Train Epoch: 17 [344000/1309486 (26%)]\tLoss: 34918.820312\n",
      "Train Epoch: 17 [352000/1309486 (27%)]\tLoss: 36006.757812\n",
      "Train Epoch: 17 [360000/1309486 (27%)]\tLoss: 39721.078125\n",
      "Train Epoch: 17 [368000/1309486 (28%)]\tLoss: 38340.812500\n",
      "Train Epoch: 17 [376000/1309486 (29%)]\tLoss: 38858.753906\n",
      "Train Epoch: 17 [384000/1309486 (29%)]\tLoss: 33126.000000\n",
      "Train Epoch: 17 [392000/1309486 (30%)]\tLoss: 39988.363281\n",
      "Train Epoch: 17 [400000/1309486 (31%)]\tLoss: 38784.730469\n",
      "Train Epoch: 17 [408000/1309486 (31%)]\tLoss: 37765.414062\n",
      "Train Epoch: 17 [416000/1309486 (32%)]\tLoss: 38443.109375\n",
      "Train Epoch: 17 [424000/1309486 (32%)]\tLoss: 40753.992188\n",
      "Train Epoch: 17 [432000/1309486 (33%)]\tLoss: 37787.710938\n",
      "Train Epoch: 17 [440000/1309486 (34%)]\tLoss: 37533.898438\n",
      "Train Epoch: 17 [448000/1309486 (34%)]\tLoss: 37399.796875\n",
      "Train Epoch: 17 [456000/1309486 (35%)]\tLoss: 39692.453125\n",
      "Train Epoch: 17 [464000/1309486 (35%)]\tLoss: 42536.644531\n",
      "Train Epoch: 17 [472000/1309486 (36%)]\tLoss: 42153.328125\n",
      "Train Epoch: 17 [480000/1309486 (37%)]\tLoss: 37655.183594\n",
      "Train Epoch: 17 [488000/1309486 (37%)]\tLoss: 40860.871094\n",
      "Train Epoch: 17 [496000/1309486 (38%)]\tLoss: 38439.925781\n",
      "Train Epoch: 17 [504000/1309486 (38%)]\tLoss: 37187.148438\n",
      "Train Epoch: 17 [512000/1309486 (39%)]\tLoss: 39808.148438\n",
      "Train Epoch: 17 [520000/1309486 (40%)]\tLoss: 38319.703125\n",
      "Train Epoch: 17 [528000/1309486 (40%)]\tLoss: 35649.656250\n",
      "Train Epoch: 17 [536000/1309486 (41%)]\tLoss: 39355.261719\n",
      "Train Epoch: 17 [544000/1309486 (42%)]\tLoss: 43063.429688\n",
      "Train Epoch: 17 [552000/1309486 (42%)]\tLoss: 42902.953125\n",
      "Train Epoch: 17 [560000/1309486 (43%)]\tLoss: 40261.726562\n",
      "Train Epoch: 17 [568000/1309486 (43%)]\tLoss: 37291.914062\n",
      "Train Epoch: 17 [576000/1309486 (44%)]\tLoss: 38893.539062\n",
      "Train Epoch: 17 [584000/1309486 (45%)]\tLoss: 39021.230469\n",
      "Train Epoch: 17 [592000/1309486 (45%)]\tLoss: 40900.515625\n",
      "Train Epoch: 17 [600000/1309486 (46%)]\tLoss: 39015.757812\n",
      "Train Epoch: 17 [608000/1309486 (46%)]\tLoss: 39736.640625\n",
      "Train Epoch: 17 [616000/1309486 (47%)]\tLoss: 39968.351562\n",
      "Train Epoch: 17 [624000/1309486 (48%)]\tLoss: 40376.347656\n",
      "Train Epoch: 17 [632000/1309486 (48%)]\tLoss: 39394.531250\n",
      "Train Epoch: 17 [640000/1309486 (49%)]\tLoss: 36644.882812\n",
      "Train Epoch: 17 [648000/1309486 (49%)]\tLoss: 41188.156250\n",
      "Train Epoch: 17 [656000/1309486 (50%)]\tLoss: 37136.648438\n",
      "Train Epoch: 17 [664000/1309486 (51%)]\tLoss: 33221.777344\n",
      "Train Epoch: 17 [672000/1309486 (51%)]\tLoss: 40844.910156\n",
      "Train Epoch: 17 [680000/1309486 (52%)]\tLoss: 34318.726562\n",
      "Train Epoch: 17 [688000/1309486 (53%)]\tLoss: 35983.386719\n",
      "Train Epoch: 17 [696000/1309486 (53%)]\tLoss: 39313.984375\n",
      "Train Epoch: 17 [704000/1309486 (54%)]\tLoss: 38319.718750\n",
      "Train Epoch: 17 [712000/1309486 (54%)]\tLoss: 36449.382812\n",
      "Train Epoch: 17 [720000/1309486 (55%)]\tLoss: 34463.890625\n",
      "Train Epoch: 17 [728000/1309486 (56%)]\tLoss: 41083.445312\n",
      "Train Epoch: 17 [736000/1309486 (56%)]\tLoss: 38275.328125\n",
      "Train Epoch: 17 [744000/1309486 (57%)]\tLoss: 37042.101562\n",
      "Train Epoch: 17 [752000/1309486 (57%)]\tLoss: 42864.453125\n",
      "Train Epoch: 17 [760000/1309486 (58%)]\tLoss: 36999.292969\n",
      "Train Epoch: 17 [768000/1309486 (59%)]\tLoss: 37305.953125\n",
      "Train Epoch: 17 [776000/1309486 (59%)]\tLoss: 45529.093750\n",
      "Train Epoch: 17 [784000/1309486 (60%)]\tLoss: 36576.218750\n",
      "Train Epoch: 17 [792000/1309486 (60%)]\tLoss: 36235.500000\n",
      "Train Epoch: 17 [800000/1309486 (61%)]\tLoss: 42475.265625\n",
      "Train Epoch: 17 [808000/1309486 (62%)]\tLoss: 35814.453125\n",
      "Train Epoch: 17 [816000/1309486 (62%)]\tLoss: 35982.414062\n",
      "Train Epoch: 17 [824000/1309486 (63%)]\tLoss: 43754.507812\n",
      "Train Epoch: 17 [832000/1309486 (64%)]\tLoss: 37682.472656\n",
      "Train Epoch: 17 [840000/1309486 (64%)]\tLoss: 38086.691406\n",
      "Train Epoch: 17 [848000/1309486 (65%)]\tLoss: 44701.359375\n",
      "Train Epoch: 17 [856000/1309486 (65%)]\tLoss: 39560.621094\n",
      "Train Epoch: 17 [864000/1309486 (66%)]\tLoss: 36409.000000\n",
      "Train Epoch: 17 [872000/1309486 (67%)]\tLoss: 38412.468750\n",
      "Train Epoch: 17 [880000/1309486 (67%)]\tLoss: 38145.683594\n",
      "Train Epoch: 17 [888000/1309486 (68%)]\tLoss: 35106.281250\n",
      "Train Epoch: 17 [896000/1309486 (68%)]\tLoss: 39672.105469\n",
      "Train Epoch: 17 [904000/1309486 (69%)]\tLoss: 38001.664062\n",
      "Train Epoch: 17 [912000/1309486 (70%)]\tLoss: 41949.390625\n",
      "Train Epoch: 17 [920000/1309486 (70%)]\tLoss: 37592.765625\n",
      "Train Epoch: 17 [928000/1309486 (71%)]\tLoss: 36477.890625\n",
      "Train Epoch: 17 [936000/1309486 (71%)]\tLoss: 38989.730469\n",
      "Train Epoch: 17 [944000/1309486 (72%)]\tLoss: 37971.250000\n",
      "Train Epoch: 17 [952000/1309486 (73%)]\tLoss: 38861.125000\n",
      "Train Epoch: 17 [960000/1309486 (73%)]\tLoss: 35401.304688\n",
      "Train Epoch: 17 [968000/1309486 (74%)]\tLoss: 39809.535156\n",
      "Train Epoch: 17 [976000/1309486 (75%)]\tLoss: 42965.566406\n",
      "Train Epoch: 17 [984000/1309486 (75%)]\tLoss: 36272.316406\n",
      "Train Epoch: 17 [992000/1309486 (76%)]\tLoss: 36829.777344\n",
      "Train Epoch: 17 [1000000/1309486 (76%)]\tLoss: 39784.835938\n",
      "Train Epoch: 17 [1008000/1309486 (77%)]\tLoss: 38103.914062\n",
      "Train Epoch: 17 [1016000/1309486 (78%)]\tLoss: 39249.046875\n",
      "Train Epoch: 17 [1024000/1309486 (78%)]\tLoss: 39281.585938\n",
      "Train Epoch: 17 [1032000/1309486 (79%)]\tLoss: 43519.878906\n",
      "Train Epoch: 17 [1040000/1309486 (79%)]\tLoss: 38384.242188\n",
      "Train Epoch: 17 [1048000/1309486 (80%)]\tLoss: 40437.367188\n",
      "Train Epoch: 17 [1056000/1309486 (81%)]\tLoss: 39923.234375\n",
      "Train Epoch: 17 [1064000/1309486 (81%)]\tLoss: 38711.984375\n",
      "Train Epoch: 17 [1072000/1309486 (82%)]\tLoss: 37905.746094\n",
      "Train Epoch: 17 [1080000/1309486 (82%)]\tLoss: 40085.082031\n",
      "Train Epoch: 17 [1088000/1309486 (83%)]\tLoss: 40434.445312\n",
      "Train Epoch: 17 [1096000/1309486 (84%)]\tLoss: 41941.378906\n",
      "Train Epoch: 17 [1104000/1309486 (84%)]\tLoss: 36838.148438\n",
      "Train Epoch: 17 [1112000/1309486 (85%)]\tLoss: 39884.125000\n",
      "Train Epoch: 17 [1120000/1309486 (86%)]\tLoss: 39532.164062\n",
      "Train Epoch: 17 [1128000/1309486 (86%)]\tLoss: 43406.707031\n",
      "Train Epoch: 17 [1136000/1309486 (87%)]\tLoss: 37450.710938\n",
      "Train Epoch: 17 [1144000/1309486 (87%)]\tLoss: 37115.894531\n",
      "Train Epoch: 17 [1152000/1309486 (88%)]\tLoss: 36892.562500\n",
      "Train Epoch: 17 [1160000/1309486 (89%)]\tLoss: 37660.953125\n",
      "Train Epoch: 17 [1168000/1309486 (89%)]\tLoss: 43737.125000\n",
      "Train Epoch: 17 [1176000/1309486 (90%)]\tLoss: 38955.398438\n",
      "Train Epoch: 17 [1184000/1309486 (90%)]\tLoss: 38701.414062\n",
      "Train Epoch: 17 [1192000/1309486 (91%)]\tLoss: 41613.218750\n",
      "Train Epoch: 17 [1200000/1309486 (92%)]\tLoss: 39237.574219\n",
      "Train Epoch: 17 [1208000/1309486 (92%)]\tLoss: 33605.093750\n",
      "Train Epoch: 17 [1216000/1309486 (93%)]\tLoss: 37138.539062\n",
      "Train Epoch: 17 [1224000/1309486 (93%)]\tLoss: 40631.492188\n",
      "Train Epoch: 17 [1232000/1309486 (94%)]\tLoss: 41082.757812\n",
      "Train Epoch: 17 [1240000/1309486 (95%)]\tLoss: 32831.515625\n",
      "Train Epoch: 17 [1248000/1309486 (95%)]\tLoss: 44375.488281\n",
      "Train Epoch: 17 [1256000/1309486 (96%)]\tLoss: 35677.835938\n",
      "Train Epoch: 17 [1264000/1309486 (97%)]\tLoss: 41637.847656\n",
      "Train Epoch: 17 [1272000/1309486 (97%)]\tLoss: 36164.570312\n",
      "Train Epoch: 17 [1280000/1309486 (98%)]\tLoss: 39688.980469\n",
      "Train Epoch: 17 [1288000/1309486 (98%)]\tLoss: 38015.300781\n",
      "Train Epoch: 17 [1296000/1309486 (99%)]\tLoss: 39252.804688\n",
      "Train Epoch: 17 [1304000/1309486 (100%)]\tLoss: 37304.597656\n",
      "Epoch 17 model saved!\n",
      "epoch train time:  784.10725594\n",
      "Train Epoch: 18 [8000/1309486 (1%)]\tLoss: 39279.796875\n",
      "Train Epoch: 18 [16000/1309486 (1%)]\tLoss: 35743.210938\n",
      "Train Epoch: 18 [24000/1309486 (2%)]\tLoss: 37587.425781\n",
      "Train Epoch: 18 [32000/1309486 (2%)]\tLoss: 35038.671875\n",
      "Train Epoch: 18 [40000/1309486 (3%)]\tLoss: 39664.117188\n",
      "Train Epoch: 18 [48000/1309486 (4%)]\tLoss: 36977.187500\n",
      "Train Epoch: 18 [56000/1309486 (4%)]\tLoss: 39907.890625\n",
      "Train Epoch: 18 [64000/1309486 (5%)]\tLoss: 41375.437500\n",
      "Train Epoch: 18 [72000/1309486 (5%)]\tLoss: 35936.410156\n",
      "Train Epoch: 18 [80000/1309486 (6%)]\tLoss: 40819.359375\n",
      "Train Epoch: 18 [88000/1309486 (7%)]\tLoss: 36146.613281\n",
      "Train Epoch: 18 [96000/1309486 (7%)]\tLoss: 39434.191406\n",
      "Train Epoch: 18 [104000/1309486 (8%)]\tLoss: 41428.628906\n",
      "Train Epoch: 18 [112000/1309486 (9%)]\tLoss: 35597.542969\n",
      "Train Epoch: 18 [120000/1309486 (9%)]\tLoss: 36721.617188\n",
      "Train Epoch: 18 [128000/1309486 (10%)]\tLoss: 41119.160156\n",
      "Train Epoch: 18 [136000/1309486 (10%)]\tLoss: 35765.179688\n",
      "Train Epoch: 18 [144000/1309486 (11%)]\tLoss: 39755.296875\n",
      "Train Epoch: 18 [152000/1309486 (12%)]\tLoss: 38517.894531\n",
      "Train Epoch: 18 [160000/1309486 (12%)]\tLoss: 38066.121094\n",
      "Train Epoch: 18 [168000/1309486 (13%)]\tLoss: 34689.304688\n",
      "Train Epoch: 18 [176000/1309486 (13%)]\tLoss: 38960.066406\n",
      "Train Epoch: 18 [184000/1309486 (14%)]\tLoss: 38804.578125\n",
      "Train Epoch: 18 [192000/1309486 (15%)]\tLoss: 37804.039062\n",
      "Train Epoch: 18 [200000/1309486 (15%)]\tLoss: 41341.960938\n",
      "Train Epoch: 18 [208000/1309486 (16%)]\tLoss: 42842.519531\n",
      "Train Epoch: 18 [216000/1309486 (16%)]\tLoss: 37661.308594\n",
      "Train Epoch: 18 [224000/1309486 (17%)]\tLoss: 37187.625000\n",
      "Train Epoch: 18 [232000/1309486 (18%)]\tLoss: 37457.992188\n",
      "Train Epoch: 18 [240000/1309486 (18%)]\tLoss: 34408.132812\n",
      "Train Epoch: 18 [248000/1309486 (19%)]\tLoss: 44929.273438\n",
      "Train Epoch: 18 [256000/1309486 (20%)]\tLoss: 34603.871094\n",
      "Train Epoch: 18 [264000/1309486 (20%)]\tLoss: 40030.789062\n",
      "Train Epoch: 18 [272000/1309486 (21%)]\tLoss: 35359.296875\n",
      "Train Epoch: 18 [280000/1309486 (21%)]\tLoss: 38018.390625\n",
      "Train Epoch: 18 [288000/1309486 (22%)]\tLoss: 39328.332031\n",
      "Train Epoch: 18 [296000/1309486 (23%)]\tLoss: 34368.164062\n",
      "Train Epoch: 18 [304000/1309486 (23%)]\tLoss: 38228.851562\n",
      "Train Epoch: 18 [312000/1309486 (24%)]\tLoss: 42792.593750\n",
      "Train Epoch: 18 [320000/1309486 (24%)]\tLoss: 38947.960938\n",
      "Train Epoch: 18 [328000/1309486 (25%)]\tLoss: 38018.437500\n",
      "Train Epoch: 18 [336000/1309486 (26%)]\tLoss: 43462.964844\n",
      "Train Epoch: 18 [344000/1309486 (26%)]\tLoss: 38263.945312\n",
      "Train Epoch: 18 [352000/1309486 (27%)]\tLoss: 40658.269531\n",
      "Train Epoch: 18 [360000/1309486 (27%)]\tLoss: 33289.343750\n",
      "Train Epoch: 18 [368000/1309486 (28%)]\tLoss: 37163.058594\n",
      "Train Epoch: 18 [376000/1309486 (29%)]\tLoss: 34111.156250\n",
      "Train Epoch: 18 [384000/1309486 (29%)]\tLoss: 36193.503906\n",
      "Train Epoch: 18 [392000/1309486 (30%)]\tLoss: 43357.015625\n",
      "Train Epoch: 18 [400000/1309486 (31%)]\tLoss: 35235.101562\n",
      "Train Epoch: 18 [408000/1309486 (31%)]\tLoss: 39873.238281\n",
      "Train Epoch: 18 [416000/1309486 (32%)]\tLoss: 37885.125000\n",
      "Train Epoch: 18 [424000/1309486 (32%)]\tLoss: 38094.187500\n",
      "Train Epoch: 18 [432000/1309486 (33%)]\tLoss: 39311.671875\n",
      "Train Epoch: 18 [440000/1309486 (34%)]\tLoss: 38452.824219\n",
      "Train Epoch: 18 [448000/1309486 (34%)]\tLoss: 36899.628906\n",
      "Train Epoch: 18 [456000/1309486 (35%)]\tLoss: 38092.523438\n",
      "Train Epoch: 18 [464000/1309486 (35%)]\tLoss: 36261.980469\n",
      "Train Epoch: 18 [472000/1309486 (36%)]\tLoss: 40884.625000\n",
      "Train Epoch: 18 [480000/1309486 (37%)]\tLoss: 37046.027344\n",
      "Train Epoch: 18 [488000/1309486 (37%)]\tLoss: 40644.031250\n",
      "Train Epoch: 18 [496000/1309486 (38%)]\tLoss: 39072.468750\n",
      "Train Epoch: 18 [504000/1309486 (38%)]\tLoss: 39440.953125\n",
      "Train Epoch: 18 [512000/1309486 (39%)]\tLoss: 37673.796875\n",
      "Train Epoch: 18 [520000/1309486 (40%)]\tLoss: 39010.316406\n",
      "Train Epoch: 18 [528000/1309486 (40%)]\tLoss: 40662.343750\n",
      "Train Epoch: 18 [536000/1309486 (41%)]\tLoss: 37095.031250\n",
      "Train Epoch: 18 [544000/1309486 (42%)]\tLoss: 37921.625000\n",
      "Train Epoch: 18 [552000/1309486 (42%)]\tLoss: 36830.250000\n",
      "Train Epoch: 18 [560000/1309486 (43%)]\tLoss: 37955.343750\n",
      "Train Epoch: 18 [568000/1309486 (43%)]\tLoss: 39300.640625\n",
      "Train Epoch: 18 [576000/1309486 (44%)]\tLoss: 37097.921875\n",
      "Train Epoch: 18 [584000/1309486 (45%)]\tLoss: 36358.089844\n",
      "Train Epoch: 18 [592000/1309486 (45%)]\tLoss: 40781.539062\n",
      "Train Epoch: 18 [600000/1309486 (46%)]\tLoss: 35995.804688\n",
      "Train Epoch: 18 [608000/1309486 (46%)]\tLoss: 35341.574219\n",
      "Train Epoch: 18 [616000/1309486 (47%)]\tLoss: 38348.996094\n",
      "Train Epoch: 18 [624000/1309486 (48%)]\tLoss: 35913.453125\n",
      "Train Epoch: 18 [632000/1309486 (48%)]\tLoss: 35828.625000\n",
      "Train Epoch: 18 [640000/1309486 (49%)]\tLoss: 43265.683594\n",
      "Train Epoch: 18 [648000/1309486 (49%)]\tLoss: 38228.585938\n",
      "Train Epoch: 18 [656000/1309486 (50%)]\tLoss: 38270.238281\n",
      "Train Epoch: 18 [664000/1309486 (51%)]\tLoss: 41455.640625\n",
      "Train Epoch: 18 [672000/1309486 (51%)]\tLoss: 37022.000000\n",
      "Train Epoch: 18 [680000/1309486 (52%)]\tLoss: 38229.023438\n",
      "Train Epoch: 18 [688000/1309486 (53%)]\tLoss: 38617.472656\n",
      "Train Epoch: 18 [696000/1309486 (53%)]\tLoss: 37871.523438\n",
      "Train Epoch: 18 [704000/1309486 (54%)]\tLoss: 38186.914062\n",
      "Train Epoch: 18 [712000/1309486 (54%)]\tLoss: 41913.796875\n",
      "Train Epoch: 18 [720000/1309486 (55%)]\tLoss: 36407.453125\n",
      "Train Epoch: 18 [728000/1309486 (56%)]\tLoss: 40372.183594\n",
      "Train Epoch: 18 [736000/1309486 (56%)]\tLoss: 38944.085938\n",
      "Train Epoch: 18 [744000/1309486 (57%)]\tLoss: 35141.382812\n",
      "Train Epoch: 18 [752000/1309486 (57%)]\tLoss: 36812.281250\n",
      "Train Epoch: 18 [760000/1309486 (58%)]\tLoss: 39834.867188\n",
      "Train Epoch: 18 [768000/1309486 (59%)]\tLoss: 36981.101562\n",
      "Train Epoch: 18 [776000/1309486 (59%)]\tLoss: 38954.156250\n",
      "Train Epoch: 18 [784000/1309486 (60%)]\tLoss: 37654.101562\n",
      "Train Epoch: 18 [792000/1309486 (60%)]\tLoss: 38819.007812\n",
      "Train Epoch: 18 [800000/1309486 (61%)]\tLoss: 40750.109375\n",
      "Train Epoch: 18 [808000/1309486 (62%)]\tLoss: 41259.539062\n",
      "Train Epoch: 18 [816000/1309486 (62%)]\tLoss: 36900.812500\n",
      "Train Epoch: 18 [824000/1309486 (63%)]\tLoss: 36926.460938\n",
      "Train Epoch: 18 [832000/1309486 (64%)]\tLoss: 40917.437500\n",
      "Train Epoch: 18 [840000/1309486 (64%)]\tLoss: 38572.902344\n",
      "Train Epoch: 18 [848000/1309486 (65%)]\tLoss: 38891.804688\n",
      "Train Epoch: 18 [856000/1309486 (65%)]\tLoss: 39063.671875\n",
      "Train Epoch: 18 [864000/1309486 (66%)]\tLoss: 40452.644531\n",
      "Train Epoch: 18 [872000/1309486 (67%)]\tLoss: 37862.371094\n",
      "Train Epoch: 18 [880000/1309486 (67%)]\tLoss: 35949.539062\n",
      "Train Epoch: 18 [888000/1309486 (68%)]\tLoss: 42371.164062\n",
      "Train Epoch: 18 [896000/1309486 (68%)]\tLoss: 35410.582031\n",
      "Train Epoch: 18 [904000/1309486 (69%)]\tLoss: 33566.992188\n",
      "Train Epoch: 18 [912000/1309486 (70%)]\tLoss: 36260.726562\n",
      "Train Epoch: 18 [920000/1309486 (70%)]\tLoss: 43507.578125\n",
      "Train Epoch: 18 [928000/1309486 (71%)]\tLoss: 38850.664062\n",
      "Train Epoch: 18 [936000/1309486 (71%)]\tLoss: 38125.015625\n",
      "Train Epoch: 18 [944000/1309486 (72%)]\tLoss: 37793.128906\n",
      "Train Epoch: 18 [952000/1309486 (73%)]\tLoss: 38366.242188\n",
      "Train Epoch: 18 [960000/1309486 (73%)]\tLoss: 41848.824219\n",
      "Train Epoch: 18 [968000/1309486 (74%)]\tLoss: 39044.585938\n",
      "Train Epoch: 18 [976000/1309486 (75%)]\tLoss: 37559.148438\n",
      "Train Epoch: 18 [984000/1309486 (75%)]\tLoss: 39917.222656\n",
      "Train Epoch: 18 [992000/1309486 (76%)]\tLoss: 36656.718750\n",
      "Train Epoch: 18 [1000000/1309486 (76%)]\tLoss: 36379.652344\n",
      "Train Epoch: 18 [1008000/1309486 (77%)]\tLoss: 39000.035156\n",
      "Train Epoch: 18 [1016000/1309486 (78%)]\tLoss: 39112.531250\n",
      "Train Epoch: 18 [1024000/1309486 (78%)]\tLoss: 45100.832031\n",
      "Train Epoch: 18 [1032000/1309486 (79%)]\tLoss: 36283.089844\n",
      "Train Epoch: 18 [1040000/1309486 (79%)]\tLoss: 39561.898438\n",
      "Train Epoch: 18 [1048000/1309486 (80%)]\tLoss: 40744.171875\n",
      "Train Epoch: 18 [1056000/1309486 (81%)]\tLoss: 42765.171875\n",
      "Train Epoch: 18 [1064000/1309486 (81%)]\tLoss: 37640.343750\n",
      "Train Epoch: 18 [1072000/1309486 (82%)]\tLoss: 34287.843750\n",
      "Train Epoch: 18 [1080000/1309486 (82%)]\tLoss: 40270.132812\n",
      "Train Epoch: 18 [1088000/1309486 (83%)]\tLoss: 43171.855469\n",
      "Train Epoch: 18 [1096000/1309486 (84%)]\tLoss: 38283.593750\n",
      "Train Epoch: 18 [1104000/1309486 (84%)]\tLoss: 38234.937500\n",
      "Train Epoch: 18 [1112000/1309486 (85%)]\tLoss: 38818.171875\n",
      "Train Epoch: 18 [1120000/1309486 (86%)]\tLoss: 39922.347656\n",
      "Train Epoch: 18 [1128000/1309486 (86%)]\tLoss: 41818.648438\n",
      "Train Epoch: 18 [1136000/1309486 (87%)]\tLoss: 39743.117188\n",
      "Train Epoch: 18 [1144000/1309486 (87%)]\tLoss: 40161.753906\n",
      "Train Epoch: 18 [1152000/1309486 (88%)]\tLoss: 37053.039062\n",
      "Train Epoch: 18 [1160000/1309486 (89%)]\tLoss: 38393.863281\n",
      "Train Epoch: 18 [1168000/1309486 (89%)]\tLoss: 34093.816406\n",
      "Train Epoch: 18 [1176000/1309486 (90%)]\tLoss: 41659.703125\n",
      "Train Epoch: 18 [1184000/1309486 (90%)]\tLoss: 38711.371094\n",
      "Train Epoch: 18 [1192000/1309486 (91%)]\tLoss: 40681.234375\n",
      "Train Epoch: 18 [1200000/1309486 (92%)]\tLoss: 36981.714844\n",
      "Train Epoch: 18 [1208000/1309486 (92%)]\tLoss: 36858.476562\n",
      "Train Epoch: 18 [1216000/1309486 (93%)]\tLoss: 37356.316406\n",
      "Train Epoch: 18 [1224000/1309486 (93%)]\tLoss: 37862.601562\n",
      "Train Epoch: 18 [1232000/1309486 (94%)]\tLoss: 40160.664062\n",
      "Train Epoch: 18 [1240000/1309486 (95%)]\tLoss: 38264.035156\n",
      "Train Epoch: 18 [1248000/1309486 (95%)]\tLoss: 40329.914062\n",
      "Train Epoch: 18 [1256000/1309486 (96%)]\tLoss: 36073.632812\n",
      "Train Epoch: 18 [1264000/1309486 (97%)]\tLoss: 37223.648438\n",
      "Train Epoch: 18 [1272000/1309486 (97%)]\tLoss: 40443.632812\n",
      "Train Epoch: 18 [1280000/1309486 (98%)]\tLoss: 36363.171875\n",
      "Train Epoch: 18 [1288000/1309486 (98%)]\tLoss: 35938.843750\n",
      "Train Epoch: 18 [1296000/1309486 (99%)]\tLoss: 40941.406250\n",
      "Train Epoch: 18 [1304000/1309486 (100%)]\tLoss: 42524.921875\n",
      "Epoch 18 model saved!\n",
      "epoch train time:  784.10340047\n",
      "Train Epoch: 19 [8000/1309486 (1%)]\tLoss: 36711.820312\n",
      "Train Epoch: 19 [16000/1309486 (1%)]\tLoss: 35238.050781\n",
      "Train Epoch: 19 [24000/1309486 (2%)]\tLoss: 37792.914062\n",
      "Train Epoch: 19 [32000/1309486 (2%)]\tLoss: 38906.921875\n",
      "Train Epoch: 19 [40000/1309486 (3%)]\tLoss: 38537.980469\n",
      "Train Epoch: 19 [48000/1309486 (4%)]\tLoss: 36617.683594\n",
      "Train Epoch: 19 [56000/1309486 (4%)]\tLoss: 37621.988281\n",
      "Train Epoch: 19 [64000/1309486 (5%)]\tLoss: 36799.929688\n",
      "Train Epoch: 19 [72000/1309486 (5%)]\tLoss: 35644.785156\n",
      "Train Epoch: 19 [80000/1309486 (6%)]\tLoss: 39706.589844\n",
      "Train Epoch: 19 [88000/1309486 (7%)]\tLoss: 40004.257812\n",
      "Train Epoch: 19 [96000/1309486 (7%)]\tLoss: 34912.453125\n",
      "Train Epoch: 19 [104000/1309486 (8%)]\tLoss: 36210.484375\n",
      "Train Epoch: 19 [112000/1309486 (9%)]\tLoss: 39187.410156\n",
      "Train Epoch: 19 [120000/1309486 (9%)]\tLoss: 43342.503906\n",
      "Train Epoch: 19 [128000/1309486 (10%)]\tLoss: 36345.718750\n",
      "Train Epoch: 19 [136000/1309486 (10%)]\tLoss: 34144.382812\n",
      "Train Epoch: 19 [144000/1309486 (11%)]\tLoss: 35681.914062\n",
      "Train Epoch: 19 [152000/1309486 (12%)]\tLoss: 38193.390625\n",
      "Train Epoch: 19 [160000/1309486 (12%)]\tLoss: 36468.773438\n",
      "Train Epoch: 19 [168000/1309486 (13%)]\tLoss: 40175.238281\n",
      "Train Epoch: 19 [176000/1309486 (13%)]\tLoss: 38208.152344\n",
      "Train Epoch: 19 [184000/1309486 (14%)]\tLoss: 36847.843750\n",
      "Train Epoch: 19 [192000/1309486 (15%)]\tLoss: 39495.000000\n",
      "Train Epoch: 19 [200000/1309486 (15%)]\tLoss: 38387.468750\n",
      "Train Epoch: 19 [208000/1309486 (16%)]\tLoss: 35570.160156\n",
      "Train Epoch: 19 [216000/1309486 (16%)]\tLoss: 41646.312500\n",
      "Train Epoch: 19 [224000/1309486 (17%)]\tLoss: 42575.335938\n",
      "Train Epoch: 19 [232000/1309486 (18%)]\tLoss: 37202.445312\n",
      "Train Epoch: 19 [240000/1309486 (18%)]\tLoss: 36815.128906\n",
      "Train Epoch: 19 [248000/1309486 (19%)]\tLoss: 37949.527344\n",
      "Train Epoch: 19 [256000/1309486 (20%)]\tLoss: 39985.226562\n",
      "Train Epoch: 19 [264000/1309486 (20%)]\tLoss: 44792.878906\n",
      "Train Epoch: 19 [272000/1309486 (21%)]\tLoss: 37607.093750\n",
      "Train Epoch: 19 [280000/1309486 (21%)]\tLoss: 36796.148438\n",
      "Train Epoch: 19 [288000/1309486 (22%)]\tLoss: 37685.710938\n",
      "Train Epoch: 19 [296000/1309486 (23%)]\tLoss: 34018.750000\n",
      "Train Epoch: 19 [304000/1309486 (23%)]\tLoss: 38340.867188\n",
      "Train Epoch: 19 [312000/1309486 (24%)]\tLoss: 37896.757812\n",
      "Train Epoch: 19 [320000/1309486 (24%)]\tLoss: 40389.472656\n",
      "Train Epoch: 19 [328000/1309486 (25%)]\tLoss: 40204.808594\n",
      "Train Epoch: 19 [336000/1309486 (26%)]\tLoss: 35831.687500\n",
      "Train Epoch: 19 [344000/1309486 (26%)]\tLoss: 33432.320312\n",
      "Train Epoch: 19 [352000/1309486 (27%)]\tLoss: 38453.484375\n",
      "Train Epoch: 19 [360000/1309486 (27%)]\tLoss: 38551.355469\n",
      "Train Epoch: 19 [368000/1309486 (28%)]\tLoss: 40554.621094\n",
      "Train Epoch: 19 [376000/1309486 (29%)]\tLoss: 40332.488281\n",
      "Train Epoch: 19 [384000/1309486 (29%)]\tLoss: 35408.476562\n",
      "Train Epoch: 19 [392000/1309486 (30%)]\tLoss: 39192.539062\n",
      "Train Epoch: 19 [400000/1309486 (31%)]\tLoss: 36083.109375\n",
      "Train Epoch: 19 [408000/1309486 (31%)]\tLoss: 38317.687500\n",
      "Train Epoch: 19 [416000/1309486 (32%)]\tLoss: 37643.785156\n",
      "Train Epoch: 19 [424000/1309486 (32%)]\tLoss: 36604.250000\n",
      "Train Epoch: 19 [432000/1309486 (33%)]\tLoss: 36055.875000\n",
      "Train Epoch: 19 [440000/1309486 (34%)]\tLoss: 36884.125000\n",
      "Train Epoch: 19 [448000/1309486 (34%)]\tLoss: 33969.644531\n",
      "Train Epoch: 19 [456000/1309486 (35%)]\tLoss: 40331.437500\n",
      "Train Epoch: 19 [464000/1309486 (35%)]\tLoss: 36924.714844\n",
      "Train Epoch: 19 [472000/1309486 (36%)]\tLoss: 41190.257812\n",
      "Train Epoch: 19 [480000/1309486 (37%)]\tLoss: 37149.164062\n",
      "Train Epoch: 19 [488000/1309486 (37%)]\tLoss: 39008.625000\n",
      "Train Epoch: 19 [496000/1309486 (38%)]\tLoss: 35570.835938\n",
      "Train Epoch: 19 [504000/1309486 (38%)]\tLoss: 39488.332031\n",
      "Train Epoch: 19 [512000/1309486 (39%)]\tLoss: 37374.257812\n",
      "Train Epoch: 19 [520000/1309486 (40%)]\tLoss: 34536.890625\n",
      "Train Epoch: 19 [528000/1309486 (40%)]\tLoss: 34710.722656\n",
      "Train Epoch: 19 [536000/1309486 (41%)]\tLoss: 41680.445312\n",
      "Train Epoch: 19 [544000/1309486 (42%)]\tLoss: 37514.507812\n",
      "Train Epoch: 19 [552000/1309486 (42%)]\tLoss: 42821.050781\n",
      "Train Epoch: 19 [560000/1309486 (43%)]\tLoss: 36432.984375\n",
      "Train Epoch: 19 [568000/1309486 (43%)]\tLoss: 35632.964844\n",
      "Train Epoch: 19 [576000/1309486 (44%)]\tLoss: 38291.570312\n",
      "Train Epoch: 19 [584000/1309486 (45%)]\tLoss: 38912.859375\n",
      "Train Epoch: 19 [592000/1309486 (45%)]\tLoss: 38292.519531\n",
      "Train Epoch: 19 [600000/1309486 (46%)]\tLoss: 38893.023438\n",
      "Train Epoch: 19 [608000/1309486 (46%)]\tLoss: 39013.500000\n",
      "Train Epoch: 19 [616000/1309486 (47%)]\tLoss: 41319.054688\n",
      "Train Epoch: 19 [624000/1309486 (48%)]\tLoss: 39233.542969\n",
      "Train Epoch: 19 [632000/1309486 (48%)]\tLoss: 39998.601562\n",
      "Train Epoch: 19 [640000/1309486 (49%)]\tLoss: 36810.453125\n",
      "Train Epoch: 19 [648000/1309486 (49%)]\tLoss: 37360.218750\n",
      "Train Epoch: 19 [656000/1309486 (50%)]\tLoss: 38894.425781\n",
      "Train Epoch: 19 [664000/1309486 (51%)]\tLoss: 40406.828125\n",
      "Train Epoch: 19 [672000/1309486 (51%)]\tLoss: 40326.054688\n",
      "Train Epoch: 19 [680000/1309486 (52%)]\tLoss: 39956.855469\n",
      "Train Epoch: 19 [688000/1309486 (53%)]\tLoss: 37731.324219\n",
      "Train Epoch: 19 [696000/1309486 (53%)]\tLoss: 41918.445312\n",
      "Train Epoch: 19 [704000/1309486 (54%)]\tLoss: 35578.804688\n",
      "Train Epoch: 19 [712000/1309486 (54%)]\tLoss: 39375.328125\n",
      "Train Epoch: 19 [720000/1309486 (55%)]\tLoss: 35787.359375\n",
      "Train Epoch: 19 [728000/1309486 (56%)]\tLoss: 36407.210938\n",
      "Train Epoch: 19 [736000/1309486 (56%)]\tLoss: 37784.160156\n",
      "Train Epoch: 19 [744000/1309486 (57%)]\tLoss: 36686.882812\n",
      "Train Epoch: 19 [752000/1309486 (57%)]\tLoss: 38737.785156\n",
      "Train Epoch: 19 [760000/1309486 (58%)]\tLoss: 39291.117188\n",
      "Train Epoch: 19 [768000/1309486 (59%)]\tLoss: 37422.792969\n",
      "Train Epoch: 19 [776000/1309486 (59%)]\tLoss: 37984.312500\n",
      "Train Epoch: 19 [784000/1309486 (60%)]\tLoss: 38815.445312\n",
      "Train Epoch: 19 [792000/1309486 (60%)]\tLoss: 38596.878906\n",
      "Train Epoch: 19 [800000/1309486 (61%)]\tLoss: 35861.992188\n",
      "Train Epoch: 19 [808000/1309486 (62%)]\tLoss: 40764.046875\n",
      "Train Epoch: 19 [816000/1309486 (62%)]\tLoss: 38302.257812\n",
      "Train Epoch: 19 [824000/1309486 (63%)]\tLoss: 38905.234375\n",
      "Train Epoch: 19 [832000/1309486 (64%)]\tLoss: 36578.800781\n",
      "Train Epoch: 19 [840000/1309486 (64%)]\tLoss: 38701.230469\n",
      "Train Epoch: 19 [848000/1309486 (65%)]\tLoss: 36440.820312\n",
      "Train Epoch: 19 [856000/1309486 (65%)]\tLoss: 35871.773438\n",
      "Train Epoch: 19 [864000/1309486 (66%)]\tLoss: 36261.921875\n",
      "Train Epoch: 19 [872000/1309486 (67%)]\tLoss: 36577.824219\n",
      "Train Epoch: 19 [880000/1309486 (67%)]\tLoss: 40745.007812\n",
      "Train Epoch: 19 [888000/1309486 (68%)]\tLoss: 38149.152344\n",
      "Train Epoch: 19 [896000/1309486 (68%)]\tLoss: 40638.871094\n",
      "Train Epoch: 19 [904000/1309486 (69%)]\tLoss: 39074.425781\n",
      "Train Epoch: 19 [912000/1309486 (70%)]\tLoss: 37800.578125\n",
      "Train Epoch: 19 [920000/1309486 (70%)]\tLoss: 38649.234375\n",
      "Train Epoch: 19 [928000/1309486 (71%)]\tLoss: 39423.300781\n",
      "Train Epoch: 19 [936000/1309486 (71%)]\tLoss: 39005.460938\n",
      "Train Epoch: 19 [944000/1309486 (72%)]\tLoss: 41379.148438\n",
      "Train Epoch: 19 [952000/1309486 (73%)]\tLoss: 37349.367188\n",
      "Train Epoch: 19 [960000/1309486 (73%)]\tLoss: 36574.914062\n",
      "Train Epoch: 19 [968000/1309486 (74%)]\tLoss: 37074.664062\n",
      "Train Epoch: 19 [976000/1309486 (75%)]\tLoss: 37248.351562\n",
      "Train Epoch: 19 [984000/1309486 (75%)]\tLoss: 39242.425781\n",
      "Train Epoch: 19 [992000/1309486 (76%)]\tLoss: 38691.554688\n",
      "Train Epoch: 19 [1000000/1309486 (76%)]\tLoss: 35282.765625\n",
      "Train Epoch: 19 [1008000/1309486 (77%)]\tLoss: 38583.566406\n",
      "Train Epoch: 19 [1016000/1309486 (78%)]\tLoss: 38447.687500\n",
      "Train Epoch: 19 [1024000/1309486 (78%)]\tLoss: 38100.492188\n",
      "Train Epoch: 19 [1032000/1309486 (79%)]\tLoss: 38492.476562\n",
      "Train Epoch: 19 [1040000/1309486 (79%)]\tLoss: 35338.347656\n",
      "Train Epoch: 19 [1048000/1309486 (80%)]\tLoss: 40939.359375\n",
      "Train Epoch: 19 [1056000/1309486 (81%)]\tLoss: 35508.113281\n",
      "Train Epoch: 19 [1064000/1309486 (81%)]\tLoss: 39932.726562\n",
      "Train Epoch: 19 [1072000/1309486 (82%)]\tLoss: 41753.710938\n",
      "Train Epoch: 19 [1080000/1309486 (82%)]\tLoss: 36509.312500\n",
      "Train Epoch: 19 [1088000/1309486 (83%)]\tLoss: 36867.832031\n",
      "Train Epoch: 19 [1096000/1309486 (84%)]\tLoss: 41196.949219\n",
      "Train Epoch: 19 [1104000/1309486 (84%)]\tLoss: 38047.839844\n",
      "Train Epoch: 19 [1112000/1309486 (85%)]\tLoss: 44227.789062\n",
      "Train Epoch: 19 [1120000/1309486 (86%)]\tLoss: 37117.214844\n",
      "Train Epoch: 19 [1128000/1309486 (86%)]\tLoss: 40337.062500\n",
      "Train Epoch: 19 [1136000/1309486 (87%)]\tLoss: 38638.359375\n",
      "Train Epoch: 19 [1144000/1309486 (87%)]\tLoss: 36759.257812\n",
      "Train Epoch: 19 [1152000/1309486 (88%)]\tLoss: 41676.781250\n",
      "Train Epoch: 19 [1160000/1309486 (89%)]\tLoss: 40490.781250\n",
      "Train Epoch: 19 [1168000/1309486 (89%)]\tLoss: 37133.492188\n",
      "Train Epoch: 19 [1176000/1309486 (90%)]\tLoss: 35380.433594\n",
      "Train Epoch: 19 [1184000/1309486 (90%)]\tLoss: 37989.875000\n",
      "Train Epoch: 19 [1192000/1309486 (91%)]\tLoss: 42865.472656\n",
      "Train Epoch: 19 [1200000/1309486 (92%)]\tLoss: 42678.300781\n",
      "Train Epoch: 19 [1208000/1309486 (92%)]\tLoss: 37716.429688\n",
      "Train Epoch: 19 [1216000/1309486 (93%)]\tLoss: 36541.964844\n",
      "Train Epoch: 19 [1224000/1309486 (93%)]\tLoss: 38260.398438\n",
      "Train Epoch: 19 [1232000/1309486 (94%)]\tLoss: 39850.847656\n",
      "Train Epoch: 19 [1240000/1309486 (95%)]\tLoss: 38754.851562\n",
      "Train Epoch: 19 [1248000/1309486 (95%)]\tLoss: 39455.812500\n",
      "Train Epoch: 19 [1256000/1309486 (96%)]\tLoss: 39722.429688\n",
      "Train Epoch: 19 [1264000/1309486 (97%)]\tLoss: 37908.312500\n",
      "Train Epoch: 19 [1272000/1309486 (97%)]\tLoss: 38090.851562\n",
      "Train Epoch: 19 [1280000/1309486 (98%)]\tLoss: 36239.796875\n",
      "Train Epoch: 19 [1288000/1309486 (98%)]\tLoss: 34598.609375\n",
      "Train Epoch: 19 [1296000/1309486 (99%)]\tLoss: 40003.355469\n",
      "Train Epoch: 19 [1304000/1309486 (100%)]\tLoss: 40869.371094\n",
      "Epoch 19 model saved!\n",
      "epoch train time:  783.25776768\n",
      "Train Epoch: 20 [8000/1309486 (1%)]\tLoss: 38910.183594\n",
      "Train Epoch: 20 [16000/1309486 (1%)]\tLoss: 37674.234375\n",
      "Train Epoch: 20 [24000/1309486 (2%)]\tLoss: 42103.523438\n",
      "Train Epoch: 20 [32000/1309486 (2%)]\tLoss: 37883.191406\n",
      "Train Epoch: 20 [40000/1309486 (3%)]\tLoss: 36334.347656\n",
      "Train Epoch: 20 [48000/1309486 (4%)]\tLoss: 36371.515625\n",
      "Train Epoch: 20 [56000/1309486 (4%)]\tLoss: 39756.179688\n",
      "Train Epoch: 20 [64000/1309486 (5%)]\tLoss: 41780.093750\n",
      "Train Epoch: 20 [72000/1309486 (5%)]\tLoss: 42927.718750\n",
      "Train Epoch: 20 [80000/1309486 (6%)]\tLoss: 40000.039062\n",
      "Train Epoch: 20 [88000/1309486 (7%)]\tLoss: 41566.468750\n",
      "Train Epoch: 20 [96000/1309486 (7%)]\tLoss: 40499.437500\n",
      "Train Epoch: 20 [104000/1309486 (8%)]\tLoss: 41693.484375\n",
      "Train Epoch: 20 [112000/1309486 (9%)]\tLoss: 42239.562500\n",
      "Train Epoch: 20 [120000/1309486 (9%)]\tLoss: 45266.457031\n",
      "Train Epoch: 20 [128000/1309486 (10%)]\tLoss: 38058.121094\n",
      "Train Epoch: 20 [136000/1309486 (10%)]\tLoss: 35126.976562\n",
      "Train Epoch: 20 [144000/1309486 (11%)]\tLoss: 41892.148438\n",
      "Train Epoch: 20 [152000/1309486 (12%)]\tLoss: 36619.453125\n",
      "Train Epoch: 20 [160000/1309486 (12%)]\tLoss: 36587.445312\n",
      "Train Epoch: 20 [168000/1309486 (13%)]\tLoss: 38413.562500\n",
      "Train Epoch: 20 [176000/1309486 (13%)]\tLoss: 39633.367188\n",
      "Train Epoch: 20 [184000/1309486 (14%)]\tLoss: 37076.050781\n",
      "Train Epoch: 20 [192000/1309486 (15%)]\tLoss: 43763.031250\n",
      "Train Epoch: 20 [200000/1309486 (15%)]\tLoss: 36769.671875\n",
      "Train Epoch: 20 [208000/1309486 (16%)]\tLoss: 36123.511719\n",
      "Train Epoch: 20 [216000/1309486 (16%)]\tLoss: 40335.117188\n",
      "Train Epoch: 20 [224000/1309486 (17%)]\tLoss: 45188.320312\n",
      "Train Epoch: 20 [232000/1309486 (18%)]\tLoss: 38529.835938\n",
      "Train Epoch: 20 [240000/1309486 (18%)]\tLoss: 38495.519531\n",
      "Train Epoch: 20 [248000/1309486 (19%)]\tLoss: 34553.472656\n",
      "Train Epoch: 20 [256000/1309486 (20%)]\tLoss: 39114.703125\n",
      "Train Epoch: 20 [264000/1309486 (20%)]\tLoss: 37067.117188\n",
      "Train Epoch: 20 [272000/1309486 (21%)]\tLoss: 37980.039062\n",
      "Train Epoch: 20 [280000/1309486 (21%)]\tLoss: 35971.578125\n",
      "Train Epoch: 20 [288000/1309486 (22%)]\tLoss: 38435.234375\n",
      "Train Epoch: 20 [296000/1309486 (23%)]\tLoss: 40497.703125\n",
      "Train Epoch: 20 [304000/1309486 (23%)]\tLoss: 39111.058594\n",
      "Train Epoch: 20 [312000/1309486 (24%)]\tLoss: 42413.132812\n",
      "Train Epoch: 20 [320000/1309486 (24%)]\tLoss: 41945.863281\n",
      "Train Epoch: 20 [328000/1309486 (25%)]\tLoss: 35692.125000\n",
      "Train Epoch: 20 [336000/1309486 (26%)]\tLoss: 36794.199219\n",
      "Train Epoch: 20 [344000/1309486 (26%)]\tLoss: 38646.210938\n",
      "Train Epoch: 20 [352000/1309486 (27%)]\tLoss: 39444.710938\n",
      "Train Epoch: 20 [360000/1309486 (27%)]\tLoss: 40251.492188\n",
      "Train Epoch: 20 [368000/1309486 (28%)]\tLoss: 41860.617188\n",
      "Train Epoch: 20 [376000/1309486 (29%)]\tLoss: 38299.875000\n",
      "Train Epoch: 20 [384000/1309486 (29%)]\tLoss: 38736.113281\n",
      "Train Epoch: 20 [392000/1309486 (30%)]\tLoss: 36047.382812\n",
      "Train Epoch: 20 [400000/1309486 (31%)]\tLoss: 37047.101562\n",
      "Train Epoch: 20 [408000/1309486 (31%)]\tLoss: 40955.976562\n",
      "Train Epoch: 20 [416000/1309486 (32%)]\tLoss: 39513.179688\n",
      "Train Epoch: 20 [424000/1309486 (32%)]\tLoss: 37354.738281\n",
      "Train Epoch: 20 [432000/1309486 (33%)]\tLoss: 37334.093750\n",
      "Train Epoch: 20 [440000/1309486 (34%)]\tLoss: 33700.539062\n",
      "Train Epoch: 20 [448000/1309486 (34%)]\tLoss: 38130.191406\n",
      "Train Epoch: 20 [456000/1309486 (35%)]\tLoss: 41662.992188\n",
      "Train Epoch: 20 [464000/1309486 (35%)]\tLoss: 38271.339844\n",
      "Train Epoch: 20 [472000/1309486 (36%)]\tLoss: 37745.121094\n",
      "Train Epoch: 20 [480000/1309486 (37%)]\tLoss: 39628.210938\n",
      "Train Epoch: 20 [488000/1309486 (37%)]\tLoss: 38832.371094\n",
      "Train Epoch: 20 [496000/1309486 (38%)]\tLoss: 39306.648438\n",
      "Train Epoch: 20 [504000/1309486 (38%)]\tLoss: 38301.730469\n",
      "Train Epoch: 20 [512000/1309486 (39%)]\tLoss: 37689.484375\n",
      "Train Epoch: 20 [520000/1309486 (40%)]\tLoss: 37778.882812\n",
      "Train Epoch: 20 [528000/1309486 (40%)]\tLoss: 37201.253906\n",
      "Train Epoch: 20 [536000/1309486 (41%)]\tLoss: 36963.425781\n",
      "Train Epoch: 20 [544000/1309486 (42%)]\tLoss: 36809.007812\n",
      "Train Epoch: 20 [552000/1309486 (42%)]\tLoss: 34716.921875\n",
      "Train Epoch: 20 [560000/1309486 (43%)]\tLoss: 39060.929688\n",
      "Train Epoch: 20 [568000/1309486 (43%)]\tLoss: 38474.492188\n",
      "Train Epoch: 20 [576000/1309486 (44%)]\tLoss: 42438.789062\n",
      "Train Epoch: 20 [584000/1309486 (45%)]\tLoss: 41434.960938\n",
      "Train Epoch: 20 [592000/1309486 (45%)]\tLoss: 44285.671875\n",
      "Train Epoch: 20 [600000/1309486 (46%)]\tLoss: 39863.179688\n",
      "Train Epoch: 20 [608000/1309486 (46%)]\tLoss: 35377.171875\n",
      "Train Epoch: 20 [616000/1309486 (47%)]\tLoss: 38515.175781\n",
      "Train Epoch: 20 [624000/1309486 (48%)]\tLoss: 38541.890625\n",
      "Train Epoch: 20 [632000/1309486 (48%)]\tLoss: 38564.765625\n",
      "Train Epoch: 20 [640000/1309486 (49%)]\tLoss: 37103.203125\n",
      "Train Epoch: 20 [648000/1309486 (49%)]\tLoss: 40666.031250\n",
      "Train Epoch: 20 [656000/1309486 (50%)]\tLoss: 37147.312500\n",
      "Train Epoch: 20 [664000/1309486 (51%)]\tLoss: 41481.410156\n",
      "Train Epoch: 20 [672000/1309486 (51%)]\tLoss: 35022.171875\n",
      "Train Epoch: 20 [680000/1309486 (52%)]\tLoss: 39852.234375\n",
      "Train Epoch: 20 [688000/1309486 (53%)]\tLoss: 39309.945312\n",
      "Train Epoch: 20 [696000/1309486 (53%)]\tLoss: 40459.718750\n",
      "Train Epoch: 20 [704000/1309486 (54%)]\tLoss: 41651.433594\n",
      "Train Epoch: 20 [712000/1309486 (54%)]\tLoss: 37644.320312\n",
      "Train Epoch: 20 [720000/1309486 (55%)]\tLoss: 37321.464844\n",
      "Train Epoch: 20 [728000/1309486 (56%)]\tLoss: 38749.398438\n",
      "Train Epoch: 20 [736000/1309486 (56%)]\tLoss: 34694.425781\n",
      "Train Epoch: 20 [744000/1309486 (57%)]\tLoss: 38798.000000\n",
      "Train Epoch: 20 [752000/1309486 (57%)]\tLoss: 37581.375000\n",
      "Train Epoch: 20 [760000/1309486 (58%)]\tLoss: 39643.472656\n",
      "Train Epoch: 20 [768000/1309486 (59%)]\tLoss: 44724.718750\n",
      "Train Epoch: 20 [776000/1309486 (59%)]\tLoss: 36684.992188\n",
      "Train Epoch: 20 [784000/1309486 (60%)]\tLoss: 41761.843750\n",
      "Train Epoch: 20 [792000/1309486 (60%)]\tLoss: 38466.644531\n",
      "Train Epoch: 20 [800000/1309486 (61%)]\tLoss: 42591.257812\n",
      "Train Epoch: 20 [808000/1309486 (62%)]\tLoss: 38583.113281\n",
      "Train Epoch: 20 [816000/1309486 (62%)]\tLoss: 38141.042969\n",
      "Train Epoch: 20 [824000/1309486 (63%)]\tLoss: 38070.523438\n",
      "Train Epoch: 20 [832000/1309486 (64%)]\tLoss: 36076.429688\n",
      "Train Epoch: 20 [840000/1309486 (64%)]\tLoss: 40332.687500\n",
      "Train Epoch: 20 [848000/1309486 (65%)]\tLoss: 40564.261719\n",
      "Train Epoch: 20 [856000/1309486 (65%)]\tLoss: 42249.000000\n",
      "Train Epoch: 20 [864000/1309486 (66%)]\tLoss: 39535.203125\n",
      "Train Epoch: 20 [872000/1309486 (67%)]\tLoss: 34137.367188\n",
      "Train Epoch: 20 [880000/1309486 (67%)]\tLoss: 38028.835938\n",
      "Train Epoch: 20 [888000/1309486 (68%)]\tLoss: 38683.851562\n",
      "Train Epoch: 20 [896000/1309486 (68%)]\tLoss: 39979.687500\n",
      "Train Epoch: 20 [904000/1309486 (69%)]\tLoss: 39329.609375\n",
      "Train Epoch: 20 [912000/1309486 (70%)]\tLoss: 40198.875000\n",
      "Train Epoch: 20 [920000/1309486 (70%)]\tLoss: 39708.324219\n",
      "Train Epoch: 20 [928000/1309486 (71%)]\tLoss: 35076.250000\n",
      "Train Epoch: 20 [936000/1309486 (71%)]\tLoss: 39426.414062\n",
      "Train Epoch: 20 [944000/1309486 (72%)]\tLoss: 39339.265625\n",
      "Train Epoch: 20 [952000/1309486 (73%)]\tLoss: 36072.800781\n",
      "Train Epoch: 20 [960000/1309486 (73%)]\tLoss: 39149.750000\n",
      "Train Epoch: 20 [968000/1309486 (74%)]\tLoss: 39490.757812\n",
      "Train Epoch: 20 [976000/1309486 (75%)]\tLoss: 41698.304688\n",
      "Train Epoch: 20 [984000/1309486 (75%)]\tLoss: 38176.144531\n",
      "Train Epoch: 20 [992000/1309486 (76%)]\tLoss: 39850.601562\n",
      "Train Epoch: 20 [1000000/1309486 (76%)]\tLoss: 37911.289062\n",
      "Train Epoch: 20 [1008000/1309486 (77%)]\tLoss: 38969.632812\n",
      "Train Epoch: 20 [1016000/1309486 (78%)]\tLoss: 40239.804688\n",
      "Train Epoch: 20 [1024000/1309486 (78%)]\tLoss: 35839.578125\n",
      "Train Epoch: 20 [1032000/1309486 (79%)]\tLoss: 36016.937500\n",
      "Train Epoch: 20 [1040000/1309486 (79%)]\tLoss: 35089.000000\n",
      "Train Epoch: 20 [1048000/1309486 (80%)]\tLoss: 38898.457031\n",
      "Train Epoch: 20 [1056000/1309486 (81%)]\tLoss: 36886.394531\n",
      "Train Epoch: 20 [1064000/1309486 (81%)]\tLoss: 36875.660156\n",
      "Train Epoch: 20 [1072000/1309486 (82%)]\tLoss: 36715.007812\n",
      "Train Epoch: 20 [1080000/1309486 (82%)]\tLoss: 36607.562500\n",
      "Train Epoch: 20 [1088000/1309486 (83%)]\tLoss: 37940.257812\n",
      "Train Epoch: 20 [1096000/1309486 (84%)]\tLoss: 40158.695312\n",
      "Train Epoch: 20 [1104000/1309486 (84%)]\tLoss: 37851.953125\n",
      "Train Epoch: 20 [1112000/1309486 (85%)]\tLoss: 37345.464844\n",
      "Train Epoch: 20 [1120000/1309486 (86%)]\tLoss: 37847.984375\n",
      "Train Epoch: 20 [1128000/1309486 (86%)]\tLoss: 40099.093750\n",
      "Train Epoch: 20 [1136000/1309486 (87%)]\tLoss: 36803.359375\n",
      "Train Epoch: 20 [1144000/1309486 (87%)]\tLoss: 39174.625000\n",
      "Train Epoch: 20 [1152000/1309486 (88%)]\tLoss: 37584.468750\n",
      "Train Epoch: 20 [1160000/1309486 (89%)]\tLoss: 37383.359375\n",
      "Train Epoch: 20 [1168000/1309486 (89%)]\tLoss: 35193.101562\n",
      "Train Epoch: 20 [1176000/1309486 (90%)]\tLoss: 37604.535156\n",
      "Train Epoch: 20 [1184000/1309486 (90%)]\tLoss: 37265.507812\n",
      "Train Epoch: 20 [1192000/1309486 (91%)]\tLoss: 38064.343750\n",
      "Train Epoch: 20 [1200000/1309486 (92%)]\tLoss: 38442.750000\n",
      "Train Epoch: 20 [1208000/1309486 (92%)]\tLoss: 40102.265625\n",
      "Train Epoch: 20 [1216000/1309486 (93%)]\tLoss: 39095.367188\n",
      "Train Epoch: 20 [1224000/1309486 (93%)]\tLoss: 38423.164062\n",
      "Train Epoch: 20 [1232000/1309486 (94%)]\tLoss: 42625.824219\n",
      "Train Epoch: 20 [1240000/1309486 (95%)]\tLoss: 43765.843750\n",
      "Train Epoch: 20 [1248000/1309486 (95%)]\tLoss: 34441.296875\n",
      "Train Epoch: 20 [1256000/1309486 (96%)]\tLoss: 38876.011719\n",
      "Train Epoch: 20 [1264000/1309486 (97%)]\tLoss: 35963.875000\n",
      "Train Epoch: 20 [1272000/1309486 (97%)]\tLoss: 40914.898438\n",
      "Train Epoch: 20 [1280000/1309486 (98%)]\tLoss: 39223.929688\n",
      "Train Epoch: 20 [1288000/1309486 (98%)]\tLoss: 37702.476562\n",
      "Train Epoch: 20 [1296000/1309486 (99%)]\tLoss: 39002.453125\n",
      "Train Epoch: 20 [1304000/1309486 (100%)]\tLoss: 37031.242188\n",
      "Epoch 20 model saved!\n",
      "epoch train time:  783.62430382\n"
     ]
    }
   ],
   "source": [
    "# Detect devices\n",
    "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")   # use CPU or GPU\n",
    "\n",
    "# Data loading parameters\n",
    "params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                #transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # gray -> GRB 3 channel (lambda function)\n",
    "                                #transforms.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])\n",
    "                               ])  # for grayscale images\n",
    "\n",
    "# tiles dataset\n",
    "tiles_dataset = Dataset(dir_path='../WSI/tiles/', transform=transform, dataset='tr')\n",
    "\n",
    "# Data loader \n",
    "train_loader = torch.utils.data.DataLoader(dataset=tiles_dataset, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True)\n",
    "#valid_loader = torch.utils.data.DataLoader(dataset=tiles_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create model\n",
    "autoencoder = autoencoder(fc_hidden1=fc_hidden1, drop_p=dropout_p, embed_dim=embed_dim).to(device, non_blocking=True)\n",
    "    \n",
    "# Recover model                                \n",
    "if start_epoch != 0:\n",
    "    model_load_path = os.path.join(save_model_path, 'model_epoch{}.pth'.format(start_epoch))\n",
    "    autoencoder.load_state_dict(torch.load(model_load_path))\n",
    "\n",
    "print(\"Using\", torch.cuda.device_count(), \"GPU\")\n",
    "model_params = list(autoencoder.parameters())\n",
    "optimizer = torch.optim.Adam(model_params, lr=learning_rate)\n",
    "# Recover optimizer                                \n",
    "if start_epoch != 0:\n",
    "    optimizer_load_path = os.path.join(save_model_path, 'optimizer_epoch{}.pth'.format(start_epoch))\n",
    "    optimizer.load_state_dict(torch.load(optimizer_load_path))\n",
    "    \n",
    "\n",
    "epoch_train_losses = []\n",
    "epoch_test_losses = []\n",
    "\n",
    "# start training\n",
    "for epoch in range(start_epoch, epochs):\n",
    "\n",
    "    # train, test model\n",
    "    X_train, z_train, train_losses = train(log_interval, autoencoder, device, train_loader, optimizer, epoch, save_model_path)\n",
    "    #X_test, y_test, z_test, mu_test, logvar_test, epoch_test_loss = validation(resnet_vae, device, optimizer, valid_loader)\n",
    "\n",
    "    # save results\n",
    "    epoch_train_losses.append(train_losses)\n",
    "    #epoch_test_losses.append(epoch_test_loss)\n",
    "\n",
    "    \n",
    "    # save all train test results\n",
    "    A = np.array(epoch_train_losses)\n",
    "    #C = np.array(epoch_test_losses)\n",
    "    \n",
    "    np.save(os.path.join(save_model_path, 'autoencoder_training_loss.npy'), A)\n",
    "    np.save(os.path.join(save_model_path, 'X_ae_train_epoch{}.npy'.format(epoch + 1)), X_train) #save last batch\n",
    "    np.save(os.path.join(save_model_path, 'z_ae_train_epoch{}.npy'.format(epoch + 1)), z_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "caeeaff7",
   "metadata": {},
   "source": [
    "Show training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f49592d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 163685)\n",
      "[[71545.328125   70975.140625   70417.421875   ... 39303.56640625\n",
      "  35512.20703125 38439.3984375 ]\n",
      " [39098.61328125 35166.28515625 39041.21484375 ... 38016.46875\n",
      "  40186.1953125  35283.3046875 ]\n",
      " [36541.3828125  37781.11328125 36086.01171875 ... 40265.44921875\n",
      "  38248.640625   39695.0703125 ]\n",
      " ...\n",
      " [39321.1328125  41430.8984375  39124.3671875  ... 39837.90625\n",
      "  37137.5234375  39156.5390625 ]\n",
      " [38816.3359375  36866.0703125  43822.3203125  ... 38284.0234375\n",
      "  41085.078125   40760.7421875 ]\n",
      " [39500.51171875 38099.1953125  40881.03125    ... 39864.62890625\n",
      "  35835.67578125 37615.39453125]]\n"
     ]
    }
   ],
   "source": [
    "ae_loss_train = np.load(save_model_path+'/autoencoder_training_loss.npy')\n",
    "print(ae_loss_train.shape)\n",
    "print(ae_loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c92cf879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38885.89458641 38641.3736713  38621.90975302 38611.69315601\n",
      " 38604.99246506 38599.78575334 38595.23700282 38591.69774939\n",
      " 38588.72012555 38586.48332883 38584.60108565 38582.95643312\n",
      " 38581.53827514 38580.24141399 38578.98082685 38578.06584902\n",
      " 38577.18614987 38576.43666789 38575.57259353 38574.94935338]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "avg_epoch_loss = np.average(ae_loss_train, axis=1)\n",
    "print(avg_epoch_loss)\n",
    "\n",
    "plt.plot(avg_epoch_loss)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.yticks(ticks=[38900, 38850, 38800, 38750, 38700, 38650, 38600],labels=[])\n",
    "plt.xticks(ticks=[4*i for i in range(6)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb310025-bea0-499d-905a-0b14e33ca9cf",
   "metadata": {},
   "source": [
    "Predict on tr set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abdf0bb1-857e-40c1-99f1-d10a37fce6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 GPU\n",
      "Eval: [8000/1309486 (1%)]\tLoss: 33060.929688\n",
      "Eval: [16000/1309486 (1%)]\tLoss: 35236.378906\n",
      "Eval: [24000/1309486 (2%)]\tLoss: 35466.214844\n",
      "Eval: [32000/1309486 (2%)]\tLoss: 34721.296875\n",
      "Eval: [40000/1309486 (3%)]\tLoss: 35039.175781\n",
      "Eval: [48000/1309486 (4%)]\tLoss: 32412.117188\n",
      "Eval: [56000/1309486 (4%)]\tLoss: 39307.292969\n",
      "Eval: [64000/1309486 (5%)]\tLoss: 38355.195312\n",
      "Eval: [72000/1309486 (5%)]\tLoss: 35446.828125\n",
      "Eval: [80000/1309486 (6%)]\tLoss: 50918.410156\n",
      "Eval: [88000/1309486 (7%)]\tLoss: 55088.156250\n",
      "Eval: [96000/1309486 (7%)]\tLoss: 37801.089844\n",
      "Eval: [104000/1309486 (8%)]\tLoss: 46589.015625\n",
      "Eval: [112000/1309486 (9%)]\tLoss: 52493.839844\n",
      "Eval: [120000/1309486 (9%)]\tLoss: 31926.824219\n",
      "Eval: [128000/1309486 (10%)]\tLoss: 44575.347656\n",
      "Eval: [136000/1309486 (10%)]\tLoss: 39596.308594\n",
      "Eval: [144000/1309486 (11%)]\tLoss: 41060.851562\n",
      "Eval: [152000/1309486 (12%)]\tLoss: 34182.140625\n",
      "Eval: [160000/1309486 (12%)]\tLoss: 34724.046875\n",
      "Eval: [168000/1309486 (13%)]\tLoss: 40271.500000\n",
      "Eval: [176000/1309486 (13%)]\tLoss: 37147.968750\n",
      "Eval: [184000/1309486 (14%)]\tLoss: 34830.718750\n",
      "Eval: [192000/1309486 (15%)]\tLoss: 38217.746094\n",
      "Eval: [200000/1309486 (15%)]\tLoss: 37476.914062\n",
      "Eval: [208000/1309486 (16%)]\tLoss: 35039.804688\n",
      "Eval: [216000/1309486 (16%)]\tLoss: 37364.496094\n",
      "Eval: [224000/1309486 (17%)]\tLoss: 35653.039062\n",
      "Eval: [232000/1309486 (18%)]\tLoss: 36948.382812\n",
      "Eval: [240000/1309486 (18%)]\tLoss: 38883.867188\n",
      "Eval: [248000/1309486 (19%)]\tLoss: 32271.468750\n",
      "Eval: [256000/1309486 (20%)]\tLoss: 41453.625000\n",
      "Eval: [264000/1309486 (20%)]\tLoss: 40290.523438\n",
      "Eval: [272000/1309486 (21%)]\tLoss: 41096.300781\n",
      "Eval: [280000/1309486 (21%)]\tLoss: 38900.890625\n",
      "Eval: [288000/1309486 (22%)]\tLoss: 36562.691406\n",
      "Eval: [296000/1309486 (23%)]\tLoss: 34402.054688\n",
      "Eval: [304000/1309486 (23%)]\tLoss: 30741.179688\n",
      "Eval: [312000/1309486 (24%)]\tLoss: 40227.054688\n",
      "Eval: [320000/1309486 (24%)]\tLoss: 31556.445312\n",
      "Eval: [328000/1309486 (25%)]\tLoss: 36139.570312\n",
      "Eval: [336000/1309486 (26%)]\tLoss: 35942.511719\n",
      "Eval: [344000/1309486 (26%)]\tLoss: 33368.429688\n",
      "Eval: [352000/1309486 (27%)]\tLoss: 39068.242188\n",
      "Eval: [360000/1309486 (27%)]\tLoss: 33068.351562\n",
      "Eval: [368000/1309486 (28%)]\tLoss: 42975.890625\n",
      "Eval: [376000/1309486 (29%)]\tLoss: 45271.035156\n",
      "Eval: [384000/1309486 (29%)]\tLoss: 47224.218750\n",
      "Eval: [392000/1309486 (30%)]\tLoss: 43969.472656\n",
      "Eval: [400000/1309486 (31%)]\tLoss: 45255.535156\n",
      "Eval: [408000/1309486 (31%)]\tLoss: 33652.882812\n",
      "Eval: [416000/1309486 (32%)]\tLoss: 40351.402344\n",
      "Eval: [424000/1309486 (32%)]\tLoss: 37443.058594\n",
      "Eval: [432000/1309486 (33%)]\tLoss: 35547.390625\n",
      "Eval: [440000/1309486 (34%)]\tLoss: 33946.648438\n",
      "Eval: [448000/1309486 (34%)]\tLoss: 33138.656250\n",
      "Eval: [456000/1309486 (35%)]\tLoss: 35826.187500\n",
      "Eval: [464000/1309486 (35%)]\tLoss: 31249.498047\n",
      "Eval: [472000/1309486 (36%)]\tLoss: 28622.410156\n",
      "Eval: [480000/1309486 (37%)]\tLoss: 31433.851562\n",
      "Eval: [488000/1309486 (37%)]\tLoss: 38919.656250\n",
      "Eval: [496000/1309486 (38%)]\tLoss: 33134.117188\n",
      "Eval: [504000/1309486 (38%)]\tLoss: 30181.417969\n",
      "Eval: [512000/1309486 (39%)]\tLoss: 33645.117188\n",
      "Eval: [520000/1309486 (40%)]\tLoss: 34558.738281\n",
      "Eval: [528000/1309486 (40%)]\tLoss: 43501.617188\n",
      "Eval: [536000/1309486 (41%)]\tLoss: 44408.898438\n",
      "Eval: [544000/1309486 (42%)]\tLoss: 39119.609375\n",
      "Eval: [552000/1309486 (42%)]\tLoss: 45944.734375\n",
      "Eval: [560000/1309486 (43%)]\tLoss: 47312.164062\n",
      "Eval: [568000/1309486 (43%)]\tLoss: 40334.476562\n",
      "Eval: [576000/1309486 (44%)]\tLoss: 35791.804688\n",
      "Eval: [584000/1309486 (45%)]\tLoss: 35294.562500\n",
      "Eval: [592000/1309486 (45%)]\tLoss: 44536.707031\n",
      "Eval: [600000/1309486 (46%)]\tLoss: 47425.507812\n",
      "Eval: [608000/1309486 (46%)]\tLoss: 45826.304688\n",
      "Eval: [616000/1309486 (47%)]\tLoss: 32058.982422\n",
      "Eval: [624000/1309486 (48%)]\tLoss: 33099.437500\n",
      "Eval: [632000/1309486 (48%)]\tLoss: 38406.496094\n",
      "Eval: [640000/1309486 (49%)]\tLoss: 34704.410156\n",
      "Eval: [648000/1309486 (49%)]\tLoss: 40296.320312\n",
      "Eval: [656000/1309486 (50%)]\tLoss: 35872.664062\n",
      "Eval: [664000/1309486 (51%)]\tLoss: 33978.621094\n",
      "Eval: [672000/1309486 (51%)]\tLoss: 36589.929688\n",
      "Eval: [680000/1309486 (52%)]\tLoss: 34991.718750\n",
      "Eval: [688000/1309486 (53%)]\tLoss: 34457.578125\n",
      "Eval: [696000/1309486 (53%)]\tLoss: 32330.779297\n",
      "Eval: [704000/1309486 (54%)]\tLoss: 32021.632812\n",
      "Eval: [712000/1309486 (54%)]\tLoss: 61728.214844\n",
      "Eval: [720000/1309486 (55%)]\tLoss: 37139.382812\n",
      "Eval: [728000/1309486 (56%)]\tLoss: 31084.095703\n",
      "Eval: [736000/1309486 (56%)]\tLoss: 37097.109375\n",
      "Eval: [744000/1309486 (57%)]\tLoss: 33749.777344\n",
      "Eval: [752000/1309486 (57%)]\tLoss: 39568.992188\n",
      "Eval: [760000/1309486 (58%)]\tLoss: 35838.117188\n",
      "Eval: [768000/1309486 (59%)]\tLoss: 35652.289062\n",
      "Eval: [776000/1309486 (59%)]\tLoss: 36488.968750\n",
      "Eval: [784000/1309486 (60%)]\tLoss: 33717.445312\n",
      "Eval: [792000/1309486 (60%)]\tLoss: 28924.851562\n",
      "Eval: [800000/1309486 (61%)]\tLoss: 39657.226562\n",
      "Eval: [808000/1309486 (62%)]\tLoss: 40602.343750\n",
      "Eval: [816000/1309486 (62%)]\tLoss: 40491.246094\n",
      "Eval: [824000/1309486 (63%)]\tLoss: 32525.853516\n",
      "Eval: [832000/1309486 (64%)]\tLoss: 41714.859375\n",
      "Eval: [840000/1309486 (64%)]\tLoss: 34611.617188\n",
      "Eval: [848000/1309486 (65%)]\tLoss: 35395.304688\n",
      "Eval: [856000/1309486 (65%)]\tLoss: 33921.757812\n",
      "Eval: [864000/1309486 (66%)]\tLoss: 33840.148438\n",
      "Eval: [872000/1309486 (67%)]\tLoss: 39797.597656\n",
      "Eval: [880000/1309486 (67%)]\tLoss: 37606.621094\n",
      "Eval: [888000/1309486 (68%)]\tLoss: 31487.539062\n",
      "Eval: [896000/1309486 (68%)]\tLoss: 33635.433594\n",
      "Eval: [904000/1309486 (69%)]\tLoss: 50647.781250\n",
      "Eval: [912000/1309486 (70%)]\tLoss: 42983.625000\n",
      "Eval: [920000/1309486 (70%)]\tLoss: 46737.640625\n",
      "Eval: [928000/1309486 (71%)]\tLoss: 49250.777344\n",
      "Eval: [936000/1309486 (71%)]\tLoss: 43851.007812\n",
      "Eval: [944000/1309486 (72%)]\tLoss: 35579.839844\n",
      "Eval: [952000/1309486 (73%)]\tLoss: 34467.000000\n",
      "Eval: [960000/1309486 (73%)]\tLoss: 36883.593750\n",
      "Eval: [968000/1309486 (74%)]\tLoss: 40463.968750\n",
      "Eval: [976000/1309486 (75%)]\tLoss: 42787.125000\n",
      "Eval: [984000/1309486 (75%)]\tLoss: 39902.660156\n",
      "Eval: [992000/1309486 (76%)]\tLoss: 42188.531250\n",
      "Eval: [1000000/1309486 (76%)]\tLoss: 32501.410156\n",
      "Eval: [1008000/1309486 (77%)]\tLoss: 45555.609375\n",
      "Eval: [1016000/1309486 (78%)]\tLoss: 36439.109375\n",
      "Eval: [1024000/1309486 (78%)]\tLoss: 39635.945312\n",
      "Eval: [1032000/1309486 (79%)]\tLoss: 39133.777344\n",
      "Eval: [1040000/1309486 (79%)]\tLoss: 45584.777344\n",
      "Eval: [1048000/1309486 (80%)]\tLoss: 44508.960938\n",
      "Eval: [1056000/1309486 (81%)]\tLoss: 48066.652344\n",
      "Eval: [1064000/1309486 (81%)]\tLoss: 34051.207031\n",
      "Eval: [1072000/1309486 (82%)]\tLoss: 39142.925781\n",
      "Eval: [1080000/1309486 (82%)]\tLoss: 33083.507812\n",
      "Eval: [1088000/1309486 (83%)]\tLoss: 33650.429688\n",
      "Eval: [1096000/1309486 (84%)]\tLoss: 31454.339844\n",
      "Eval: [1104000/1309486 (84%)]\tLoss: 35376.574219\n",
      "Eval: [1112000/1309486 (85%)]\tLoss: 33336.058594\n",
      "Eval: [1120000/1309486 (86%)]\tLoss: 31540.675781\n",
      "Eval: [1128000/1309486 (86%)]\tLoss: 31410.046875\n",
      "Eval: [1136000/1309486 (87%)]\tLoss: 31635.982422\n",
      "Eval: [1144000/1309486 (87%)]\tLoss: 44907.492188\n",
      "Eval: [1152000/1309486 (88%)]\tLoss: 44464.031250\n",
      "Eval: [1160000/1309486 (89%)]\tLoss: 50112.023438\n",
      "Eval: [1168000/1309486 (89%)]\tLoss: 37218.859375\n",
      "Eval: [1176000/1309486 (90%)]\tLoss: 35820.218750\n",
      "Eval: [1184000/1309486 (90%)]\tLoss: 35684.804688\n",
      "Eval: [1192000/1309486 (91%)]\tLoss: 32138.175781\n",
      "Eval: [1200000/1309486 (92%)]\tLoss: 33666.152344\n",
      "Eval: [1208000/1309486 (92%)]\tLoss: 43665.453125\n",
      "Eval: [1216000/1309486 (93%)]\tLoss: 44384.308594\n",
      "Eval: [1224000/1309486 (93%)]\tLoss: 42291.125000\n",
      "Eval: [1232000/1309486 (94%)]\tLoss: 40386.414062\n",
      "Eval: [1240000/1309486 (95%)]\tLoss: 38472.937500\n",
      "Eval: [1248000/1309486 (95%)]\tLoss: 45963.585938\n",
      "Eval: [1256000/1309486 (96%)]\tLoss: 34803.816406\n",
      "Eval: [1264000/1309486 (97%)]\tLoss: 44737.414062\n",
      "Eval: [1272000/1309486 (97%)]\tLoss: 32481.185547\n",
      "Eval: [1280000/1309486 (98%)]\tLoss: 34112.578125\n",
      "Eval: [1288000/1309486 (98%)]\tLoss: 38270.679688\n",
      "Eval: [1296000/1309486 (99%)]\tLoss: 32522.613281\n",
      "Eval: [1304000/1309486 (100%)]\tLoss: 37208.855469\n",
      "\n",
      "Test set (1309486 samples): Average loss: 4819.8666\n",
      "\n",
      "eval time:  2501.41464019\n"
     ]
    }
   ],
   "source": [
    "recover_epoch = 20\n",
    "\n",
    "# Detect devices\n",
    "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")   # use CPU or GPU\n",
    "\n",
    "# Data loading parameters\n",
    "params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               ])\n",
    "\n",
    "# tiles dataset\n",
    "tiles_dataset = Dataset(dir_path='../WSI/tiles/', transform=transform, dataset='tr')\n",
    "\n",
    "# Data loader \n",
    "valid_loader = torch.utils.data.DataLoader(dataset=tiles_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create model\n",
    "autoencoder = autoencoder(fc_hidden1=fc_hidden1, drop_p=dropout_p, embed_dim=embed_dim).to(device)\n",
    "    \n",
    "# Recover model                                \n",
    "model_load_path = os.path.join(save_model_path, 'model_epoch{}.pth'.format(recover_epoch))\n",
    "autoencoder.load_state_dict(torch.load(model_load_path))\n",
    "\n",
    "print(\"Using\", torch.cuda.device_count(), \"GPU\")\n",
    "model_params = list(autoencoder.parameters())\n",
    "optimizer = torch.optim.Adam(model_params, lr=learning_rate)\n",
    "\n",
    "# Recover optimizer                                \n",
    "optimizer_load_path = os.path.join(save_model_path, 'optimizer_epoch{}.pth'.format(recover_epoch))\n",
    "optimizer.load_state_dict(torch.load(optimizer_load_path))\n",
    "    \n",
    "\n",
    "# Evaluate\n",
    "\n",
    "X_test, z_test, X_reconst_test, epoch_test_loss = evaluate(log_interval, autoencoder, device, optimizer, valid_loader)\n",
    "\n",
    "        \n",
    "np.save(os.path.join(save_model_path, 'AE_test_loss.npy'), epoch_test_loss)\n",
    "np.save(os.path.join(save_model_path, 'X_ae_test_epoch{}.npy'.format(recover_epoch)), X_test)\n",
    "np.save(os.path.join(save_model_path, 'z_ae_test_epoch{}.npy'.format(recover_epoch)), z_test)\n",
    "np.save(os.path.join(save_model_path, 'X_reconst_test_epoch{}.npy'.format(recover_epoch)), X_reconst_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a6b830-827e-42bb-bf16-3d29baeec5c6",
   "metadata": {},
   "source": [
    "# Internal Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcc526b7-b7c6-476f-ad13-fd6b8f443cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 GPU\n",
      "Eval: [8000/206606 (4%)]\tLoss: 35052.765625\n",
      "Eval: [16000/206606 (8%)]\tLoss: 43821.136719\n",
      "Eval: [24000/206606 (12%)]\tLoss: 37380.542969\n",
      "Eval: [32000/206606 (15%)]\tLoss: 36236.531250\n",
      "Eval: [40000/206606 (19%)]\tLoss: 38896.359375\n",
      "Eval: [48000/206606 (23%)]\tLoss: 35486.359375\n",
      "Eval: [56000/206606 (27%)]\tLoss: 34534.472656\n",
      "Eval: [64000/206606 (31%)]\tLoss: 35624.203125\n",
      "Eval: [72000/206606 (35%)]\tLoss: 40547.000000\n",
      "Eval: [80000/206606 (39%)]\tLoss: 34803.140625\n",
      "Eval: [88000/206606 (43%)]\tLoss: 33519.859375\n",
      "Eval: [96000/206606 (46%)]\tLoss: 37413.722656\n",
      "Eval: [104000/206606 (50%)]\tLoss: 34898.296875\n",
      "Eval: [112000/206606 (54%)]\tLoss: 33045.585938\n",
      "Eval: [120000/206606 (58%)]\tLoss: 35537.781250\n",
      "Eval: [128000/206606 (62%)]\tLoss: 38671.828125\n",
      "Eval: [136000/206606 (66%)]\tLoss: 31723.685547\n",
      "Eval: [144000/206606 (70%)]\tLoss: 30760.294922\n",
      "Eval: [152000/206606 (74%)]\tLoss: 30764.140625\n",
      "Eval: [160000/206606 (77%)]\tLoss: 37341.531250\n",
      "Eval: [168000/206606 (81%)]\tLoss: 43183.511719\n",
      "Eval: [176000/206606 (85%)]\tLoss: 35154.105469\n",
      "Eval: [184000/206606 (89%)]\tLoss: 33011.906250\n",
      "Eval: [192000/206606 (93%)]\tLoss: 35096.761719\n",
      "Eval: [200000/206606 (97%)]\tLoss: 39296.539062\n",
      "\n",
      "Test set (206606 samples): Average loss: 4671.6347\n",
      "\n",
      "eval time:  1325.44824791\n"
     ]
    }
   ],
   "source": [
    "recover_epoch = 20\n",
    "\n",
    "# Detect devices\n",
    "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")   # use CPU or GPU\n",
    "\n",
    "# Data loading parameters\n",
    "params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               ])\n",
    "\n",
    "# tiles dataset\n",
    "tiles_dataset = Dataset(dir_path='../WSI/tiles/', transform=transform, dataset='ts')\n",
    "\n",
    "# Data loader \n",
    "valid_loader = torch.utils.data.DataLoader(dataset=tiles_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create model\n",
    "autoencoder = autoencoder(fc_hidden1=fc_hidden1, drop_p=dropout_p, embed_dim=embed_dim).to(device)\n",
    "    \n",
    "# Recover model                                \n",
    "model_load_path = os.path.join(save_model_path, 'model_epoch{}.pth'.format(recover_epoch))\n",
    "autoencoder.load_state_dict(torch.load(model_load_path))\n",
    "\n",
    "print(\"Using\", torch.cuda.device_count(), \"GPU\")\n",
    "model_params = list(autoencoder.parameters())\n",
    "optimizer = torch.optim.Adam(model_params, lr=learning_rate)\n",
    "\n",
    "# Recover optimizer                                \n",
    "optimizer_load_path = os.path.join(save_model_path, 'optimizer_epoch{}.pth'.format(recover_epoch))\n",
    "optimizer.load_state_dict(torch.load(optimizer_load_path))\n",
    "    \n",
    "\n",
    "# Evaluate\n",
    "\n",
    "X_test, z_test, X_reconst_test, epoch_test_loss = evaluate(log_interval, autoencoder, device, optimizer, valid_loader)\n",
    "\n",
    "    \n",
    "        \n",
    "np.save(os.path.join(save_model_path, 'AE_internal_test_loss.npy'), epoch_test_loss)\n",
    "np.save(os.path.join(save_model_path, 'X_ae_internal_test_epoch{}.npy'.format(recover_epoch)), X_test)\n",
    "np.save(os.path.join(save_model_path, 'z_ae_internal_test_epoch{}.npy'.format(recover_epoch)), z_test)\n",
    "np.save(os.path.join(save_model_path, 'X_reconst_internal_test_epoch{}.npy'.format(recover_epoch)), X_reconst_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72acb1d3-ead2-40b6-8cf5-21cf7676b0b7",
   "metadata": {},
   "source": [
    "# External Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a25d3d75-a850-40b0-9dc2-579b2b464ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 GPU\n",
      "Eval: [8000/545326 (1%)]\tLoss: 37496.898438\n",
      "Eval: [16000/545326 (3%)]\tLoss: 38869.218750\n",
      "Eval: [24000/545326 (4%)]\tLoss: 36126.898438\n",
      "Eval: [32000/545326 (6%)]\tLoss: 35333.132812\n",
      "Eval: [40000/545326 (7%)]\tLoss: 54255.375000\n",
      "Eval: [48000/545326 (9%)]\tLoss: 67768.734375\n",
      "Eval: [56000/545326 (10%)]\tLoss: 39079.984375\n",
      "Eval: [64000/545326 (12%)]\tLoss: 37335.082031\n",
      "Eval: [72000/545326 (13%)]\tLoss: 39311.414062\n",
      "Eval: [80000/545326 (15%)]\tLoss: 44714.281250\n",
      "Eval: [88000/545326 (16%)]\tLoss: 42738.597656\n",
      "Eval: [96000/545326 (18%)]\tLoss: 36847.679688\n",
      "Eval: [104000/545326 (19%)]\tLoss: 41668.242188\n",
      "Eval: [112000/545326 (21%)]\tLoss: 38707.687500\n",
      "Eval: [120000/545326 (22%)]\tLoss: 37313.058594\n",
      "Eval: [128000/545326 (23%)]\tLoss: 51559.769531\n",
      "Eval: [136000/545326 (25%)]\tLoss: 42461.039062\n",
      "Eval: [144000/545326 (26%)]\tLoss: 42559.394531\n",
      "Eval: [152000/545326 (28%)]\tLoss: 43168.808594\n",
      "Eval: [160000/545326 (29%)]\tLoss: 42373.980469\n",
      "Eval: [168000/545326 (31%)]\tLoss: 39734.722656\n",
      "Eval: [176000/545326 (32%)]\tLoss: 38637.097656\n",
      "Eval: [184000/545326 (34%)]\tLoss: 40081.296875\n",
      "Eval: [192000/545326 (35%)]\tLoss: 41228.367188\n",
      "Eval: [200000/545326 (37%)]\tLoss: 37446.601562\n",
      "Eval: [208000/545326 (38%)]\tLoss: 40378.085938\n",
      "Eval: [216000/545326 (40%)]\tLoss: 37587.195312\n",
      "Eval: [224000/545326 (41%)]\tLoss: 36878.218750\n",
      "Eval: [232000/545326 (43%)]\tLoss: 41343.039062\n",
      "Eval: [240000/545326 (44%)]\tLoss: 39242.515625\n",
      "Eval: [248000/545326 (45%)]\tLoss: 45741.609375\n",
      "Eval: [256000/545326 (47%)]\tLoss: 56333.589844\n",
      "Eval: [264000/545326 (48%)]\tLoss: 51197.359375\n",
      "Eval: [272000/545326 (50%)]\tLoss: 53301.359375\n",
      "Eval: [280000/545326 (51%)]\tLoss: 36475.203125\n",
      "Eval: [288000/545326 (53%)]\tLoss: 34777.253906\n",
      "Eval: [296000/545326 (54%)]\tLoss: 46998.140625\n",
      "Eval: [304000/545326 (56%)]\tLoss: 46892.312500\n",
      "Eval: [312000/545326 (57%)]\tLoss: 45931.343750\n",
      "Eval: [320000/545326 (59%)]\tLoss: 44657.269531\n",
      "Eval: [328000/545326 (60%)]\tLoss: 44673.398438\n",
      "Eval: [336000/545326 (62%)]\tLoss: 39605.816406\n",
      "Eval: [344000/545326 (63%)]\tLoss: 42154.375000\n",
      "Eval: [352000/545326 (65%)]\tLoss: 41703.843750\n",
      "Eval: [360000/545326 (66%)]\tLoss: 45415.140625\n",
      "Eval: [368000/545326 (67%)]\tLoss: 40993.128906\n",
      "Eval: [376000/545326 (69%)]\tLoss: 41856.527344\n",
      "Eval: [384000/545326 (70%)]\tLoss: 41432.824219\n",
      "Eval: [392000/545326 (72%)]\tLoss: 37593.605469\n",
      "Eval: [400000/545326 (73%)]\tLoss: 58273.765625\n",
      "Eval: [408000/545326 (75%)]\tLoss: 46944.320312\n",
      "Eval: [416000/545326 (76%)]\tLoss: 39685.730469\n",
      "Eval: [424000/545326 (78%)]\tLoss: 45088.515625\n",
      "Eval: [432000/545326 (79%)]\tLoss: 38334.148438\n",
      "Eval: [440000/545326 (81%)]\tLoss: 43660.265625\n",
      "Eval: [448000/545326 (82%)]\tLoss: 37056.070312\n",
      "Eval: [456000/545326 (84%)]\tLoss: 44473.601562\n",
      "Eval: [464000/545326 (85%)]\tLoss: 35426.953125\n",
      "Eval: [472000/545326 (87%)]\tLoss: 37701.191406\n",
      "Eval: [480000/545326 (88%)]\tLoss: 43840.250000\n",
      "Eval: [488000/545326 (89%)]\tLoss: 36613.851562\n",
      "Eval: [496000/545326 (91%)]\tLoss: 39708.898438\n",
      "Eval: [504000/545326 (92%)]\tLoss: 41996.125000\n",
      "Eval: [512000/545326 (94%)]\tLoss: 36319.437500\n",
      "Eval: [520000/545326 (95%)]\tLoss: 35359.398438\n",
      "Eval: [528000/545326 (97%)]\tLoss: 38521.554688\n",
      "Eval: [536000/545326 (98%)]\tLoss: 41875.000000\n",
      "Eval: [544000/545326 (100%)]\tLoss: 41272.226562\n",
      "\n",
      "Test set (545326 samples): Average loss: 5399.3673\n",
      "\n",
      "eval time:  4818.29075170\n"
     ]
    }
   ],
   "source": [
    "recover_epoch = 20\n",
    "\n",
    "# Detect devices\n",
    "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")   # use CPU or GPU\n",
    "\n",
    "# Data loading parameters\n",
    "params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                # Normalize to have tr dataset mean and std (params calculated in norm_params.ipynb\n",
    "                                #transforms.Normalize(mean=[-0.2792, -0.2173, -0.0899], std=[1.2762, 1.2154, 1.0878], inplace=True)\n",
    "                               ])\n",
    "\n",
    "# tiles dataset\n",
    "tiles_dataset = Dataset(dir_path='../WSI/tiles/', transform=transform, dataset='ext')\n",
    "\n",
    "# Data loader \n",
    "valid_loader = torch.utils.data.DataLoader(dataset=tiles_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create model\n",
    "autoencoder = autoencoder(fc_hidden1=fc_hidden1, drop_p=dropout_p, embed_dim=embed_dim).to(device)\n",
    "    \n",
    "# Recover model                                \n",
    "model_load_path = os.path.join(save_model_path, 'model_epoch{}.pth'.format(recover_epoch))\n",
    "autoencoder.load_state_dict(torch.load(model_load_path))\n",
    "\n",
    "print(\"Using\", torch.cuda.device_count(), \"GPU\")\n",
    "model_params = list(autoencoder.parameters())\n",
    "optimizer = torch.optim.Adam(model_params, lr=learning_rate)\n",
    "\n",
    "# Recover optimizer                                \n",
    "optimizer_load_path = os.path.join(save_model_path, 'optimizer_epoch{}.pth'.format(recover_epoch))\n",
    "optimizer.load_state_dict(torch.load(optimizer_load_path))\n",
    "    \n",
    "\n",
    "# Evaluate\n",
    "X_test, z_test, X_reconst_test, epoch_test_loss = evaluate(log_interval, autoencoder, device, optimizer, valid_loader)\n",
    "\n",
    "        \n",
    "np.save(os.path.join(save_model_path, 'AE_ext_test_loss.npy'), epoch_test_loss)\n",
    "np.save(os.path.join(save_model_path, 'X_ae_ext_test_epoch{}.npy'.format(recover_epoch)), X_test)\n",
    "np.save(os.path.join(save_model_path, 'z_ae_ext_test_epoch{}.npy'.format(recover_epoch)), z_test)\n",
    "#np.save(os.path.join(save_model_path, 'X_reconst_ext_test_epoch{}.npy'.format(recover_epoch)), X_reconst_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdl1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16 (default, Jan 17 2023, 22:20:44) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "994350ee2423e6504ff55d05744e896480bca47f12344c18a8c4a4655e3d73cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
